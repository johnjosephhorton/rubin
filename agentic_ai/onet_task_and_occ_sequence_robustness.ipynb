{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Dec 16, 2025\n",
    "#### Last Edit: Jan 25, 2026\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97af73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "# Install caffeinate package\n",
    "%pip install caffeinate\n",
    "\n",
    "# Use macOS built-in caffeinate command for reliability\n",
    "# This prevents the system from sleeping while the process is running\n",
    "try:\n",
    "    # Start caffeinate in the background\n",
    "    caff_process = subprocess.Popen(['caffeinate', '-d'], \n",
    "                                   stdout=subprocess.DEVNULL, \n",
    "                                   stderr=subprocess.DEVNULL)\n",
    "    print(f\"Caffeinate mode ON ‚òï ‚Äì Device will stay awake (PID: {caff_process.pid})\")\n",
    "    print(\"System sleep is disabled while this process runs.\")\n",
    "    \n",
    "    # Store the process ID for later cleanup\n",
    "    caff_pid = caff_process.pid\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not start caffeinate: {e}\")\n",
    "    print(\"Continuing without caffeinate - system may sleep during long processes.\")\n",
    "    caff_process = None\n",
    "    caff_pid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load O*NET data and extract unique occupation titles\n",
    "ONET = pd.read_csv(f'{output_data_path}/ONET_cleaned_tasks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252758cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsl import QuestionFreeText, Scenario, Model, Survey\n",
    "from textwrap import dedent\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_task_sequence(occupation, tasks_data, user_prompt, output_data_path, prompt_number=1):\n",
    "    \"\"\"\n",
    "    Extract task sequence for an occupation using EDSL workflow.\n",
    "    Returns the ordered sequence of tasks.\n",
    "    \"\"\"\n",
    "    # Check if output file already exists\n",
    "    safe_title = occupation.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    # Create a folder per occupation and include the prompt number in the filename\n",
    "    output_folder = f'{output_data_path}/tasks_sequences_robustness_repetitive/{safe_title}'\n",
    "    output_file = os.path.join(output_folder, f\"{safe_title}_{prompt_number}.csv\")\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        return output_file, True  # Return file path and flag indicating it already existed\n",
    "    \n",
    "    # Check if we have tasks for this occupation\n",
    "    if tasks_data.empty:\n",
    "        print(f\"‚ö†Ô∏è  Warning: No tasks found for occupation '{occupation}' - skipping\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "    \n",
    "    # Create task mappings\n",
    "    task_id_mapping = dict(zip(tasks_data['Task Title'], tasks_data['Task ID']))\n",
    "    soc_code_mapping = dict(zip(tasks_data['Task Title'], tasks_data['O*NET-SOC Code']))\n",
    "\n",
    "    # Format tasks as numbered list\n",
    "    tasks_list = tasks_data['Task Title'].tolist()\n",
    "    tasks_text = \"\\n\".join([f\"{i}. {task}\" for i, task in enumerate(tasks_list, 1)])\n",
    "    num_tasks = len(tasks_list)\n",
    "    max_tokens = 32000\n",
    "    \n",
    "    print(f\"   ‚Ä¢ {num_tasks} tasks, using {max_tokens} max tokens\")\n",
    "\n",
    "    # Create scenario\n",
    "    scenario = Scenario({\n",
    "        \"occupation\": occupation,\n",
    "        \"tasks_list\": tasks_text,\n",
    "        \"num_tasks\": num_tasks\n",
    "    })\n",
    "\n",
    "    # Create question for task sequencing using the chosen template\n",
    "    q_sequence = QuestionFreeText(\n",
    "        question_name=\"task_sequence\",\n",
    "        question_text=user_prompt\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create model using openai_v2 for reasoning capabilities\n",
    "        model = Model(\"gpt-5-mini\", service_name=\"openai_v2\", temperature=0.0, max_tokens=max_tokens)\n",
    "        \n",
    "        # Run sequence question\n",
    "        sequence_results = q_sequence.by(model).by([scenario]).run(progress_bar=False)\n",
    "        sequence_df = sequence_results.to_pandas()\n",
    "        sequence_json = sequence_df['answer.task_sequence'][0]\n",
    "        \n",
    "        # Debug: Print the raw response before cleaning\n",
    "        print(f\"   ‚Ä¢ Raw JSON length: {len(str(sequence_json))}\")\n",
    "        print(f\"   ‚Ä¢ Raw JSON preview: {str(sequence_json)[:50]}...\")\n",
    "        \n",
    "        # Clean the JSON response by removing markdown code blocks if present\n",
    "        if isinstance(sequence_json, str):\n",
    "            # Simple string replacement approach\n",
    "            cleaned_json = sequence_json\n",
    "            if '```json' in cleaned_json:\n",
    "                cleaned_json = cleaned_json.replace('```json', '')\n",
    "            if '```' in cleaned_json:\n",
    "                cleaned_json = cleaned_json.replace('```', '')\n",
    "            sequence_json = cleaned_json.strip()\n",
    "            print(f\"   ‚Ä¢ Cleaned JSON preview: {sequence_json[:50]}...\")\n",
    "        \n",
    "        # Check if the response is valid\n",
    "        if pd.isna(sequence_json) or not isinstance(sequence_json, str):\n",
    "            print(f\"‚ùå Error: Invalid response for '{occupation}' - got {type(sequence_json)} instead of string\")\n",
    "            return None, True  # Treat as already processed to skip\n",
    "        \n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            sequence_data = json.loads(sequence_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå JSON parsing failed, trying to clean response further...\")\n",
    "            print(f\"   Original error: {e}\")\n",
    "            print(f\"   Response starts with: {sequence_json[:100]}...\")\n",
    "            # Try additional cleanup\n",
    "            if sequence_json.startswith('```'):\n",
    "                lines = sequence_json.split('\\n')\n",
    "                if lines[0].strip() in ['```', '```json']:\n",
    "                    lines = lines[1:]  # Remove first line\n",
    "                if lines[-1].strip() == '```':\n",
    "                    lines = lines[:-1]  # Remove last line\n",
    "                sequence_json = '\\n'.join(lines).strip()\n",
    "                print(f\"   Cleaned response starts with: {sequence_json[:100]}...\")\n",
    "                try:\n",
    "                    sequence_data = json.loads(sequence_json)\n",
    "                    print(f\"   ‚úÖ Successfully parsed after additional cleanup\")\n",
    "                except json.JSONDecodeError as e2:\n",
    "                    print(f\"   ‚ùå Still failed after cleanup: {e2}\")\n",
    "                    raise e  # Re-raise original error\n",
    "            else:\n",
    "                raise e  # Re-raise original error\n",
    "                \n",
    "        ordered_sequence_df = pd.DataFrame(sequence_data)\n",
    "        \n",
    "        # Add metadata columns\n",
    "        ordered_sequence_df['Occupation Title'] = occupation\n",
    "        ordered_sequence_df['Task ID'] = ordered_sequence_df['Task Title'].map(task_id_mapping)\n",
    "        ordered_sequence_df['O*NET-SOC Code'] = ordered_sequence_df['Task Title'].map(soc_code_mapping)\n",
    "        \n",
    "        # Reorder columns\n",
    "        ordered_sequence_df = ordered_sequence_df[['Task Position', 'Task Title', 'Task ID', 'O*NET-SOC Code', 'Occupation Title']]\n",
    "\n",
    "        # Save to file\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        ordered_sequence_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"   ‚úÖ Successfully processed and saved task sequence\")\n",
    "        return output_file, False  # Return file path and flag indicating it was newly created\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON Error for '{occupation}': {e}\")\n",
    "        print(f\"   Raw response: {sequence_json}\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error for '{occupation}': {e}\")\n",
    "        return None, True  # Treat as already processed to skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5185e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_text = dedent(\"\"\"\\You are an expert in workflow analysis for the occupation: {{ occupation }}.\n",
    "           Below is a list of {{ num_tasks }} tasks that are part of this occupation:\n",
    "           {{ tasks_list }}\n",
    "    \"\"\")\n",
    "\n",
    "return_characteristics_text = dedent(\"\"\"\\\n",
    "    Return your answer as a JSON array where each element has:\n",
    "        - \"Task Position\": the sequence number (1, 2, 3, etc.)\n",
    "        - \"Task Title\": the exact task text from the list above\n",
    "    Format: [{\"Task Position\": 1, \"Task Title\": \"...\"}, {\"Task Position\": 2, \"Task Title\": \"...\"}, ...]\n",
    "    Only return the JSON array, nothing else.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "user_prompts_list = [\n",
    "    # 1. Main spec ‚Äî preserved verbatim\n",
    "    'Provide the typical sequential order in which these tasks are performed in a real-world workflow.',\n",
    "    # 2. Narrative / temporal\n",
    "    'Imagine a typical workday for this occupation. As the day unfolds, tasks arise and are completed as needed. Order the tasks in the sequence they most naturally occur.',\n",
    "    # 3. Input‚Äìoutput logic\n",
    "    'For each task, consider its inputs and outputs. Order tasks so outputs of earlier tasks plausibly feed into later tasks. If tasks are parallel, place the more upstream task first.',\n",
    "    # 4. Efficiency without templates\n",
    "    'Order tasks to minimize rework, waiting, and unnecessary handoffs. Assume an experienced worker executing the workflow efficiently.',\n",
    "    # 5. Backward reasoning, then forward\n",
    "    'Think about what must ultimately be produced in this occupation and what needs to happen before that. Use this reasoning to produce a natural forward sequence of tasks.',\n",
    "    # 6. Dependency-first, no phases\n",
    "    'Identify which tasks logically depend on others, then order the tasks in a single sequence consistent with those dependencies and typical practice.',\n",
    "    # 7. Information flow, no staging\n",
    "    'Order tasks according to how information is generated, transformed, and used over the course of the work.',\n",
    "    # 8. Error prevention lens\n",
    "    'Order tasks based on when mistakes would be most costly, placing tasks that prevent or constrain downstream errors earlier.',\n",
    "    # 9. Decision salience\n",
    "    'Order tasks so that tasks informing important decisions tend to occur before tasks that rely on those decisions.',\n",
    "    # 10. Practitioner intuition\n",
    "    'Order the tasks as an experienced practitioner would intuitively carry them out, without explicitly planning or formalizing the workflow.',\n",
    "    # 11. Revealed practice\n",
    "    'Order the tasks to reflect how the work is most commonly carried out in practice, rather than how it is formally described.'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique occupation titles from the dataset\n",
    "# occupations_list = sorted(ONET['Occupation Title'].unique().tolist())\n",
    "repetitive_occupations = pd.read_csv(f\"{input_data_path}/computed_objects/repetitive_onet_detailedOcc_occ_crosswalk.csv\")\n",
    "occupations_list = repetitive_occupations['Occupation Title'].unique().tolist()\n",
    "print(f\"Found {len(occupations_list)} unique occupations in the dataset:\")\n",
    "\n",
    "# Set seed for reproducible random sampling\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# # Randomly sample 10% of occupations\n",
    "# sample_size = max(1, int(len(occupations_list) * 0.1))  # Ensure at least 1 occupation\n",
    "# sampled_occupations = random.sample(occupations_list, sample_size)\n",
    "# print(f\"Randomly selected {len(sampled_occupations)} occupations (5% of total) for processing:\")\n",
    "# print(f\"Sample: {sampled_occupations[:5]}...\" if len(sampled_occupations) > 5 else f\"Sample: {sampled_occupations}\")\n",
    "sampled_occupations = occupations_list\n",
    "\n",
    "\n",
    "# Process each occupation\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "\n",
    "for prompt_number, prompt_text in enumerate(user_prompts_list):\n",
    "    # Combine prefix, prompt, and return characteristics into a single user prompt\n",
    "    user_prompt = prefix_text + '\\n' + prompt_text + '\\n' + return_characteristics_text\n",
    "\n",
    "    for i, occupation in enumerate(sampled_occupations, 1):\n",
    "        # Filter data for this occupation\n",
    "        occupation_data = ONET[ONET['Occupation Title'] == occupation].copy()\n",
    "        \n",
    "        # Prepare task data\n",
    "        occupation_task_data = occupation_data[['Task ID', 'Task Title', 'O*NET-SOC Code']].drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        # Enhanced progress output\n",
    "        num_tasks = len(occupation_task_data)\n",
    "        print(f\"\\n[{i}/{len(sampled_occupations)}] {occupation}\")\n",
    "        \n",
    "        # Extract task sequence\n",
    "        output_file, already_existed = extract_task_sequence(occupation, occupation_task_data, user_prompt, output_data_path, prompt_number)\n",
    "        \n",
    "        if output_file is None:\n",
    "            error_count += 1\n",
    "        elif already_existed:\n",
    "            print(f\"   ‚è≠Ô∏è  Already exists - skipping\")\n",
    "            skipped_count += 1\n",
    "        else:\n",
    "            processed_count += 1\n",
    "\n",
    "    # Prompt-level summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROMPT {prompt_number+1}/{len(user_prompts_list)} COMPLETE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"‚Ä¢ {processed_count} occupations processed\")\n",
    "    print(f\"‚Ä¢ {skipped_count} occupations skipped\")\n",
    "    print(f\"‚Ä¢ {error_count} occupations failed\")\n",
    "    print(f\"‚Ä¢ {len(sampled_occupations)} total occupations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee389c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up caffeinate process\n",
    "try:\n",
    "    if 'caff_process' in globals() and caff_process is not None:\n",
    "        caff_process.terminate()\n",
    "        caff_process.wait()  # Wait for process to terminate\n",
    "        print(\"Caffeinate mode OFF üí° - System sleep is now enabled.\")\n",
    "    else:\n",
    "        print(\"Caffeinate was not running or already stopped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Caffeinate process may have already ended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
