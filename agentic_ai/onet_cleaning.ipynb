{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 10, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be56448",
   "metadata": {},
   "source": [
    "### O*NET Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "857f1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all datasets\n",
    "task_ratings_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Task Ratings.txt', sep='\\t')\n",
    "task_categories_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Task Categories.txt', sep='\\t')\n",
    "tasks_to_dwas_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Tasks to DWAs.txt', sep='\\t')\n",
    "dwa_reference_df = pd.read_csv(f'{input_data_path}/db_27_3_text/DWA Reference.txt', sep='\\t')\n",
    "job_zones_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Job Zones.txt', sep='\\t')\n",
    "task_statements_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Task Statements.txt', sep='\\t')\n",
    "occupation_data_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Occupation Data.txt', sep='\\t')\n",
    "soc_structure_df = pd.read_csv(f'{input_data_path}/SOC_Structure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e4c8ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dataset created with 17,953 rows before task time calculations\n"
     ]
    }
   ],
   "source": [
    "# Merge and transform O*NET task data\n",
    "ONET = task_ratings_df.merge(task_categories_df, on=['Scale ID', 'Category'], how='left')\n",
    "\n",
    "# Process Category Description\n",
    "ONET['Category Description'] = ONET['Category Description'].apply(lambda x: f'FT_{x}' if pd.notna(x) else x)\n",
    "ONET['Category Description'] = ONET['Category Description'].fillna(ONET['Scale ID'])\n",
    "ONET['Category Description'] = ONET['Category Description'].replace({'IM': 'Importance', 'RT': 'Relevance'})\n",
    "\n",
    "# Reshape from long to wide format\n",
    "ONET = ONET.pivot_table(\n",
    "    index=['O*NET-SOC Code', 'Task ID'],\n",
    "    columns='Category Description',\n",
    "    values='Data Value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "ONET.columns.name = None\n",
    "\n",
    "# Merge essential data (but NOT DWA data yet - save that for after task time calculations)\n",
    "ONET = ONET.merge(occupation_data_df[['O*NET-SOC Code', 'Title']], on='O*NET-SOC Code', how='left')\n",
    "ONET.rename(columns={'Title': 'Occupation Title'}, inplace=True)\n",
    "\n",
    "ONET = ONET.merge(task_statements_df[['O*NET-SOC Code', 'Task ID', 'Task', 'Task Type']], on=['O*NET-SOC Code', 'Task ID'], how='left')\n",
    "ONET.rename(columns={'Task': 'Task Title'}, inplace=True)\n",
    "\n",
    "ONET = ONET.merge(job_zones_df[['O*NET-SOC Code', 'Job Zone']], on='O*NET-SOC Code', how='left')\n",
    "\n",
    "print(f\"Base dataset created with {len(ONET):,} rows before task time calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d947ef",
   "metadata": {},
   "source": [
    "### Filter Occupations Containing \"All Other\" and \"Teachers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "71645324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove rows where occupation title contains \"All Other\"\n",
    "# print(f\"Number of rows before removing 'All Other': {ONET.shape[0]:,}\")\n",
    "# ONET = ONET[~ONET[\"Occupation Title\"].str.contains(\"All Other\", case=False, na=False)]\n",
    "# print(f\"Number of rows after removing 'All Other': {ONET.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "a28e5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter and remove \"Teachers\"-related occupations\n",
    "# contains_teacher = ONET[ONET['Occupation Title'].str.contains('Teachers', case=False, na=False)]\n",
    "\n",
    "# unique_teacher_occupations = contains_teacher['Occupation Title'].nunique()\n",
    "# print(f'Number of unique occupations containing the word \"Teachers\": {unique_teacher_occupations}')\n",
    "\n",
    "# # Remove rows that contain \"Teacher\" (case-insensitive)\n",
    "# ONET = ONET[~ONET['Occupation Title'].str.contains('Teachers', case=False, na=False)].reset_index(drop=True)\n",
    "# print(f\"Rows after removing Teachers: {len(ONET):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335b787",
   "metadata": {},
   "source": [
    "### Task Time Measurement Calculation\n",
    "**IMPORTANT**: Calculate task time measures BEFORE merging DWA data to avoid duplication issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "40b7fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Time Measurement Complete!\n",
      "✓ 17,953 tasks processed across 873 occupations\n",
      "✓ 17,953 tasks have valid time measures\n"
     ]
    }
   ],
   "source": [
    "# Task Time Measurement Creation\n",
    "# ==============================\n",
    "\n",
    "# Step 1: Define frequency mappings (annual occurrence rates)\n",
    "frequency_mapping = {\n",
    "    'FT_Several times daily': 1000,      # ~4 times/day × 250 work days\n",
    "    'FT_Hourly or more': 2000,           # Multiple times per hour\n",
    "    'FT_Daily': 250,                      # Once per day\n",
    "    'FT_More than weekly': 156,          # ~3 times/week × 52 weeks\n",
    "    'FT_More than monthly': 24,          # ~2 times/month × 12 months\n",
    "    'FT_More than yearly': 6,            # A few times per year\n",
    "    'FT_Yearly or less': 1               # Once a year\n",
    "}\n",
    "\n",
    "# Step 2: Calculate weighted frequency using percentage distributions\n",
    "def calculate_weighted_frequency(row):\n",
    "    \"\"\"Calculate weighted annual frequency based on percentage distributions.\"\"\"\n",
    "    total_weighted_freq = 0.0\n",
    "    \n",
    "    for freq_col, freq_value in frequency_mapping.items():\n",
    "        if freq_col in row.index and pd.notna(row[freq_col]):\n",
    "            percentage = row[freq_col] / 100.0  # Convert percentage to decimal\n",
    "            total_weighted_freq += freq_value * percentage\n",
    "    \n",
    "    return total_weighted_freq if total_weighted_freq > 0 else np.nan\n",
    "\n",
    "ONET['Weighted_Frequency_Annual'] = ONET.apply(calculate_weighted_frequency, axis=1)\n",
    "\n",
    "# Step 3: Calculate raw task score (frequency × importance × relevance)\n",
    "ONET['Raw_Task_Score'] = (ONET['Weighted_Frequency_Annual'] * \n",
    "                         ONET['Importance'] * \n",
    "                         ONET['Relevance'])\n",
    "\n",
    "# Step 4: Normalize within each occupation (CRITICAL STEP)\n",
    "# Each occupation represents a full-time job (40 hours/week)\n",
    "# Task proportions within each occupation must sum to 1.0 (100% of work time)\n",
    "ONET['Total_Raw_Score_by_Occupation'] = ONET.groupby('O*NET-SOC Code')['Raw_Task_Score'].transform('sum')\n",
    "ONET['Task_Time_Proportion'] = ONET['Raw_Task_Score'] / ONET['Total_Raw_Score_by_Occupation']\n",
    "ONET['Task_Time_Percentage'] = ONET['Task_Time_Proportion'] * 100\n",
    "\n",
    "# # Step 5: Convert to time estimates \n",
    "# # Full-time job = 40 hours/week = 2000 hours/year (50 weeks)\n",
    "# ONET['Hours_Per_Week'] = ONET['Task_Time_Proportion'] * 40  # Convert proportion to hours per week\n",
    "# ONET['Estimated_Annual_Hours'] = ONET['Task_Time_Proportion'] * 2000  # Annual hours\n",
    "# ONET['Hours_Per_Occurrence'] = np.where(\n",
    "#     ONET['Weighted_Frequency_Annual'] > 0,\n",
    "#     ONET['Estimated_Annual_Hours'] / ONET['Weighted_Frequency_Annual'],\n",
    "#     np.nan\n",
    "# )\n",
    "\n",
    "# Step 6: Drop \"Raw_Task_Score\", \"Total_Raw_Score_by_Occupation\", \"Task_Time_Proportion\"\n",
    "ONET = ONET.drop(columns=[\"Raw_Task_Score\", \"Total_Raw_Score_by_Occupation\", \"Task_Time_Proportion\", \"Weighted_Frequency_Annual\"])\n",
    "\n",
    "print(\"Task Time Measurement Complete!\")\n",
    "print(f\"✓ {len(ONET):,} tasks processed across {ONET['O*NET-SOC Code'].nunique():,} occupations\")\n",
    "print(f\"✓ {ONET['Task_Time_Percentage'].gt(0).sum():,} tasks have valid time measures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4a9cd",
   "metadata": {},
   "source": [
    "### Add SOC Industry Levels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "8c842646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SOC Code-to-Label mapping\n",
    "code_label_rows = []\n",
    "for idx, row in soc_structure_df.iterrows():\n",
    "    if pd.notna(row['Major Group']):\n",
    "        code = row['Major Group']\n",
    "    elif pd.notna(row['Minor Group']):\n",
    "        code = row['Minor Group']\n",
    "    elif pd.notna(row['Broad Occupation']):\n",
    "        code = row['Broad Occupation']\n",
    "    elif pd.notna(row['Detailed Occupation']):\n",
    "        code = row['Detailed Occupation']\n",
    "    elif pd.notna(row['Detailed O*NET-SOC']):\n",
    "        code = row['Detailed O*NET-SOC']\n",
    "    else:\n",
    "        continue\n",
    "    code_label_rows.append({'Code': code, 'Label': row['SOC or O*NET-SOC 2019 Title']})\n",
    "\n",
    "soc_code_label = pd.DataFrame(code_label_rows)\n",
    "soc_code_label.to_csv(f'{output_data_path}/SOC_Code_Label_Mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "1c480f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create industry codes at different hierarchy levels\n",
    "ONET['SOC_Code_7digit'] = ONET['O*NET-SOC Code'].str.split('.').str[0]\n",
    "\n",
    "industry_levels = {\n",
    "    2: 'Major_Group',\n",
    "    5: 'Minor_Group', \n",
    "    6: 'Broad_Occupation',\n",
    "    7: 'Detailed_Occupation'\n",
    "}\n",
    "\n",
    "for num_digits, level_name in industry_levels.items():\n",
    "    if num_digits == 2:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit'].str[:2] + '-0000'\n",
    "    elif num_digits == 5:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit'].str[:5] + '00'\n",
    "    elif num_digits == 6:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit'].str[:6] + '0'\n",
    "    else:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit']\n",
    "\n",
    "# Drop SOC_Code_7digit from columns\n",
    "ONET = ONET.drop(columns=['SOC_Code_7digit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e3e69f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hierarchical Task Counts Summary:\n",
      "  Total rows: 2,179\n",
      "\n",
      "Breakdown by aggregation level:\n",
      "  Major Group: 22\n",
      "  Minor Group: 95\n",
      "  Broad Occupation: 430\n",
      "  Detailed Occupation: 759\n",
      "  Occupation: 873\n"
     ]
    }
   ],
   "source": [
    "# Add title labels for each industry level\n",
    "for num_digits, level_name in industry_levels.items():\n",
    "    code_col = f'{level_name}_Code'\n",
    "    label_col = f'{level_name}_Title'\n",
    "    ONET = ONET.merge(\n",
    "        soc_code_label.rename(columns={'Code': code_col, 'Label': label_col}),\n",
    "        on=code_col,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "# Aggregate unique task and DWA counts at each hierarchical level\n",
    "def aggregate_by_level(df, code_col, title_col, level_name):\n",
    "    \"\"\"\n",
    "    Aggregate unique task and DWA counts for a given hierarchical level.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to aggregate\n",
    "    - code_col: Column name for the code/ID\n",
    "    - title_col: Column name for the title/description\n",
    "    - level_name: Name of the hierarchical level (e.g., 'Major Group')\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns: Level, Code, Title, num_unique_tasks, num_unique_dwas\n",
    "    \"\"\"\n",
    "    agg = (\n",
    "        df.groupby([code_col, title_col])\n",
    "        .agg(\n",
    "            num_unique_tasks=('Task ID', 'nunique'),\n",
    "            num_unique_dwas=('DWA ID', 'nunique') if 'DWA ID' in df.columns else ('Task ID', lambda x: 0)\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={code_col: 'Code', title_col: 'Title'})\n",
    "    )\n",
    "    agg['Level'] = level_name\n",
    "    return agg[['Level', 'Code', 'Title', 'num_unique_tasks', 'num_unique_dwas']]\n",
    "\n",
    "\n",
    "# Create aggregations for each hierarchical level\n",
    "major_group_agg = aggregate_by_level(ONET, 'Major_Group_Code', 'Major_Group_Title', 'Major Group')\n",
    "minor_group_agg = aggregate_by_level(ONET, 'Minor_Group_Code', 'Minor_Group_Title', 'Minor Group')\n",
    "broad_occ_agg = aggregate_by_level(ONET, 'Broad_Occupation_Code', 'Broad_Occupation_Title', 'Broad Occupation')\n",
    "detailed_occ_agg = aggregate_by_level(ONET, 'Detailed_Occupation_Code', 'Detailed_Occupation_Title', 'Detailed Occupation')\n",
    "occupation_agg = aggregate_by_level(ONET, 'O*NET-SOC Code', 'Occupation Title', 'Occupation')\n",
    "\n",
    "# Combine all levels into one dataset\n",
    "hierarchical_task_counts = pd.concat([\n",
    "    major_group_agg,\n",
    "    minor_group_agg,\n",
    "    broad_occ_agg,\n",
    "    detailed_occ_agg,\n",
    "    occupation_agg\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "hierarchical_task_counts.to_csv(f'{output_data_path}/hierarchical_task_counts.csv', index=False)\n",
    "\n",
    "print(f\"\\nHierarchical Task Counts Summary:\")\n",
    "print(f\"  Total rows: {len(hierarchical_task_counts):,}\")\n",
    "print(f\"\\nBreakdown by aggregation level:\")\n",
    "for level in ['Major Group', 'Minor Group', 'Broad Occupation', 'Detailed Occupation', 'Occupation']:\n",
    "    count = len(hierarchical_task_counts[hierarchical_task_counts['Level'] == level])\n",
    "    print(f\"  {level}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a93b5",
   "metadata": {},
   "source": [
    "### Merge DWA Data\n",
    "Now that task time measures are calculated, we can safely merge DWA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a2356885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape before DWA merge: (17953, 24)\n",
      "Dataset shape after DWA merge: (22310, 26)\n",
      "Number of duplicate rows (with DWA ID & Title): 0\n",
      "Number of duplicate rows (without DWA ID & Title): 4357\n",
      "Number of tasks matched to multiple DWAs: 3780\n",
      "Number of tasks matched to multiple DWAs: 3780\n"
     ]
    }
   ],
   "source": [
    "# Merge DWA (Detailed Work Activities) data\n",
    "# This is done AFTER task time calculations to avoid duplication issues\n",
    "print(f\"Dataset shape before DWA merge: {ONET.shape}\")\n",
    "\n",
    "# Merge DWA mappings and titles\n",
    "ONET = ONET.merge(tasks_to_dwas_df[['O*NET-SOC Code', 'Task ID', 'DWA ID']], on=['O*NET-SOC Code', 'Task ID'], how='left')\n",
    "ONET = ONET.merge(dwa_reference_df[['DWA ID', 'DWA Title']], on='DWA ID', how='left')\n",
    "\n",
    "print(f\"Dataset shape after DWA merge: {ONET.shape}\")\n",
    "\n",
    "# Check for duplicates with DWA ID and DWA Title\n",
    "dup_cols_full = ['O*NET-SOC Code', 'Task ID', 'Task Type', 'DWA ID', 'DWA Title']\n",
    "num_duplicates_full = ONET.duplicated(subset=dup_cols_full).sum()\n",
    "print(f\"Number of duplicate rows (with DWA ID & Title): {num_duplicates_full}\")\n",
    "\n",
    "# Check for duplicates without DWA ID and DWA Title\n",
    "dup_cols_task = ['O*NET-SOC Code', 'Task ID', 'Task Type']\n",
    "num_duplicates_task = ONET.duplicated(subset=dup_cols_task).sum()\n",
    "print(f\"Number of duplicate rows (without DWA ID & Title): {num_duplicates_task}\")\n",
    "\n",
    "# Show how many tasks are matched to multiple DWAs\n",
    "task_counts = ONET.groupby(dup_cols_task)['DWA ID'].nunique()\n",
    "multi_dwa_tasks = (task_counts > 1).sum()\n",
    "print(f\"Number of tasks matched to multiple DWAs: {multi_dwa_tasks}\")\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "if num_duplicates_full > 0:\n",
    "    ONET = ONET.drop_duplicates(subset=dup_cols_full).reset_index(drop=True)\n",
    "    print(f\"Removed {num_duplicates_full} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "386a86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DWA repetition dataset across hierarchical levels\n",
    "dwa_repetition_data = []\n",
    "\n",
    "# Filter out rows where DWA ID is null\n",
    "onet_with_dwa = ONET[ONET['DWA ID'].notna()]\n",
    "\n",
    "for dwa_id in onet_with_dwa['DWA ID'].unique():\n",
    "    dwa_data = onet_with_dwa[onet_with_dwa['DWA ID'] == dwa_id]\n",
    "    dwa_title = dwa_data['DWA Title'].iloc[0]\n",
    "    \n",
    "    dwa_repetition_data.append({\n",
    "        'DWA ID': dwa_id,\n",
    "        'DWA Title': dwa_title,\n",
    "        'num_occupations': dwa_data['O*NET-SOC Code'].nunique(),\n",
    "        'num_detailed_occupations': dwa_data['Detailed_Occupation_Code'].nunique(),\n",
    "        'num_broad_occupations': dwa_data['Broad_Occupation_Code'].nunique(),\n",
    "        'num_minor_groups': dwa_data['Minor_Group_Code'].nunique(),\n",
    "        'num_major_groups': dwa_data['Major_Group_Code'].nunique()\n",
    "    })\n",
    "\n",
    "dwa_repetition_df = pd.DataFrame(dwa_repetition_data)\n",
    "dwa_repetition_df.to_csv(f'{output_data_path}/dwa_repetition_by_hierarchy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "fd3c14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create task repetition dataset across hierarchical levels\n",
    "# task_repetition_data = []\n",
    "\n",
    "# for task_id in ONET['Task ID'].unique():\n",
    "#     task_data = ONET[ONET['Task ID'] == task_id]\n",
    "    \n",
    "#     # Get task title (should be consistent for same task ID)\n",
    "#     task_title = task_data['Task Title'].iloc[0]\n",
    "    \n",
    "#     # Count occurrences at each hierarchical level\n",
    "#     task_repetition_data.append({\n",
    "#         'Task ID': task_id,\n",
    "#         'Task Title': task_title,\n",
    "#         'num_occupations': task_data['O*NET-SOC Code'].nunique(),\n",
    "#         'num_detailed_occupations': task_data['Detailed_Occupation_Code'].nunique(),\n",
    "#         'num_broad_occupations': task_data['Broad_Occupation_Code'].nunique(),\n",
    "#         'num_minor_groups': task_data['Minor_Group_Code'].nunique(),\n",
    "#         'num_major_groups': task_data['Major_Group_Code'].nunique()\n",
    "#     })\n",
    "\n",
    "# task_repetition_df = pd.DataFrame(task_repetition_data)\n",
    "\n",
    "# # Save task repetition dataset\n",
    "# task_repetition_df.to_csv(f'{output_data_path}/task_repetition_by_hierarchy.csv', index=False)\n",
    "\n",
    "# print(\"Task Repetition Analysis:\")\n",
    "# print(f\"  Total unique tasks: {len(task_repetition_df):,}\")\n",
    "# print(f\"\\nTasks appearing in multiple occupations:\")\n",
    "# print(f\"  Tasks in 2+ occupations: {(task_repetition_df['num_occupations'] >= 2).sum():,}\")\n",
    "# print(f\"  Tasks in 5+ occupations: {(task_repetition_df['num_occupations'] >= 5).sum():,}\")\n",
    "# print(f\"  Tasks in 10+ occupations: {(task_repetition_df['num_occupations'] >= 10).sum():,}\")\n",
    "# print(f\"\\nMax repetition across levels:\")\n",
    "# print(f\"  Max occupations per task: {task_repetition_df['num_occupations'].max()}\")\n",
    "# print(f\"  Max detailed occupations per task: {task_repetition_df['num_detailed_occupations'].max()}\")\n",
    "# print(f\"  Max broad occupations per task: {task_repetition_df['num_broad_occupations'].max()}\")\n",
    "# print(f\"  Max minor groups per task: {task_repetition_df['num_minor_groups'].max()}\")\n",
    "# print(f\"  Max major groups per task: {task_repetition_df['num_major_groups'].max()}\")\n",
    "# print(f\"\\nSaved to: {output_data_path}/task_repetition_by_hierarchy.csv\")\n",
    "\n",
    "# task_repetition_df.head(10)\n",
    "\n",
    "# # Tasks are unique! ==> All 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "e72818e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved with 22,310 rows\n"
     ]
    }
   ],
   "source": [
    "# Reorder columns for final dataset\n",
    "first_cols = ['O*NET-SOC Code', 'Occupation Title', 'Task ID', 'Task Title', 'Task Type',\n",
    "              'DWA ID', 'DWA Title', 'Job Zone', 'Task_Time_Percentage']\n",
    "last_cols = ['Major_Group_Code', 'Major_Group_Title', 'Minor_Group_Code', 'Minor_Group_Title', \n",
    "             'Broad_Occupation_Code', 'Broad_Occupation_Title', 'Detailed_Occupation_Code', 'Detailed_Occupation_Title']\n",
    "middle_cols = [col for col in ONET.columns if col not in first_cols + last_cols]\n",
    "ONET = ONET[first_cols + middle_cols + last_cols]\n",
    "\n",
    "# Save final dataset\n",
    "ONET.to_csv(f'{output_data_path}/ONET_cleaned_tasks.csv', index=False)\n",
    "print(f\"Final dataset saved with {len(ONET):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "cec7206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (22310, 26)\n",
      "Total rows: 22,310\n",
      "\n",
      "Columns with NaN values:\n",
      "==================================================\n",
      "Minor_Group_Title: 4,231 (18.96%)\n",
      "DWA ID: 425 (1.90%)\n",
      "DWA Title: 425 (1.90%)\n",
      "Broad_Occupation_Title: 215 (0.96%)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: NaN values\n",
    "nan_counts = ONET.isna().sum()\n",
    "nan_summary = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"Dataset shape: {ONET.shape}\")\n",
    "print(f\"Total rows: {len(ONET):,}\\n\")\n",
    "\n",
    "if len(nan_summary) > 0:\n",
    "    print(\"Columns with NaN values:\")\n",
    "    print(\"=\" * 50)\n",
    "    for col, count in nan_summary.items():\n",
    "        percentage = (count / len(ONET)) * 100\n",
    "        print(f\"{col}: {count:,} ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"✓ No NaN values found in any column!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25d661",
   "metadata": {},
   "source": [
    "### Add BLS Wage Data\n",
    "Load and merge Bureau of Labor Statistics wage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading BLS wage data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[427], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m ONET \u001b[38;5;241m=\u001b[39m ONET\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH_MEAN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly_Mean_Wage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA_MEAN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnual_Mean_Wage\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Report results\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m wage_coverage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(ONET[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly_Mean_Wage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     25\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ONET)\n\u001b[1;32m     26\u001b[0m coverage_pct \u001b[38;5;241m=\u001b[39m wage_coverage\u001b[38;5;241m/\u001b[39mtotal_rows\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:248\u001b[0m, in \u001b[0;36m_coerce_method.<locals>.wrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on a single element Series is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will raise a TypeError in the future. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    246\u001b[0m     )\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert the series to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "# Read and clean BLS wage data\n",
    "print(\"Reading BLS wage data...\")\n",
    "bls_wage_df = pd.read_excel(f\"{main_folder_path}/data/oesm23nat/national_M2023_dl.xlsx\")\n",
    "\n",
    "# Clean wage columns - convert non-numeric values to NaN\n",
    "bls_wage_df['H_MEAN'] = pd.to_numeric(bls_wage_df['H_MEAN'], errors='coerce')\n",
    "bls_wage_df['A_MEAN'] = pd.to_numeric(bls_wage_df['A_MEAN'], errors='coerce')\n",
    "\n",
    "# Fill missing wage values using 2080 hours/year conversion\n",
    "missing_h_mean = bls_wage_df['H_MEAN'].isna() & bls_wage_df['A_MEAN'].notna()\n",
    "bls_wage_df.loc[missing_h_mean, 'H_MEAN'] = bls_wage_df.loc[missing_h_mean, 'A_MEAN'] / 2080\n",
    "\n",
    "missing_a_mean = bls_wage_df['A_MEAN'].isna() & bls_wage_df['H_MEAN'].notna()\n",
    "bls_wage_df.loc[missing_a_mean, 'A_MEAN'] = bls_wage_df.loc[missing_a_mean, 'H_MEAN'] * 2080\n",
    "\n",
    "# Merge with ONET dataset\n",
    "ONET['Base_SOC_Code'] = ONET['O*NET-SOC Code'].str.split('.').str[0]\n",
    "wage_data = bls_wage_df[['OCC_CODE', 'H_MEAN', 'A_MEAN']].rename(columns={'OCC_CODE': 'Base_SOC_Code'})\n",
    "\n",
    "ONET = ONET.merge(wage_data, on='Base_SOC_Code', how='left')\n",
    "ONET = ONET.rename(columns={'H_MEAN': 'Hourly_Mean_Wage', 'A_MEAN': 'Annual_Mean_Wage'})\n",
    "\n",
    "# Report results\n",
    "print(\"✓ Wage data merged successfully\")\n",
    "print(f\"✓ Final dataset shape: {ONET.shape}\")\n",
    "print(f\"✓ Contains both wage variables: Hourly_Mean_Wage, Annual_Mean_Wage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns and save final dataset\n",
    "first_cols = ['O*NET-SOC Code', 'Occupation Title', 'Task ID', 'Task Title', 'Task Type',\n",
    "              'DWA ID', 'DWA Title', 'Job Zone', 'Task_Time_Percentage', 'Hourly_Mean_Wage', 'Annual_Mean_Wage']\n",
    "last_cols = ['Major_Group_Code', 'Major_Group_Title', 'Minor_Group_Code', 'Minor_Group_Title', \n",
    "             'Broad_Occupation_Code', 'Broad_Occupation_Title', 'Detailed_Occupation_Code', 'Detailed_Occupation_Title']\n",
    "middle_cols = [col for col in ONET.columns if col not in first_cols + last_cols]\n",
    "\n",
    "ONET = ONET[first_cols + middle_cols + last_cols]\n",
    "\n",
    "# Save final dataset\n",
    "ONET.to_csv(f'{output_data_path}/ONET_cleaned_tasks_with_wages.csv', index=False)\n",
    "print(f\"✓ Final dataset saved: {output_data_path}/ONET_cleaned_tasks_with_wages.csv\")\n",
    "print(f\"✓ Final shape: {ONET.shape}\")\n",
    "print(f\"✓ Contains both wage variables: Hourly_Mean_Wage, Annual_Mean_Wage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
