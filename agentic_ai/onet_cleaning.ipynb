{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 10, 2025\n",
    "#### Last Edit: Oct 29, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be56448",
   "metadata": {},
   "source": [
    "### O*NET Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all datasets\n",
    "task_ratings_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Task Ratings.txt', sep='\\t')\n",
    "task_categories_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Task Categories.txt', sep='\\t')\n",
    "tasks_to_dwas_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Tasks to DWAs.txt', sep='\\t')\n",
    "dwa_reference_df = pd.read_csv(f'{input_data_path}/db_27_3_text/DWA Reference.txt', sep='\\t')\n",
    "job_zones_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Job Zones.txt', sep='\\t')\n",
    "task_statements_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Task Statements.txt', sep='\\t')\n",
    "occupation_data_df = pd.read_csv(f'{input_data_path}/db_27_3_text/Occupation Data.txt', sep='\\t')\n",
    "soc_structure_df = pd.read_csv(f'{input_data_path}/SOC_Structure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dataset created with 17,953 rows before task time calculations\n"
     ]
    }
   ],
   "source": [
    "# Merge and transform O*NET task data\n",
    "ONET = task_ratings_df.merge(task_categories_df, on=['Scale ID', 'Category'], how='left')\n",
    "\n",
    "# Process Category Description\n",
    "ONET['Category Description'] = ONET['Category Description'].apply(lambda x: f'FT_{x}' if pd.notna(x) else x)\n",
    "ONET['Category Description'] = ONET['Category Description'].fillna(ONET['Scale ID'])\n",
    "ONET['Category Description'] = ONET['Category Description'].replace({'IM': 'Importance', 'RT': 'Relevance'})\n",
    "\n",
    "# Reshape from long to wide format\n",
    "ONET = ONET.pivot_table(\n",
    "    index=['O*NET-SOC Code', 'Task ID'],\n",
    "    columns='Category Description',\n",
    "    values='Data Value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "ONET.columns.name = None\n",
    "\n",
    "# Merge essential data (but NOT DWA data yet - save that for after task time calculations)\n",
    "ONET = ONET.merge(occupation_data_df[['O*NET-SOC Code', 'Title']], on='O*NET-SOC Code', how='left')\n",
    "ONET.rename(columns={'Title': 'Occupation Title'}, inplace=True)\n",
    "\n",
    "ONET = ONET.merge(task_statements_df[['O*NET-SOC Code', 'Task ID', 'Task', 'Task Type']], on=['O*NET-SOC Code', 'Task ID'], how='left')\n",
    "ONET.rename(columns={'Task': 'Task Title'}, inplace=True)\n",
    "\n",
    "ONET = ONET.merge(job_zones_df[['O*NET-SOC Code', 'Job Zone']], on='O*NET-SOC Code', how='left')\n",
    "\n",
    "print(f\"Base dataset created with {len(ONET):,} rows before task time calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d947ef",
   "metadata": {},
   "source": [
    "### Filter Occupations Containing \"All Other\" and \"Teachers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71645324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove rows where occupation title contains \"All Other\"\n",
    "# print(f\"Number of rows before removing 'All Other': {ONET.shape[0]:,}\")\n",
    "# ONET = ONET[~ONET[\"Occupation Title\"].str.contains(\"All Other\", case=False, na=False)]\n",
    "# print(f\"Number of rows after removing 'All Other': {ONET.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter and remove \"Teachers\"-related occupations\n",
    "# contains_teacher = ONET[ONET['Occupation Title'].str.contains('Teachers', case=False, na=False)]\n",
    "\n",
    "# unique_teacher_occupations = contains_teacher['Occupation Title'].nunique()\n",
    "# print(f'Number of unique occupations containing the word \"Teachers\": {unique_teacher_occupations}')\n",
    "\n",
    "# # Remove rows that contain \"Teacher\" (case-insensitive)\n",
    "# ONET = ONET[~ONET['Occupation Title'].str.contains('Teachers', case=False, na=False)].reset_index(drop=True)\n",
    "# print(f\"Rows after removing Teachers: {len(ONET):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335b787",
   "metadata": {},
   "source": [
    "### Task Time Measurement Calculation\n",
    "**IMPORTANT**: Calculate task time measures BEFORE merging DWA data to avoid duplication issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Time Measurement Complete!\n",
      "✓ 17,953 tasks processed across 873 occupations\n",
      "✓ 17,953 tasks have valid time measures\n"
     ]
    }
   ],
   "source": [
    "# Task Time Measurement Creation\n",
    "# ==============================\n",
    "\n",
    "# Step 1: Define frequency mappings (annual occurrence rates)\n",
    "frequency_mapping = {\n",
    "    'FT_Several times daily': 1000,      # ~4 times/day × 250 work days\n",
    "    'FT_Hourly or more': 2000,           # Multiple times per hour\n",
    "    'FT_Daily': 250,                      # Once per day\n",
    "    'FT_More than weekly': 156,          # ~3 times/week × 52 weeks\n",
    "    'FT_More than monthly': 24,          # ~2 times/month × 12 months\n",
    "    'FT_More than yearly': 6,            # A few times per year\n",
    "    'FT_Yearly or less': 1               # Once a year\n",
    "}\n",
    "\n",
    "# Step 2: Calculate weighted frequency using percentage distributions\n",
    "def calculate_weighted_frequency(row):\n",
    "    \"\"\"Calculate weighted annual frequency based on percentage distributions.\"\"\"\n",
    "    total_weighted_freq = 0.0\n",
    "    \n",
    "    for freq_col, freq_value in frequency_mapping.items():\n",
    "        if freq_col in row.index and pd.notna(row[freq_col]):\n",
    "            percentage = row[freq_col] / 100.0  # Convert percentage to decimal\n",
    "            total_weighted_freq += freq_value * percentage\n",
    "    \n",
    "    return total_weighted_freq if total_weighted_freq > 0 else np.nan\n",
    "\n",
    "ONET['Weighted_Frequency_Annual'] = ONET.apply(calculate_weighted_frequency, axis=1)\n",
    "\n",
    "# Step 3: Calculate raw task score (frequency × importance × relevance)\n",
    "ONET['Raw_Task_Score'] = (ONET['Weighted_Frequency_Annual'] * \n",
    "                         ONET['Importance'] * \n",
    "                         ONET['Relevance'])\n",
    "\n",
    "# Step 4: Normalize within each occupation (CRITICAL STEP)\n",
    "# Each occupation represents a full-time job (40 hours/week)\n",
    "# Task proportions within each occupation must sum to 1.0 (100% of work time)\n",
    "ONET['Total_Raw_Score_by_Occupation'] = ONET.groupby('O*NET-SOC Code')['Raw_Task_Score'].transform('sum')\n",
    "ONET['Task_Time_Proportion'] = ONET['Raw_Task_Score'] / ONET['Total_Raw_Score_by_Occupation']\n",
    "ONET['Task_Time_Percentage'] = ONET['Task_Time_Proportion'] * 100\n",
    "\n",
    "# # Step 5: Convert to time estimates \n",
    "# # Full-time job = 40 hours/week = 2000 hours/year (50 weeks)\n",
    "# ONET['Hours_Per_Week'] = ONET['Task_Time_Proportion'] * 40  # Convert proportion to hours per week\n",
    "# ONET['Estimated_Annual_Hours'] = ONET['Task_Time_Proportion'] * 2000  # Annual hours\n",
    "# ONET['Hours_Per_Occurrence'] = np.where(\n",
    "#     ONET['Weighted_Frequency_Annual'] > 0,\n",
    "#     ONET['Estimated_Annual_Hours'] / ONET['Weighted_Frequency_Annual'],\n",
    "#     np.nan\n",
    "# )\n",
    "\n",
    "# Step 6: Drop \"Raw_Task_Score\", \"Total_Raw_Score_by_Occupation\", \"Task_Time_Proportion\"\n",
    "ONET = ONET.drop(columns=[\"Raw_Task_Score\", \"Total_Raw_Score_by_Occupation\", \"Task_Time_Proportion\", \"Weighted_Frequency_Annual\"])\n",
    "\n",
    "print(\"Task Time Measurement Complete!\")\n",
    "print(f\"✓ {len(ONET):,} tasks processed across {ONET['O*NET-SOC Code'].nunique():,} occupations\")\n",
    "print(f\"✓ {ONET['Task_Time_Percentage'].gt(0).sum():,} tasks have valid time measures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4a9cd",
   "metadata": {},
   "source": [
    "### Add SOC Industry Levels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c842646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SOC Code-to-Label mapping\n",
    "code_label_rows = []\n",
    "for idx, row in soc_structure_df.iterrows():\n",
    "    if pd.notna(row['Major Group']):\n",
    "        code = row['Major Group']\n",
    "    elif pd.notna(row['Minor Group']):\n",
    "        code = row['Minor Group']\n",
    "    elif pd.notna(row['Broad Occupation']):\n",
    "        code = row['Broad Occupation']\n",
    "    elif pd.notna(row['Detailed Occupation']):\n",
    "        code = row['Detailed Occupation']\n",
    "    elif pd.notna(row['Detailed O*NET-SOC']):\n",
    "        code = row['Detailed O*NET-SOC']\n",
    "    else:\n",
    "        continue\n",
    "    code_label_rows.append({'Code': code, 'Label': row['SOC or O*NET-SOC 2019 Title']})\n",
    "\n",
    "soc_code_label = pd.DataFrame(code_label_rows)\n",
    "soc_code_label.to_csv(f'{output_data_path}/SOC_Code_Label_Mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c480f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create industry codes at different hierarchy levels\n",
    "ONET['SOC_Code_7digit'] = ONET['O*NET-SOC Code'].str.split('.').str[0]\n",
    "\n",
    "industry_levels = {\n",
    "    2: 'Major_Group',\n",
    "    5: 'Minor_Group', \n",
    "    6: 'Broad_Occupation',\n",
    "    7: 'Detailed_Occupation'\n",
    "}\n",
    "\n",
    "for num_digits, level_name in industry_levels.items():\n",
    "    if num_digits == 2:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit'].str[:2] + '-0000'\n",
    "    elif num_digits == 5:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit'].str[:5] + '00'\n",
    "    elif num_digits == 6:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit'].str[:6] + '0'\n",
    "    else:\n",
    "        ONET[f'{level_name}_Code'] = ONET['SOC_Code_7digit']\n",
    "\n",
    "# Drop SOC_Code_7digit from columns\n",
    "ONET = ONET.drop(columns=['SOC_Code_7digit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e69f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hierarchical Task Counts Summary:\n",
      "  Total rows: 2,179\n",
      "\n",
      "Breakdown by aggregation level:\n",
      "  Major Group: 22\n",
      "  Minor Group: 95\n",
      "  Broad Occupation: 430\n",
      "  Detailed Occupation: 759\n",
      "  Occupation: 873\n"
     ]
    }
   ],
   "source": [
    "# Add title labels for each industry level\n",
    "for num_digits, level_name in industry_levels.items():\n",
    "    code_col = f'{level_name}_Code'\n",
    "    label_col = f'{level_name}_Title'\n",
    "    ONET = ONET.merge(\n",
    "        soc_code_label.rename(columns={'Code': code_col, 'Label': label_col}),\n",
    "        on=code_col,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "# Aggregate unique task and DWA counts at each hierarchical level\n",
    "def aggregate_by_level(df, code_col, title_col, level_name):\n",
    "    \"\"\"\n",
    "    Aggregate unique task and DWA counts for a given hierarchical level.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to aggregate\n",
    "    - code_col: Column name for the code/ID\n",
    "    - title_col: Column name for the title/description\n",
    "    - level_name: Name of the hierarchical level (e.g., 'Major Group')\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns: Level, Code, Title, num_unique_tasks, num_unique_dwas\n",
    "    \"\"\"\n",
    "    agg = (\n",
    "        df.groupby([code_col, title_col])\n",
    "        .agg(\n",
    "            num_unique_tasks=('Task ID', 'nunique'),\n",
    "            num_unique_dwas=('DWA ID', 'nunique') if 'DWA ID' in df.columns else ('Task ID', lambda x: 0)\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={code_col: 'Code', title_col: 'Title'})\n",
    "    )\n",
    "    agg['Level'] = level_name\n",
    "    return agg[['Level', 'Code', 'Title', 'num_unique_tasks', 'num_unique_dwas']]\n",
    "\n",
    "\n",
    "# Create aggregations for each hierarchical level\n",
    "major_group_agg = aggregate_by_level(ONET, 'Major_Group_Code', 'Major_Group_Title', 'Major Group')\n",
    "minor_group_agg = aggregate_by_level(ONET, 'Minor_Group_Code', 'Minor_Group_Title', 'Minor Group')\n",
    "broad_occ_agg = aggregate_by_level(ONET, 'Broad_Occupation_Code', 'Broad_Occupation_Title', 'Broad Occupation')\n",
    "detailed_occ_agg = aggregate_by_level(ONET, 'Detailed_Occupation_Code', 'Detailed_Occupation_Title', 'Detailed Occupation')\n",
    "occupation_agg = aggregate_by_level(ONET, 'O*NET-SOC Code', 'Occupation Title', 'Occupation')\n",
    "\n",
    "# Combine all levels into one dataset\n",
    "hierarchical_task_counts = pd.concat([\n",
    "    major_group_agg,\n",
    "    minor_group_agg,\n",
    "    broad_occ_agg,\n",
    "    detailed_occ_agg,\n",
    "    occupation_agg\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "hierarchical_task_counts.to_csv(f'{output_data_path}/hierarchical_task_counts.csv', index=False)\n",
    "\n",
    "print(f\"\\nHierarchical Task Counts Summary:\")\n",
    "print(f\"  Total rows: {len(hierarchical_task_counts):,}\")\n",
    "print(f\"\\nBreakdown by aggregation level:\")\n",
    "for level in ['Major Group', 'Minor Group', 'Broad Occupation', 'Detailed Occupation', 'Occupation']:\n",
    "    count = len(hierarchical_task_counts[hierarchical_task_counts['Level'] == level])\n",
    "    print(f\"  {level}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a93b5",
   "metadata": {},
   "source": [
    "### Merge DWA Data\n",
    "Now that task time measures are calculated, we can safely merge DWA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2356885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape before DWA merge: (17953, 24)\n",
      "Dataset shape after DWA merge: (22310, 26)\n",
      "Number of duplicate rows (with DWA ID & Title): 0\n",
      "Number of duplicate rows (without DWA ID & Title): 4357\n",
      "Number of tasks matched to multiple DWAs: 3780\n"
     ]
    }
   ],
   "source": [
    "# Merge DWA (Detailed Work Activities) data\n",
    "# This is done AFTER task time calculations to avoid duplication issues\n",
    "print(f\"Dataset shape before DWA merge: {ONET.shape}\")\n",
    "\n",
    "# Merge DWA mappings and titles\n",
    "ONET = ONET.merge(tasks_to_dwas_df[['O*NET-SOC Code', 'Task ID', 'DWA ID']], on=['O*NET-SOC Code', 'Task ID'], how='left')\n",
    "ONET = ONET.merge(dwa_reference_df[['DWA ID', 'DWA Title']], on='DWA ID', how='left')\n",
    "\n",
    "print(f\"Dataset shape after DWA merge: {ONET.shape}\")\n",
    "\n",
    "# Check for duplicates with DWA ID and DWA Title\n",
    "dup_cols_full = ['O*NET-SOC Code', 'Task ID', 'Task Type', 'DWA ID', 'DWA Title']\n",
    "num_duplicates_full = ONET.duplicated(subset=dup_cols_full).sum()\n",
    "print(f\"Number of duplicate rows (with DWA ID & Title): {num_duplicates_full}\")\n",
    "\n",
    "# Check for duplicates without DWA ID and DWA Title\n",
    "dup_cols_task = ['O*NET-SOC Code', 'Task ID', 'Task Type']\n",
    "num_duplicates_task = ONET.duplicated(subset=dup_cols_task).sum()\n",
    "print(f\"Number of duplicate rows (without DWA ID & Title): {num_duplicates_task}\")\n",
    "\n",
    "# Show how many tasks are matched to multiple DWAs\n",
    "task_counts = ONET.groupby(dup_cols_task)['DWA ID'].nunique()\n",
    "multi_dwa_tasks = (task_counts > 1).sum()\n",
    "print(f\"Number of tasks matched to multiple DWAs: {multi_dwa_tasks}\")\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "if num_duplicates_full > 0:\n",
    "    ONET = ONET.drop_duplicates(subset=dup_cols_full).reset_index(drop=True)\n",
    "    print(f\"Removed {num_duplicates_full} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DWA repetition dataset across hierarchical levels\n",
    "dwa_repetition_data = []\n",
    "\n",
    "# Filter out rows where DWA ID is null\n",
    "onet_with_dwa = ONET[ONET['DWA ID'].notna()]\n",
    "\n",
    "for dwa_id in onet_with_dwa['DWA ID'].unique():\n",
    "    dwa_data = onet_with_dwa[onet_with_dwa['DWA ID'] == dwa_id]\n",
    "    dwa_title = dwa_data['DWA Title'].iloc[0]\n",
    "    \n",
    "    dwa_repetition_data.append({\n",
    "        'DWA ID': dwa_id,\n",
    "        'DWA Title': dwa_title,\n",
    "        'num_occupations': dwa_data['O*NET-SOC Code'].nunique(),\n",
    "        'num_detailed_occupations': dwa_data['Detailed_Occupation_Code'].nunique(),\n",
    "        'num_broad_occupations': dwa_data['Broad_Occupation_Code'].nunique(),\n",
    "        'num_minor_groups': dwa_data['Minor_Group_Code'].nunique(),\n",
    "        'num_major_groups': dwa_data['Major_Group_Code'].nunique()\n",
    "    })\n",
    "\n",
    "dwa_repetition_df = pd.DataFrame(dwa_repetition_data)\n",
    "dwa_repetition_df.to_csv(f'{output_data_path}/dwa_repetition_by_hierarchy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create task repetition dataset across hierarchical levels\n",
    "# task_repetition_data = []\n",
    "\n",
    "# for task_id in ONET['Task ID'].unique():\n",
    "#     task_data = ONET[ONET['Task ID'] == task_id]\n",
    "    \n",
    "#     # Get task title (should be consistent for same task ID)\n",
    "#     task_title = task_data['Task Title'].iloc[0]\n",
    "    \n",
    "#     # Count occurrences at each hierarchical level\n",
    "#     task_repetition_data.append({\n",
    "#         'Task ID': task_id,\n",
    "#         'Task Title': task_title,\n",
    "#         'num_occupations': task_data['O*NET-SOC Code'].nunique(),\n",
    "#         'num_detailed_occupations': task_data['Detailed_Occupation_Code'].nunique(),\n",
    "#         'num_broad_occupations': task_data['Broad_Occupation_Code'].nunique(),\n",
    "#         'num_minor_groups': task_data['Minor_Group_Code'].nunique(),\n",
    "#         'num_major_groups': task_data['Major_Group_Code'].nunique()\n",
    "#     })\n",
    "\n",
    "# task_repetition_df = pd.DataFrame(task_repetition_data)\n",
    "\n",
    "# # Save task repetition dataset\n",
    "# task_repetition_df.to_csv(f'{output_data_path}/task_repetition_by_hierarchy.csv', index=False)\n",
    "\n",
    "# print(\"Task Repetition Analysis:\")\n",
    "# print(f\"  Total unique tasks: {len(task_repetition_df):,}\")\n",
    "# print(f\"\\nTasks appearing in multiple occupations:\")\n",
    "# print(f\"  Tasks in 2+ occupations: {(task_repetition_df['num_occupations'] >= 2).sum():,}\")\n",
    "# print(f\"  Tasks in 5+ occupations: {(task_repetition_df['num_occupations'] >= 5).sum():,}\")\n",
    "# print(f\"  Tasks in 10+ occupations: {(task_repetition_df['num_occupations'] >= 10).sum():,}\")\n",
    "# print(f\"\\nMax repetition across levels:\")\n",
    "# print(f\"  Max occupations per task: {task_repetition_df['num_occupations'].max()}\")\n",
    "# print(f\"  Max detailed occupations per task: {task_repetition_df['num_detailed_occupations'].max()}\")\n",
    "# print(f\"  Max broad occupations per task: {task_repetition_df['num_broad_occupations'].max()}\")\n",
    "# print(f\"  Max minor groups per task: {task_repetition_df['num_minor_groups'].max()}\")\n",
    "# print(f\"  Max major groups per task: {task_repetition_df['num_major_groups'].max()}\")\n",
    "# print(f\"\\nSaved to: {output_data_path}/task_repetition_by_hierarchy.csv\")\n",
    "\n",
    "# task_repetition_df.head(10)\n",
    "\n",
    "# # Tasks are unique! ==> All 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25d661",
   "metadata": {},
   "source": [
    "### Add BLS Wage Data\n",
    "Load and merge Bureau of Labor Statistics wage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading BLS wage data...\n",
      "✓ Wage data merged successfully\n",
      "✓ Final dataset shape: (22310, 39)\n",
      "✓ Added 12 wage variables:\n",
      "  H_MEAN → Hourly_Mean_Wage\n",
      "  H_PCT10 → Hourly_P10_Wage\n",
      "  H_PCT25 → Hourly_P25_Wage\n",
      "  H_MEDIAN → Hourly_Median_Wage\n",
      "  H_PCT75 → Hourly_P75_Wage\n",
      "  H_PCT90 → Hourly_P90_Wage\n",
      "  A_MEAN → Annual_Mean_Wage\n",
      "  A_PCT10 → Annual_P10_Wage\n",
      "  A_PCT25 → Annual_P25_Wage\n",
      "  A_MEDIAN → Annual_Median_Wage\n",
      "  A_PCT75 → Annual_P75_Wage\n",
      "  A_PCT90 → Annual_P90_Wage\n",
      "✓ Annual wage data was used to fill missing hourly wages when available\n"
     ]
    }
   ],
   "source": [
    "# Read and clean BLS wage data\n",
    "print(\"Reading BLS wage data...\")\n",
    "bls_wage_df = pd.read_excel(f\"{main_folder_path}/data/oesm23nat/national_M2023_dl.xlsx\")\n",
    "\n",
    "# Define all wage columns to process (both hourly and annual)\n",
    "hourly_wage_cols = ['H_MEAN', 'H_PCT10', 'H_PCT25', 'H_MEDIAN', 'H_PCT75', 'H_PCT90']\n",
    "annual_wage_cols = ['A_MEAN', 'A_PCT10', 'A_PCT25', 'A_MEDIAN', 'A_PCT75', 'A_PCT90']\n",
    "all_wage_cols = hourly_wage_cols + annual_wage_cols\n",
    "\n",
    "# Clean all wage columns - convert non-numeric values to NaN\n",
    "for col in all_wage_cols:\n",
    "    bls_wage_df[col] = pd.to_numeric(bls_wage_df[col], errors='coerce')\n",
    "\n",
    "# Fill missing hourly wage values using annual wage (convert using 2080 hours/year)\n",
    "for h_col, a_col in zip(hourly_wage_cols, annual_wage_cols):\n",
    "    missing_hourly = bls_wage_df[h_col].isna() & bls_wage_df[a_col].notna()\n",
    "    bls_wage_df.loc[missing_hourly, h_col] = bls_wage_df.loc[missing_hourly, a_col] / 2080\n",
    "\n",
    "# Create mapping for column renaming\n",
    "wage_column_mapping = {\n",
    "    'H_MEAN': 'Hourly_Mean_Wage',\n",
    "    'H_PCT10': 'Hourly_P10_Wage',\n",
    "    'H_PCT25': 'Hourly_P25_Wage', \n",
    "    'H_MEDIAN': 'Hourly_Median_Wage',\n",
    "    'H_PCT75': 'Hourly_P75_Wage',\n",
    "    'H_PCT90': 'Hourly_P90_Wage',\n",
    "    'A_MEAN': 'Annual_Mean_Wage',\n",
    "    'A_PCT10': 'Annual_P10_Wage',\n",
    "    'A_PCT25': 'Annual_P25_Wage',\n",
    "    'A_MEDIAN': 'Annual_Median_Wage', \n",
    "    'A_PCT75': 'Annual_P75_Wage',\n",
    "    'A_PCT90': 'Annual_P90_Wage'\n",
    "}\n",
    "\n",
    "# Merge with ONET dataset - include all wage percentiles\n",
    "ONET['Base_SOC_Code'] = ONET['O*NET-SOC Code'].str.split('.').str[0]\n",
    "wage_data = bls_wage_df[['OCC_CODE'] + all_wage_cols].rename(columns={'OCC_CODE': 'Base_SOC_Code'})\n",
    "wage_data = wage_data.rename(columns=wage_column_mapping)\n",
    "\n",
    "ONET = ONET.merge(wage_data, on='Base_SOC_Code', how='left')\n",
    "\n",
    "# Report results\n",
    "print(\"✓ Wage data merged successfully\")\n",
    "print(f\"✓ Final dataset shape: {ONET.shape}\")\n",
    "print(f\"✓ Added {len(wage_column_mapping)} wage variables:\")\n",
    "for old_col, new_col in wage_column_mapping.items():\n",
    "    print(f\"  {old_col} → {new_col}\")\n",
    "print(f\"✓ Annual wage data was used to fill missing hourly wages when available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c615844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLS Wage Data Columns:\n",
      "['AREA_TITLE', 'AREA_TYPE', 'PCT_TOTAL', 'PCT_RPT', 'H_MEAN', 'A_MEAN', 'H_PCT10', 'H_PCT25', 'H_MEDIAN', 'H_PCT75', 'H_PCT90', 'A_PCT10', 'A_PCT25', 'A_MEDIAN', 'A_PCT75', 'A_PCT90']\n",
      "\n",
      "BLS dataset shape: (1403, 32)\n",
      "Total columns: 32\n",
      "\n",
      "Sample of wage data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA_TITLE</th>\n",
       "      <th>AREA_TYPE</th>\n",
       "      <th>PCT_TOTAL</th>\n",
       "      <th>PCT_RPT</th>\n",
       "      <th>H_MEAN</th>\n",
       "      <th>A_MEAN</th>\n",
       "      <th>H_PCT10</th>\n",
       "      <th>H_PCT25</th>\n",
       "      <th>H_MEDIAN</th>\n",
       "      <th>H_PCT75</th>\n",
       "      <th>H_PCT90</th>\n",
       "      <th>A_PCT10</th>\n",
       "      <th>A_PCT25</th>\n",
       "      <th>A_MEDIAN</th>\n",
       "      <th>A_PCT75</th>\n",
       "      <th>A_PCT90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.48</td>\n",
       "      <td>65,470.00</td>\n",
       "      <td>13.97</td>\n",
       "      <td>17.14</td>\n",
       "      <td>23.11</td>\n",
       "      <td>37.01</td>\n",
       "      <td>58.40</td>\n",
       "      <td>29,050.00</td>\n",
       "      <td>35,660.00</td>\n",
       "      <td>48,060.00</td>\n",
       "      <td>76,980.00</td>\n",
       "      <td>121,470.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.23</td>\n",
       "      <td>137,750.00</td>\n",
       "      <td>26.23</td>\n",
       "      <td>37.66</td>\n",
       "      <td>56.19</td>\n",
       "      <td>81.29</td>\n",
       "      <td>111.36</td>\n",
       "      <td>54,550.00</td>\n",
       "      <td>78,330.00</td>\n",
       "      <td>116,880.00</td>\n",
       "      <td>169,090.00</td>\n",
       "      <td>231,620.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.43</td>\n",
       "      <td>136,100.00</td>\n",
       "      <td>22.31</td>\n",
       "      <td>31.81</td>\n",
       "      <td>49.74</td>\n",
       "      <td>79.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46,400.00</td>\n",
       "      <td>66,170.00</td>\n",
       "      <td>103,460.00</td>\n",
       "      <td>165,500.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.47</td>\n",
       "      <td>258,900.00</td>\n",
       "      <td>38.46</td>\n",
       "      <td>62.90</td>\n",
       "      <td>99.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80,000.00</td>\n",
       "      <td>130,840.00</td>\n",
       "      <td>206,680.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.47</td>\n",
       "      <td>258,900.00</td>\n",
       "      <td>38.46</td>\n",
       "      <td>62.90</td>\n",
       "      <td>99.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80,000.00</td>\n",
       "      <td>130,840.00</td>\n",
       "      <td>206,680.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.18</td>\n",
       "      <td>129,330.00</td>\n",
       "      <td>22.28</td>\n",
       "      <td>31.34</td>\n",
       "      <td>48.69</td>\n",
       "      <td>77.06</td>\n",
       "      <td>111.59</td>\n",
       "      <td>46,340.00</td>\n",
       "      <td>65,180.00</td>\n",
       "      <td>101,280.00</td>\n",
       "      <td>160,290.00</td>\n",
       "      <td>232,110.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.18</td>\n",
       "      <td>129,330.00</td>\n",
       "      <td>22.28</td>\n",
       "      <td>31.34</td>\n",
       "      <td>48.69</td>\n",
       "      <td>77.06</td>\n",
       "      <td>111.59</td>\n",
       "      <td>46,340.00</td>\n",
       "      <td>65,180.00</td>\n",
       "      <td>101,280.00</td>\n",
       "      <td>160,290.00</td>\n",
       "      <td>232,110.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.76</td>\n",
       "      <td>68,140.00</td>\n",
       "      <td>10.10</td>\n",
       "      <td>13.85</td>\n",
       "      <td>22.74</td>\n",
       "      <td>39.52</td>\n",
       "      <td>62.26</td>\n",
       "      <td>21,010.00</td>\n",
       "      <td>28,810.00</td>\n",
       "      <td>47,290.00</td>\n",
       "      <td>82,200.00</td>\n",
       "      <td>129,510.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.76</td>\n",
       "      <td>68,140.00</td>\n",
       "      <td>10.10</td>\n",
       "      <td>13.85</td>\n",
       "      <td>22.74</td>\n",
       "      <td>39.52</td>\n",
       "      <td>62.26</td>\n",
       "      <td>21,010.00</td>\n",
       "      <td>28,810.00</td>\n",
       "      <td>47,290.00</td>\n",
       "      <td>82,200.00</td>\n",
       "      <td>129,510.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.90</td>\n",
       "      <td>159,960.00</td>\n",
       "      <td>33.26</td>\n",
       "      <td>47.68</td>\n",
       "      <td>67.23</td>\n",
       "      <td>96.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69,170.00</td>\n",
       "      <td>99,180.00</td>\n",
       "      <td>139,850.00</td>\n",
       "      <td>199,990.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AREA_TITLE  AREA_TYPE  PCT_TOTAL  PCT_RPT  H_MEAN     A_MEAN  H_PCT10  \\\n",
       "0       U.S.          1        NaN      NaN   31.48  65,470.00    13.97   \n",
       "1       U.S.          1        NaN      NaN   66.23 137,750.00    26.23   \n",
       "2       U.S.          1        NaN      NaN   65.43 136,100.00    22.31   \n",
       "3       U.S.          1        NaN      NaN  124.47 258,900.00    38.46   \n",
       "4       U.S.          1        NaN      NaN  124.47 258,900.00    38.46   \n",
       "5       U.S.          1        NaN      NaN   62.18 129,330.00    22.28   \n",
       "6       U.S.          1        NaN      NaN   62.18 129,330.00    22.28   \n",
       "7       U.S.          1        NaN      NaN   32.76  68,140.00    10.10   \n",
       "8       U.S.          1        NaN      NaN   32.76  68,140.00    10.10   \n",
       "9       U.S.          1        NaN      NaN   76.90 159,960.00    33.26   \n",
       "\n",
       "   H_PCT25  H_MEDIAN  H_PCT75  H_PCT90   A_PCT10    A_PCT25   A_MEDIAN  \\\n",
       "0    17.14     23.11    37.01    58.40 29,050.00  35,660.00  48,060.00   \n",
       "1    37.66     56.19    81.29   111.36 54,550.00  78,330.00 116,880.00   \n",
       "2    31.81     49.74    79.57      NaN 46,400.00  66,170.00 103,460.00   \n",
       "3    62.90     99.37      NaN      NaN 80,000.00 130,840.00 206,680.00   \n",
       "4    62.90     99.37      NaN      NaN 80,000.00 130,840.00 206,680.00   \n",
       "5    31.34     48.69    77.06   111.59 46,340.00  65,180.00 101,280.00   \n",
       "6    31.34     48.69    77.06   111.59 46,340.00  65,180.00 101,280.00   \n",
       "7    13.85     22.74    39.52    62.26 21,010.00  28,810.00  47,290.00   \n",
       "8    13.85     22.74    39.52    62.26 21,010.00  28,810.00  47,290.00   \n",
       "9    47.68     67.23    96.15      NaN 69,170.00  99,180.00 139,850.00   \n",
       "\n",
       "     A_PCT75    A_PCT90  \n",
       "0  76,980.00 121,470.00  \n",
       "1 169,090.00 231,620.00  \n",
       "2 165,500.00        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  \n",
       "5 160,290.00 232,110.00  \n",
       "6 160,290.00 232,110.00  \n",
       "7  82,200.00 129,510.00  \n",
       "8  82,200.00 129,510.00  \n",
       "9 199,990.00        NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available wage columns in BLS data\n",
    "print(\"BLS Wage Data Columns:\")\n",
    "wage_columns = [col for col in bls_wage_df.columns if any(x in col.upper() for x in ['WAGE', 'H_', 'A_', 'PCT'])]\n",
    "print(wage_columns)\n",
    "\n",
    "print(f\"\\nBLS dataset shape: {bls_wage_df.shape}\")\n",
    "print(f\"Total columns: {len(bls_wage_df.columns)}\")\n",
    "\n",
    "# Show first few rows of wage-related columns\n",
    "print(\"\\nSample of wage data:\")\n",
    "bls_wage_df[wage_columns].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up duplicate wage columns...\n",
      "✓ Final dataset shape: (22310, 39)\n",
      "✓ Wage columns included: 12\n",
      "✓ Saved to: ../data/computed_objects/ONET_cleaned_tasks.csv\n",
      "\n",
      "Final wage columns in dataset:\n",
      "  1. Hourly_Mean_Wage\n",
      "  2. Hourly_P10_Wage\n",
      "  3. Hourly_P25_Wage\n",
      "  4. Hourly_Median_Wage\n",
      "  5. Hourly_P75_Wage\n",
      "  6. Hourly_P90_Wage\n",
      "  7. Annual_Mean_Wage\n",
      "  8. Annual_P10_Wage\n",
      "  9. Annual_P25_Wage\n",
      "  10. Annual_Median_Wage\n",
      "  11. Annual_P75_Wage\n",
      "  12. Annual_P90_Wage\n"
     ]
    }
   ],
   "source": [
    "# Clean up duplicate wage columns\n",
    "print(\"Cleaning up duplicate wage columns...\")\n",
    "\n",
    "# Handle the duplicate Hourly_Mean_Wage columns\n",
    "if 'Hourly_Mean_Wage_x' in ONET.columns and 'Hourly_Mean_Wage_y' in ONET.columns:\n",
    "    # Use the new one (_y) and drop the old one (_x)\n",
    "    ONET['Hourly_Mean_Wage'] = ONET['Hourly_Mean_Wage_y']\n",
    "    ONET = ONET.drop(columns=['Hourly_Mean_Wage_x', 'Hourly_Mean_Wage_y'])\n",
    "    print(\"✓ Merged duplicate Hourly_Mean_Wage columns\")\n",
    "\n",
    "# Define final wage columns in proper order\n",
    "wage_cols = ['Hourly_Mean_Wage', 'Hourly_P10_Wage', 'Hourly_P25_Wage', 'Hourly_Median_Wage', \n",
    "             'Hourly_P75_Wage', 'Hourly_P90_Wage', 'Annual_Mean_Wage', 'Annual_P10_Wage', \n",
    "             'Annual_P25_Wage', 'Annual_Median_Wage', 'Annual_P75_Wage', 'Annual_P90_Wage']\n",
    "\n",
    "# Filter for existing wage columns\n",
    "existing_wage_cols = [col for col in wage_cols if col in ONET.columns]\n",
    "\n",
    "# Reorder columns properly\n",
    "first_cols = ['O*NET-SOC Code', 'Occupation Title', 'Task ID', 'Task Title', 'Task Type',\n",
    "              'DWA ID', 'DWA Title', 'Job Zone', 'Task_Time_Percentage']\n",
    "\n",
    "last_cols = ['Major_Group_Code', 'Major_Group_Title', 'Minor_Group_Code', 'Minor_Group_Title', \n",
    "             'Broad_Occupation_Code', 'Broad_Occupation_Title', 'Detailed_Occupation_Code', 'Detailed_Occupation_Title']\n",
    "\n",
    "# Filter for existing columns only\n",
    "existing_first_cols = [col for col in first_cols if col in ONET.columns]\n",
    "existing_last_cols = [col for col in last_cols if col in ONET.columns]\n",
    "\n",
    "middle_cols = [col for col in ONET.columns if col not in existing_first_cols + existing_wage_cols + existing_last_cols]\n",
    "\n",
    "ONET = ONET[existing_first_cols + existing_wage_cols + middle_cols + existing_last_cols]\n",
    "\n",
    "# Save final dataset with all wage percentiles\n",
    "ONET.to_csv(f'{output_data_path}/ONET_cleaned_tasks.csv', index=False)\n",
    "\n",
    "print(f\"✓ Final dataset shape: {ONET.shape}\")\n",
    "print(f\"✓ Wage columns included: {len(existing_wage_cols)}\")\n",
    "print(f\"✓ Saved to: {output_data_path}/ONET_cleaned_tasks.csv\")\n",
    "\n",
    "# Show the wage columns we have\n",
    "print(f\"\\nFinal wage columns in dataset:\")\n",
    "for i, col in enumerate(existing_wage_cols, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c6c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (22310, 39)\n",
      "Total rows: 22,310\n",
      "\n",
      "Columns with NaN values:\n",
      "==================================================\n",
      "Minor_Group_Title: 4,231 (18.96%)\n",
      "Annual_P90_Wage: 1,658 (7.43%)\n",
      "Hourly_P90_Wage: 1,594 (7.14%)\n",
      "Annual_P75_Wage: 1,138 (5.10%)\n",
      "Hourly_P75_Wage: 1,074 (4.81%)\n",
      "Annual_Median_Wage: 794 (3.56%)\n",
      "Hourly_Median_Wage: 730 (3.27%)\n",
      "Annual_Mean_Wage: 593 (2.66%)\n",
      "Annual_P10_Wage: 593 (2.66%)\n",
      "Annual_P25_Wage: 593 (2.66%)\n",
      "Hourly_Mean_Wage: 529 (2.37%)\n",
      "Hourly_P10_Wage: 529 (2.37%)\n",
      "Hourly_P25_Wage: 529 (2.37%)\n",
      "DWA ID: 425 (1.90%)\n",
      "DWA Title: 425 (1.90%)\n",
      "Broad_Occupation_Title: 215 (0.96%)\n",
      "\n",
      "Wage Data Coverage Summary:\n",
      "==================================================\n",
      "Hourly_Mean_Wage: 21,781 (97.63%)\n",
      "Hourly_P10_Wage: 21,781 (97.63%)\n",
      "Hourly_P25_Wage: 21,781 (97.63%)\n",
      "Hourly_Median_Wage: 21,580 (96.73%)\n",
      "Hourly_P75_Wage: 21,236 (95.19%)\n",
      "Hourly_P90_Wage: 20,716 (92.86%)\n",
      "Annual_Mean_Wage: 21,717 (97.34%)\n",
      "Annual_P10_Wage: 21,717 (97.34%)\n",
      "Annual_P25_Wage: 21,717 (97.34%)\n",
      "Annual_Median_Wage: 21,516 (96.44%)\n",
      "Annual_P75_Wage: 21,172 (94.90%)\n",
      "Annual_P90_Wage: 20,652 (92.57%)\n",
      "\n",
      "Key Wage Statistics:\n",
      "==============================\n",
      "Hourly_Mean_Wage:\n",
      "  Mean: $37.75, Median: $32.52\n",
      "  Range: $14.07 - $170.17\n",
      "\n",
      "Hourly_Median_Wage:\n",
      "  Mean: $33.73, Median: $29.61\n",
      "  Range: $14.02 - $113.46\n",
      "\n",
      "Annual_Mean_Wage:\n",
      "  Mean: $78478.61, Median: $67430.00\n",
      "  Range: $29260.00 - $353960.00\n",
      "\n",
      "Annual_Median_Wage:\n",
      "  Mean: $70182.75, Median: $61590.00\n",
      "  Range: $29170.00 - $236000.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: NaN values\n",
    "nan_counts = ONET.isna().sum()\n",
    "nan_summary = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "print(f\"Dataset shape: {ONET.shape}\")\n",
    "print(f\"Total rows: {len(ONET):,}\\n\")\n",
    "\n",
    "if len(nan_summary) > 0:\n",
    "    print(\"Columns with NaN values:\")\n",
    "    print(\"=\" * 50)\n",
    "    for col, count in nan_summary.items():\n",
    "        percentage = (count / len(ONET)) * 100\n",
    "        print(f\"{col}: {count:,} ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"✓ No NaN values found in any column!\")\n",
    "\n",
    "# Final verification of all wage data\n",
    "wage_columns = ['Hourly_Mean_Wage', 'Hourly_P10_Wage', 'Hourly_P25_Wage', 'Hourly_Median_Wage', \n",
    "                'Hourly_P75_Wage', 'Hourly_P90_Wage', 'Annual_Mean_Wage', 'Annual_P10_Wage', \n",
    "                'Annual_P25_Wage', 'Annual_Median_Wage', 'Annual_P75_Wage', 'Annual_P90_Wage']\n",
    "\n",
    "print(f\"\\nWage Data Coverage Summary:\")\n",
    "print(\"=\" * 50)\n",
    "total_tasks = len(ONET)\n",
    "\n",
    "for wage_col in wage_columns:\n",
    "    if wage_col in ONET.columns:\n",
    "        coverage = ONET[wage_col].notna().sum()\n",
    "        percentage = coverage / total_tasks * 100\n",
    "        print(f\"{wage_col}: {coverage:,} ({percentage:.2f}%)\")\n",
    "\n",
    "# Summary stats for key wage columns\n",
    "print(f\"\\nKey Wage Statistics:\")\n",
    "print(\"=\" * 30)\n",
    "key_cols = ['Hourly_Mean_Wage', 'Hourly_Median_Wage', 'Annual_Mean_Wage', 'Annual_Median_Wage']\n",
    "for col in key_cols:\n",
    "    if col in ONET.columns and ONET[col].notna().sum() > 0:\n",
    "        mean_val = ONET[col].mean()\n",
    "        median_val = ONET[col].median()\n",
    "        min_val = ONET[col].min()\n",
    "        max_val = ONET[col].max()\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Mean: ${mean_val:.2f}, Median: ${median_val:.2f}\")\n",
    "        print(f\"  Range: ${min_val:.2f} - ${max_val:.2f}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
