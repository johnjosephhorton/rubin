{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Nov 8, 2025\n",
    "#### Last Edit: Nov 8, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97af73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "# Install caffeinate package\n",
    "%pip install caffeinate\n",
    "\n",
    "# Use macOS built-in caffeinate command for reliability\n",
    "# This prevents the system from sleeping while the process is running\n",
    "try:\n",
    "    # Start caffeinate in the background\n",
    "    caff_process = subprocess.Popen(['caffeinate', '-d'], \n",
    "                                   stdout=subprocess.DEVNULL, \n",
    "                                   stderr=subprocess.DEVNULL)\n",
    "    print(f\"Caffeinate mode ON ‚òï ‚Äì Device will stay awake (PID: {caff_process.pid})\")\n",
    "    print(\"System sleep is disabled while this process runs.\")\n",
    "    \n",
    "    # Store the process ID for later cleanup\n",
    "    caff_pid = caff_process.pid\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not start caffeinate: {e}\")\n",
    "    print(\"Continuing without caffeinate - system may sleep during long processes.\")\n",
    "    caff_process = None\n",
    "    caff_pid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/similar_dwa_tasks'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608dc71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONET = pd.read_csv(f'{input_data_path}/computed_objects/ONET_cleaned_tasks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unique mapping between DWA and Tasks and save as CSV\n",
    "dwa_id_col = 'DWA ID'\n",
    "dwa_title_col = 'DWA Title'\n",
    "task_id_col = 'Task ID'\n",
    "task_title_col = 'Task Title'\n",
    "occupation_code_col = 'O*NET-SOC Code'\n",
    "occupation_title_col = 'Occupation Title'\n",
    "\n",
    "cols_map = {\n",
    "    'dwa_id': dwa_id_col,\n",
    "    'dwa_title': dwa_title_col,\n",
    "    'task_id': task_id_col,\n",
    "    'task_title': task_title_col,\n",
    "}\n",
    "print('Detected columns:')\n",
    "for k,v in cols_map.items():\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "required = [v for v in cols_map.values() if v is not None]\n",
    "if len(required) < 4:\n",
    "    raise ValueError('Could not automatically find all required columns in ONET. Columns available: ' + ', '.join(ONET.columns))\n",
    "\n",
    "# Select relevant columns, drop rows with missing values, dedupe and rename to a stable schema\n",
    "dwa_task_mapping = ONET[[dwa_id_col, dwa_title_col, task_id_col, task_title_col, occupation_code_col, occupation_title_col]].dropna(subset=[dwa_id_col, dwa_title_col, task_id_col, task_title_col, occupation_code_col, occupation_title_col])\n",
    "dwa_task_mapping = dwa_task_mapping.drop_duplicates().rename(columns={dwa_id_col: 'DWA ID', dwa_title_col: 'DWA Title', task_id_col: 'Task ID', task_title_col: 'Task Title', occupation_code_col: 'O*NET-SOC Code', occupation_title_col: 'Occupation Title'})\n",
    "dwa_task_mapping = dwa_task_mapping.sort_values(['DWA ID','Task ID']).reset_index(drop=True)\n",
    "\n",
    "csv_path = f'{output_data_path}/dwa_task_mapping.csv'\n",
    "dwa_task_mapping.to_csv(csv_path, index=False)\n",
    "print(f'Saved mapping to {csv_path} ‚Äî {len(dwa_task_mapping)} rows.')\n",
    "\n",
    "# Also save unique DWA list\n",
    "dwa_unique = dwa_task_mapping[['DWA ID','DWA Title']].drop_duplicates().sort_values('DWA ID').reset_index(drop=True)\n",
    "dwa_unique.to_csv(f'{output_data_path}/unique_dwa.csv', index=False)\n",
    "print(f'Saved unique DWA list to {output_data_path}/unique_dwa.csv ‚Äî {len(dwa_unique)} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DWA-Task mappings data\n",
    "dwa_task_mapping = pd.read_csv(f'{output_data_path}/dwa_task_mapping.csv')\n",
    "\n",
    "# Print number of unique DWAs and tasks\n",
    "num_unique_dwas = dwa_task_mapping['DWA Title'].nunique()\n",
    "num_unique_tasks = dwa_task_mapping['Task Title'].nunique()\n",
    "print(f\"Before Dropping Single-Task DWAs:\\n{num_unique_dwas} unique DWAs and {num_unique_tasks} unique tasks.\\n\")\n",
    "\n",
    "# Only keep DWAs with more than one tasks\n",
    "tasks_per_dwa = dwa_task_mapping.groupby('DWA Title')['Task Title'].nunique().reset_index()\n",
    "tasks_per_dwa = tasks_per_dwa.rename(columns={'Task Title': 'Num Tasks'})\n",
    "tasks_per_dwa = tasks_per_dwa[tasks_per_dwa['Num Tasks'] > 1]\n",
    "tasks_per_dwa_list = tasks_per_dwa['DWA Title'].tolist()\n",
    "\n",
    "# Drop DWAs with only one task from the mapping\n",
    "dwa_task_mapping = dwa_task_mapping[dwa_task_mapping['DWA Title'].isin(tasks_per_dwa_list)]\n",
    "\n",
    "# Print number of unique DWAs and tasks after filtering\n",
    "num_unique_dwas = dwa_task_mapping['DWA Title'].nunique()\n",
    "num_unique_tasks = dwa_task_mapping['Task Title'].nunique()\n",
    "print(f\"After Dropping Single-Task DWAs:\\n{num_unique_dwas} unique DWAs and {num_unique_tasks} unique tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252758cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsl import QuestionFreeText, Scenario, Model, Survey\n",
    "from textwrap import dedent\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_task_sequence(dwa, tasks_data, output_data_path):\n",
    "    \"\"\"\n",
    "    Extract task sequence for a DWA using EDSL workflow.\n",
    "    Returns the ordered sequence of tasks.\n",
    "    \"\"\"\n",
    "    # Check if output file already exists\n",
    "    safe_title = dwa.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    output_folder = f'{output_data_path}/similarTasks'\n",
    "    output_file = os.path.join(output_folder, f\"{safe_title}csv\") # no dot before csv because DWAs end with dot themselves\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        return output_file, True  # Return file path and flag indicating it already existed\n",
    "    \n",
    "    # Check if we have tasks for this DWA\n",
    "    if tasks_data.empty:\n",
    "        print(f\"‚ö†Ô∏è  Warning: No tasks found for DWA '{dwa}' - skipping\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "\n",
    "    # Format tasks as numbered list\n",
    "    tasks_title_list = tasks_data['Task Title'].tolist()\n",
    "    tasks_ids_list = tasks_data['Task ID'].tolist()\n",
    "    occupations_list = tasks_data['Occupation Title'].tolist()\n",
    "    occupation_codes_list = tasks_data['O*NET-SOC Code'].tolist()\n",
    "\n",
    "    tasks_text = \"\\n\".join([f\"{i}. {task}\" for i, task in enumerate(tasks_title_list, 1)])\n",
    "    tasks_ids_text = \"\\n\".join(f\"{i}. {task_id}\" for i, task_id in enumerate(tasks_ids_list, 1))\n",
    "    occupations_text = \"\\n\".join([f\"{i}. {occupation}\" for i, occupation in enumerate(occupations_list, 1)])\n",
    "    occupation_codes_text = \"\\n\".join([f\"{i}. {occupation_code}\" for i, occupation_code in enumerate(occupation_codes_list, 1)])\n",
    "    num_tasks = len(tasks_title_list)\n",
    "    max_tokens = 32000\n",
    "\n",
    "    print(f\"   ‚Ä¢ {num_tasks} tasks, using {max_tokens} max tokens\")\n",
    "\n",
    "    # Create scenario\n",
    "    scenario = Scenario({\n",
    "        \"detailed_work_activity\": dwa,\n",
    "        \"tasks_list\": tasks_text,\n",
    "        \"tasks_ids\": tasks_ids_text,\n",
    "        \"occupations_list\": occupations_text,\n",
    "        \"occupation_codes_list\": occupation_codes_text,\n",
    "        \"num_tasks\": num_tasks\n",
    "    })\n",
    "\n",
    "    # Create question for task sequencing\n",
    "    q_sequence = QuestionFreeText(\n",
    "        question_name=\"similar_tasks\",\n",
    "        question_text=dedent(\"\"\"\\\n",
    "            You are an expert in workflow analysis for the detailed work activity: {{ detailed_work_activity }}.\n",
    "            Below is a list of {{ num_tasks }} task IDs and titles that belong to this detailed work activity and appear across similar or different occupations (tasks and occupations are ordered such that the first task belongs to the first occupation, the second task belongs to the second occupation, etc.).\n",
    "            Tasks IDs:\n",
    "            {{ tasks_ids }}\n",
    "            \\n\n",
    "            Tasks list: \n",
    "            {{ tasks_list }}\n",
    "            \\n\n",
    "            Occupations list:\n",
    "            {{ occupations_list }}\n",
    "            \\n\n",
    "            Occupation Codes list:\n",
    "            {{ occupation_codes_list }}\n",
    "            \\n\n",
    "            Determine which tasks are similar in nature and in terms of their objectives, methods, or required skills.\n",
    "            There may be more than one task associated with an occupation. Return only the most relevant task for every occupation.\n",
    "            Only look for tasks that are actually similar. Do not feel obliged to return all occupations.\\n\n",
    "            Return the task-occupation pairs you determine as similar as a JSON array where each element has:\n",
    "            - \"Task ID\": the exact task ID from the list of task IDs above\n",
    "            - \"Task Title\": the exact task text from the list of tasks above\n",
    "            - \"O*NET-SOC Code\": the exact occupation code text from the list of occupation codes above\n",
    "            - \"Occupation Title\": the exact occupation text from the list of occupations above\n",
    "            Format: [{\"Task ID\": 1234, \"Task Title\": \"...\", \"O*NET-SOC Code\": \"...\", \"Occupation Title\": \"...\"}, {\"Task ID\": 5678, \"Task Title\": \"...\", \"O*NET-SOC Code\": \"...\", \"Occupation Title\": \"...\"}, ...]\n",
    "            Only return the JSON array, nothing else.\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create model using openai_v2 for reasoning capabilities\n",
    "        model = Model(\"gpt-5-mini\", service_name=\"openai_v2\", temperature=0.0, max_tokens=max_tokens)\n",
    "\n",
    "        # Run similarity question\n",
    "        similar_results = q_sequence.by(model).by([scenario]).run(progress_bar=False)\n",
    "        similar_df = similar_results.to_pandas()\n",
    "        \n",
    "        # -------------------------------\n",
    "        # Robustly normalize/clean the answer column to avoid NaN/type issues\n",
    "        # -------------------------------\n",
    "        col = 'answer.similar_tasks'\n",
    "\n",
    "        # 1) Normalize cell types into JSON strings (or <NA>)\n",
    "        def _to_json_str(v):\n",
    "            if isinstance(v, str):\n",
    "                return v\n",
    "            if isinstance(v, (list, dict)):\n",
    "                return json.dumps(v)\n",
    "            if v is None:\n",
    "                return pd.NA\n",
    "            if isinstance(v, float) and np.isnan(v):\n",
    "                return pd.NA\n",
    "            # fallback: last-resort string\n",
    "            return str(v)\n",
    "\n",
    "        similar_df[col] = (\n",
    "            similar_df[col]\n",
    "            .apply(_to_json_str)\n",
    "            .astype('string')        # pandas StringDtype, keeps <NA>\n",
    "            .fillna('[]')            # robust default to empty list\n",
    "            .str.strip()\n",
    "            # strip possible markdown code fences\n",
    "            .str.replace(r'^\\s*```json\\s*', '', regex=True)\n",
    "            .str.replace(r'\\s*```\\s*$', '', regex=True)\n",
    "        )\n",
    "\n",
    "        # Debug: Print the raw response before cleaning (safely)\n",
    "        val = similar_df[col].iat[0]\n",
    "        if isinstance(val, str):\n",
    "            print(f\"   ‚Ä¢ Raw/Clean JSON length: {len(val)}\")\n",
    "            print(f\"   ‚Ä¢ Raw/Clean JSON preview: {val[:50]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ Non-string value in {col}: {type(val)}\")\n",
    "\n",
    "        # 2) Parse JSON\n",
    "        similar_json = similar_df[col].iat[0]\n",
    "        try:\n",
    "            similar_data = json.loads(similar_json or '[]')\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå JSON parsing failed, trying to clean response further...\")\n",
    "            print(f\"   Original error: {e}\")\n",
    "            print(f\"   Response starts with: {similar_json[:100]}...\")\n",
    "            # Try additional cleanup (strip stray backticks/newlines)\n",
    "            cleaned = (similar_json or '').strip().strip('`').strip()\n",
    "            try:\n",
    "                similar_data = json.loads(cleaned or '[]')\n",
    "                print(f\"   ‚úÖ Successfully parsed after additional cleanup\")\n",
    "            except json.JSONDecodeError as e2:\n",
    "                print(f\"   ‚ùå Still failed after cleanup: {e2}\")\n",
    "                raise e  # Re-raise original error\n",
    "\n",
    "        similar_tasks_df = pd.DataFrame(similar_data)\n",
    "\n",
    "        # Add metadata columns\n",
    "        similar_tasks_df['DWA Title'] = dwa\n",
    "        similar_tasks_df['DWA ID'] = tasks_data['DWA ID'].iloc[0]\n",
    "        \n",
    "        # Reorder columns\n",
    "        similar_tasks_df = similar_tasks_df[['DWA ID', 'DWA Title', 'Task ID', 'Task Title', 'O*NET-SOC Code', 'Occupation Title']]\n",
    "\n",
    "        # Save to file\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        similar_tasks_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"   ‚úÖ Successfully processed and saved task sequence\")\n",
    "        return output_file, False  # Return file path and flag indicating it was newly created\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON Error for '{dwa}': {e}\")\n",
    "        print(f\"   Raw response: {similar_json}\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error for '{dwa}': {e}\")\n",
    "        return None, True  # Treat as already processed to skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fe567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique occupation titles from the dataset\n",
    "dwa_list = sorted(dwa_task_mapping['DWA Title'].unique().tolist())\n",
    "print(f\"Found {len(dwa_list)} unique occupations in the dataset:\")\n",
    "\n",
    "# Set seed for reproducible random sampling\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# # Randomly sample 10% of DWAs\n",
    "# sample_size = max(1, int(len(dwa_list) * 0.10))  # Ensure at least 1 occupation\n",
    "# sampled_dwas = random.sample(dwa_list, sample_size)\n",
    "# print(f\"Randomly selected {len(sampled_dwas)} DWAs (5% of total) for processing:\")\n",
    "# print(f\"Sample: {sampled_dwas[:5]}...\" if len(sampled_dwas) > 5 else f\"Sample: {sampled_dwas}\")\n",
    "sampled_dwas = dwa_list\n",
    "\n",
    "# Process each occupation\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "\n",
    "for i, dwa in enumerate(sampled_dwas, 1):\n",
    "    print(dwa)\n",
    "    # Filter data for this DWA\n",
    "    dwa_data = dwa_task_mapping[dwa_task_mapping['DWA Title'] == dwa].copy()\n",
    "\n",
    "    # Prepare task data\n",
    "    dwa_task_data = dwa_data[['Task ID', 'Task Title', 'O*NET-SOC Code', 'Occupation Title', 'DWA ID']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Enhanced progress output\n",
    "    num_tasks = len(dwa_task_data)\n",
    "    print(f\"\\n[{i}/{len(sampled_dwas)}] {dwa}\")\n",
    "\n",
    "    # Extract task sequence\n",
    "    output_file, already_existed = extract_task_sequence(dwa, dwa_task_data, output_data_path)\n",
    "\n",
    "    if output_file is None:\n",
    "        error_count += 1\n",
    "    elif already_existed:\n",
    "        print(f\"   ‚è≠Ô∏è  Already exists - skipping\")\n",
    "        skipped_count += 1\n",
    "    else:\n",
    "        processed_count += 1\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"‚Ä¢ {processed_count} DWAs processed\")\n",
    "print(f\"‚Ä¢ {skipped_count} DWAs skipped (already existed)\")\n",
    "print(f\"‚Ä¢ {error_count} DWAs failed\")\n",
    "print(f\"‚Ä¢ {len(sampled_dwas)} total DWAs in sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee389c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up caffeinate process\n",
    "try:\n",
    "    if 'caff_process' in globals() and caff_process is not None:\n",
    "        caff_process.terminate()\n",
    "        caff_process.wait()  # Wait for process to terminate\n",
    "        print(\"Caffeinate mode OFF üí° - System sleep is now enabled.\")\n",
    "    else:\n",
    "        print(\"Caffeinate was not running or already stopped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Caffeinate process may have already ended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
