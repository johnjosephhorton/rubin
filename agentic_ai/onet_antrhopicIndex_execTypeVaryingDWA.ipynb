{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 19, 2025\n",
    "#### Last Edit: Dec 15, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac02f0",
   "metadata": {},
   "source": [
    "## Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b14d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_var = 'is_ai'\n",
    "plot_title_variable = 'Task is AI'\n",
    "# dependent_var = 'is_automated'\n",
    "# plot_title_variable = 'Task is Automated'\n",
    "\n",
    "KEEP_RANDOM_DWAS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d28924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reshuffles\n",
    "n_shuffles = 1000\n",
    "\n",
    "TARGET_REGS = ['prev2_is_ai', 'prev_is_ai', 'next_is_ai', 'next2_is_ai']\n",
    "SPECS = ['no_fe_no_dwa', 'major_fe_no_dwa', 'minor_fe_no_dwa', 'no_fe_with_dwa']#, 'major_fe_with_dwa']\n",
    "PLOT_TITLES = ['Task Before Previous Task', 'Previous Task', 'Next Task', 'Task After Next Task']\n",
    "\n",
    "# Labels to match desired output format\n",
    "VAR_LABELS = {\n",
    "    'prev2_is_ai': '($t-2$) Task AI',\n",
    "    'prev_is_ai': '($t-1$) Task AI',\n",
    "    'next_is_ai': '($t+1$) Task AI',\n",
    "    'next2_is_ai': '($t+2$) Task AI'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc32075",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_suffix = \"\" if KEEP_RANDOM_DWAS else \"_noTasksWithRepetitiveDWAs\"\n",
    "\n",
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/execTypeVaryingDWA_anthropicIndex{path_suffix}'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots/execTypeVaryingDWA{path_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f93a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887655d",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of DWAs with tasks in multiple occupations\n",
    "dwa_list_path = f\"{input_data_path}/computed_objects/similar_dwa_tasks/similarTasks\"\n",
    "\n",
    "# Read all CSV files\n",
    "import glob\n",
    "dwa_csv_files = glob.glob(os.path.join(dwa_list_path, \"*.csv\"))\n",
    "print(f\"Found {len(dwa_csv_files)} DWA CSV files.\")\n",
    "\n",
    "# Load them into DataFrames, skipping 1-row files\n",
    "dwa_dfs = []\n",
    "skipped_files_count = 0\n",
    "for f in dwa_csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if len(df) > 1: # Skip if DWA contains only one task\n",
    "        dwa_dfs.append(df)\n",
    "    else:\n",
    "        skipped_files_count += 1\n",
    "print(f\"Skipped {skipped_files_count} DWA files with only one task.\")\n",
    "    \n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_all = pd.concat(dwa_dfs, ignore_index=True)\n",
    "repetitive_dwa_task_ids = df_all['Task ID'].unique().tolist()\n",
    "repetitive_dwa_task_titles = df_all['Task Title'].unique().tolist()\n",
    "print(f\"Found {len(repetitive_dwa_task_ids)} tasks related to these DWAs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with all tasks that have survived the DWA task similarity procedure\n",
    "survived_tasks_count_df = df_all.groupby('DWA ID')['Task ID'].nunique().reset_index(name='num_tasks_survived')\n",
    "survived_tasks_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70446bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of how many DWAs each task is mapped to\n",
    "dwa_task_mapping = pd.read_csv(f\"{input_data_path}/computed_objects/similar_dwa_tasks/dwa_task_mapping.csv\")\n",
    "dwa_task_counts = dwa_task_mapping.groupby('Task ID')['DWA ID'].nunique().reset_index(name='num_dwas_per_task')\n",
    "\n",
    "# Plot distribution of number of DWAs per task\n",
    "dwa_task_counts = dwa_task_counts.sort_values(by='num_dwas_per_task', ascending=False)\n",
    "plt.hist(dwa_task_counts['num_dwas_per_task'], bins=range(1, dwa_task_counts['num_dwas_per_task'].max()+2))\n",
    "plt.xlabel('Number of DWAs per Task')\n",
    "plt.ylabel('Number of Tasks')\n",
    "plt.title('Distribution of Number of DWAs per Task')\n",
    "# plt.savefig(f\"{output_plot_path}/dwa_task_count_distribution.png\")\n",
    "\n",
    "\n",
    "\n",
    "# One way of handling multiple-DWA tasks is to only keep those with a single DWA and drop the rest\n",
    "# Another way is to pick a random DWA for each task\n",
    "if KEEP_RANDOM_DWAS:\n",
    "    unique_task_dwa_mapping = (\n",
    "        dwa_task_mapping\n",
    "        .groupby(\"Task ID\", group_keys=False)\n",
    "        .apply(lambda g: g.sample(n=1, random_state=123))\n",
    "        .reset_index(drop=True)\n",
    "        )['Task ID'].tolist()\n",
    "else:\n",
    "    unique_task_dwa_mapping = dwa_task_counts[dwa_task_counts['num_dwas_per_task'] == 1]['Task ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DWA-level dataset with number of tasks and occupations per DWA, as well as fraction of manual, automation, and augmentation tasks per DWA\n",
    "merged_data = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\")\n",
    "merged_data['is_manual'] = merged_data['label'] == 'Manual'\n",
    "merged_data['is_automation'] = merged_data['label'] == 'Automation'\n",
    "merged_data['is_augmentation'] = merged_data['label'] == 'Augmentation'\n",
    "\n",
    "\n",
    "# Merge back DWA ID and DWA Titles to the merged_data\n",
    "dwa_task_mapping = pd.read_csv(f\"{input_data_path}/computed_objects/similar_dwa_tasks/dwa_task_mapping.csv\")\n",
    "print(f'Length of merged_data before merging DWA info: {merged_data.shape[0]}')\n",
    "merged_data = merged_data.merge(dwa_task_mapping, on=['Task ID', 'Task Title', 'O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "print(f'Length of merged_data after merging DWA info: {merged_data.shape[0]}')\n",
    "\n",
    "\n",
    "# Aggregate to get fractions\n",
    "dwa_grouped = merged_data.groupby(['DWA ID', 'DWA Title']).agg(\n",
    "    num_tasks = ('Task ID', 'nunique'),\n",
    "    num_occupations = ('O*NET-SOC Code', 'nunique'),\n",
    "    fraction_manual = ('is_manual', 'mean'),\n",
    "    fraction_automation = ('is_automation', 'mean'),\n",
    "    fraction_augmentation = ('is_augmentation', 'mean'),\n",
    ").reset_index()\n",
    "print(f\"Created DWA-level dataset with {dwa_grouped.shape[0]} DWAs.\")\n",
    "\n",
    "# Keep only DWAs with variation in terms of execution type across occupations\n",
    "dwa_grouped_filtered = dwa_grouped[\n",
    "     (dwa_grouped['num_occupations'] > 1) & (dwa_grouped['fraction_manual'] > 0) & (dwa_grouped['fraction_manual'] < 1)\n",
    "].copy()\n",
    "display(dwa_grouped_filtered)\n",
    "\n",
    "# Create list of DWAs with varying execution types\n",
    "dwas_varying_exec_types_ids = dwa_grouped_filtered['DWA ID'].unique().tolist()\n",
    "dwas_varying_exec_types_titles = dwa_grouped_filtered['DWA Title'].unique().tolist()\n",
    "print(f\"Identified {len(dwas_varying_exec_types_ids)} DWAs with varying execution types across occupations.\")\n",
    "\n",
    "# Merge back the number of tasks survived info\n",
    "dwa_grouped_filtered = dwa_grouped_filtered.merge(survived_tasks_count_df, left_on='DWA ID', right_on='DWA ID', how='left')\n",
    "\n",
    "# Save output\n",
    "dwa_grouped_filtered.to_csv(f\"{output_data_path}/dwas_varying_execution_types.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a84d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged data\n",
    "merged_data = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\")\n",
    "merged_data = merged_data[['O*NET-SOC Code', 'Occupation Title', 'Task ID', 'Task Title',\n",
    "       'Task Position', 'Task Type', \n",
    "       'Major_Group_Code', 'Major_Group_Title', \n",
    "       'Minor_Group_Code', 'Minor_Group_Title',\n",
    "       'Broad_Occupation_Code', 'Broad_Occupation_Title',\n",
    "       'Detailed_Occupation_Code', 'Detailed_Occupation_Title',\n",
    "       'gpt4_exposure', 'human_labels', \n",
    "       'automation', 'augmentation', 'label']]\n",
    "\n",
    "\n",
    "# Create is_ai and is_automated flags in merged_data\n",
    "merged_data['is_ai'] = merged_data['label'].isin(['Augmentation','Automation']).astype(int)\n",
    "merged_data['is_automated'] = merged_data['label'].isin(['Automation']).astype(int)\n",
    "merged_data['is_exposed'] = merged_data['human_labels'].isin(['E1']).astype(int)\n",
    "\n",
    "\n",
    "# Step 1: Add occupation's number of tasks info\n",
    "num_tasks_per_occupation = merged_data.groupby('O*NET-SOC Code')['Task ID'].nunique().reset_index()\n",
    "num_tasks_per_occupation = num_tasks_per_occupation.rename(columns={'Task ID': 'num_tasks'})\n",
    "merged_data = merged_data.merge(num_tasks_per_occupation, on='O*NET-SOC Code', how='left')\n",
    "\n",
    "\n",
    "# Step 2: Create flags for previous/next tasks is AI within occupation groups\n",
    "# Sort by occupation and position when possible\n",
    "merged_data['Task Position'] = pd.to_numeric(merged_data['Task Position'], errors='coerce')\n",
    "merged_data = merged_data.sort_values(['O*NET-SOC Code', 'Task Position']).reset_index(drop=True)\n",
    "group_col = 'O*NET-SOC Code'\n",
    "\n",
    "# Compute neighbor flags (prev/next) within occupation groups when possible\n",
    "merged_data['prev_is_ai'] = 0\n",
    "merged_data['prev2_is_ai'] = 0\n",
    "merged_data['next_is_ai'] = 0\n",
    "merged_data['next2_is_ai'] = 0\n",
    "pos_col = 'Task Position'\n",
    "\n",
    "def add_neighbor_flags(df):\n",
    "    df = df.copy()\n",
    "    df['Task Position'] = pd.to_numeric(df['Task Position'], errors='coerce')\n",
    "    df = df.sort_values(['O*NET-SOC Code','Task Position']).reset_index(drop=True)\n",
    "    \n",
    "    def _add_flags(g):\n",
    "        g = g.sort_values('Task Position')\n",
    "        # Don't fill NAs - leave them as NaN\n",
    "        g['prev_is_ai'] = g['is_ai'].shift(1)\n",
    "        g['prev2_is_ai'] = g['is_ai'].shift(2)\n",
    "        g['next_is_ai'] = g['is_ai'].shift(-1)\n",
    "        g['next2_is_ai'] = g['is_ai'].shift(-2)\n",
    "        return g\n",
    "    \n",
    "    return df.groupby('O*NET-SOC Code', group_keys=False).apply(_add_flags).reset_index(drop=True)\n",
    "\n",
    "# Apply the function\n",
    "merged_data = merged_data.groupby(group_col, group_keys=False).apply(add_neighbor_flags).reset_index(drop=True)\n",
    "\n",
    "# Drop rows where ANY neighbor flag is NA\n",
    "neighbor_cols = ['prev_is_ai', 'prev2_is_ai', 'next_is_ai', 'next2_is_ai']\n",
    "merged_data = merged_data.dropna(subset=neighbor_cols).reset_index(drop=True)\n",
    "\n",
    "# Convert to int after dropping NAs\n",
    "for col in neighbor_cols:\n",
    "    merged_data[col] = merged_data[col].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Add back DWA info\n",
    "# Merge back DWA ID and DWA Titles to the merged_data\n",
    "dwa_task_mapping = pd.read_csv(f\"{input_data_path}/computed_objects/similar_dwa_tasks/dwa_task_mapping.csv\")\n",
    "merged_data = merged_data.merge(dwa_task_mapping, on=['Task ID', 'Task Title', 'O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "# Note that the merge might map multiple DWAs to the same task\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Some cleanings\n",
    "# Filter 0: Remove tasks mapped to more than 1 DWA and remove duplicates in terms of (O*NET-SOC Code, Task ID) if any\n",
    "print(f'Length of merged_data before removing tasks mapped to multiple DWAs: {merged_data.shape[0]}')\n",
    "merged_data = merged_data[merged_data['Task ID'].isin(unique_task_dwa_mapping)].reset_index(drop=True)\n",
    "print(f'Length of merged_data after removing tasks mapped to multiple DWAs: {merged_data.shape[0]}\\n')\n",
    "\n",
    "print(f'Length of merged_data before dropping duplicates: {merged_data.shape[0]}')\n",
    "merged_data = merged_data.drop_duplicates(subset=['O*NET-SOC Code', 'Task ID']).reset_index(drop=True)\n",
    "print(f'Length of merged_data after dropping duplicates: {merged_data.shape[0]}\\n')\n",
    "\n",
    "# Filter 1: Remove tasks without any DWA info\n",
    "print(f'Length of merged_data before removing tasks without DWA info: {merged_data.shape[0]}')\n",
    "print(f'Number of tasks without DWA info: {merged_data[merged_data[\"DWA ID\"].isna()].shape[0]}')\n",
    "merged_data = merged_data[~merged_data['DWA ID'].isna()].reset_index(drop=True)\n",
    "print(f'Length of merged_data after removing tasks without DWA info: {merged_data.shape[0]}\\n')\n",
    "\n",
    "# Filter 2: Remove DWAs whose tasks appear in only a single occupation\n",
    "print(f'Length of merged_data before filtering DWAs with tasks in single occupation: {merged_data.shape[0]}')\n",
    "merged_data = merged_data[~merged_data['DWA ID'].isna()].reset_index(drop=True)\n",
    "dwa_occupation_counts = merged_data.groupby('DWA ID')['O*NET-SOC Code'].nunique().reset_index()\n",
    "dwa_occupation_counts = dwa_occupation_counts.rename(columns={'O*NET-SOC Code': 'num_occupations'})\n",
    "dwas_to_keep = dwa_occupation_counts[dwa_occupation_counts['num_occupations'] > 1]['DWA ID'].unique().tolist()\n",
    "print(f'Number of Tasks whose DWA contain tasks in a single occupation: {len(merged_data[~merged_data['DWA ID'].isin(dwas_to_keep)])}')\n",
    "merged_data = merged_data[merged_data['DWA ID'].isin(dwas_to_keep)].reset_index(drop=True)\n",
    "print(f'Length of merged_data after filtering DWAs with tasks in single occupation: {merged_data.shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Create variable indicating how many tasks are present the task's DWA within the occupation\n",
    "dwa_occupation_task_counts = merged_data.groupby(['DWA ID', 'O*NET-SOC Code'])['Task ID'].nunique().reset_index(name='num_tasks_in_dwa_within_occupation')\n",
    "merged_data = merged_data.merge(dwa_occupation_task_counts, on=['DWA ID', 'O*NET-SOC Code'], how='left')\n",
    "\n",
    "\n",
    "# Get list of tasks present in the final merged_data\n",
    "merged_data_task_ids = merged_data['Task ID'].unique().tolist()\n",
    "\n",
    "# Save the cleaned merged_data for future use\n",
    "merged_data.to_csv(f\"{output_data_path}/similarTasks_allEligibleTasks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create different extracts of the data for different ways of handling the repetitive tasks\n",
    "\n",
    "# # Extract 1: \n",
    "# # Remove DWAs that contain occupations with more than one task altogether -- conservative approach with minimal filtering applied to O*NET data\n",
    "# # Identify DWAs to remove\n",
    "# dwa_task_occupation_counts = merged_data.groupby(['DWA ID', 'O*NET-SOC Code'])['Task ID'].nunique().reset_index()\n",
    "# dwa_task_occupation_counts['has_multiple_tasks'] = dwa_task_occupation_counts['Task ID'] > 1\n",
    "# dwas_to_remove = dwa_task_occupation_counts[dwa_task_occupation_counts['has_multiple_tasks']]['DWA ID'].unique().tolist()\n",
    "# print(f\"Identified {len(dwas_to_remove)} DWAs to remove due to multiple tasks in at least one occupation.\")\n",
    "\n",
    "# original_ONET_ideal_DWAs_data = merged_data[~merged_data['DWA ID'].isin(dwas_to_remove)].reset_index(drop=True)\n",
    "# print(f\"Original O*NET ideal DWAs dataset has {original_ONET_ideal_DWAs_data.shape[0]} tasks across {original_ONET_ideal_DWAs_data['DWA ID'].nunique()} DWAs.\\n\")\n",
    "\n",
    "# # Save the original O*NET ideal DWAs dataset\n",
    "# original_ONET_ideal_DWAs_data.to_csv(f\"{output_data_path}/similarTasks_ONEToriginal_idealDWAs.csv\", index=False)\n",
    "\n",
    "\n",
    "# # Extract 2: \n",
    "# # Remove the tasks of occupations with multiple tasks within each DWA - less conservative approach as it keeps more observations but removes some tasks within DWAs\n",
    "# task_counts = merged_data.groupby(['DWA ID', 'O*NET-SOC Code'])['Task ID'].nunique().reset_index()\n",
    "# task_counts = task_counts.rename(columns={'Task ID': 'num_tasks_in_occupation_in_DWA'})\n",
    "# merged_data_with_counts = merged_data.merge(task_counts, on=['DWA ID', 'O*NET-SOC Code'], how='left')\n",
    "# merged_data_without_repetitive_occupation_tasks = merged_data_with_counts[merged_data_with_counts['num_tasks_in_occupation_in_DWA'] == 1].reset_index(drop=True)\n",
    "\n",
    "# # Now remove DWAs that have only one occupation left after this filtering\n",
    "# dwa_occupation_counts_after_filter = merged_data_without_repetitive_occupation_tasks.groupby('DWA ID')['O*NET-SOC Code'].nunique().reset_index()\n",
    "# dwa_occupation_counts_after_filter = dwa_occupation_counts_after_filter.rename(columns={'O*NET-SOC Code': 'num_occupations'})\n",
    "# dwas_to_keep_after_filter = dwa_occupation_counts_after_filter[dwa_occupation_counts_after_filter['num_occupations'] > 1]['DWA ID'].unique().tolist()\n",
    "# merged_data_without_repetitive_occupation_tasks = merged_data_without_repetitive_occupation_tasks[merged_data_without_repetitive_occupation_tasks['DWA ID'].isin(dwas_to_keep_after_filter)].reset_index(drop=True)\n",
    "# print(f\"Filtered O*NET ideal DWAs dataset has {merged_data_without_repetitive_occupation_tasks.shape[0]} tasks across {merged_data_without_repetitive_occupation_tasks['DWA ID'].nunique()} DWAs.\")\n",
    "\n",
    "# # Save the filtered dataset without repetitive occupation tasks\n",
    "# merged_data_without_repetitive_occupation_tasks.to_csv(f\"{output_data_path}/similarTasks_onlyKept_uniqueTaskOccupations_withinDWA.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Extract 3: \n",
    "# Retain only similar tasks within each DWA (and one task per occupation within each DWA), specified by GPT\n",
    "filtered_data = merged_data[(merged_data['DWA ID'].isin(dwas_varying_exec_types_ids)\n",
    "                             & \n",
    "                             merged_data['Task ID'].isin(repetitive_dwa_task_ids)\n",
    "                             )\n",
    "                             ].reset_index(drop=True)\n",
    "gpt_filtered_task_ids = filtered_data['Task ID'].unique().tolist()\n",
    "\n",
    "# Save the GPT-filtered dataset\n",
    "filtered_data.to_csv(f\"{output_data_path}/GPT_filtered_DWAs_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e805dee",
   "metadata": {},
   "source": [
    "## Run regression of multiple-execution-type DWA tasks against execution type of neighboring tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf153cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import norm\n",
    "from pathlib import Path\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. Robust AME Extractor\n",
    "# ==========================================\n",
    "def build_ame_df(\n",
    "    res,\n",
    "    df_used,\n",
    "    dataset_name,\n",
    "    model_name,\n",
    "    target_regs,\n",
    "    fe_label,\n",
    "    dwa_fe,\n",
    "    formula,\n",
    "    calculate_standard_errors=True,\n",
    "    cluster_col=\"DWA ID\",\n",
    "    B=5,\n",
    "    seed=123\n",
    "):\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        pr2 = res.prsquared\n",
    "        k = res.params.shape[0]\n",
    "        adj_pr2 = 1 - (res.llf - k) / res.llnull\n",
    "        nobs = int(res.nobs)\n",
    "\n",
    "        df_base = df_used.copy()\n",
    "        df_base.columns = df_base.columns.str.strip()\n",
    "\n",
    "        # ---------- point estimates ----------\n",
    "        ame_point = {}\n",
    "        for var in target_regs:\n",
    "            if var not in df_base.columns:\n",
    "                continue\n",
    "\n",
    "            df1 = df_base.copy()\n",
    "            df0 = df_base.copy()\n",
    "            df1[var] = 1\n",
    "            df0[var] = 0\n",
    "\n",
    "            p1 = res.predict(df1)\n",
    "            p0 = res.predict(df0)\n",
    "            ame_point[var] = np.mean(p1 - p0)\n",
    "\n",
    "        # ---------- optionally compute SEs ----------\n",
    "        ame_se = {v: 0.0 for v in ame_point.keys()}\n",
    "        ame_p = {v: 0.0 for v in ame_point.keys()}\n",
    "\n",
    "        if calculate_standard_errors:\n",
    "            if B % 50 == 0:\n",
    "                print(f\"Calculating AME standard errors with B={B} bootstrap samples for model {model_name}...\")\n",
    "            ame_boot = {v: [] for v in ame_point.keys()}\n",
    "            clusters = df_base[cluster_col].unique()\n",
    "\n",
    "            for _ in range(B):\n",
    "                sampled_clusters = np.random.choice(\n",
    "                    clusters, size=len(clusters), replace=True\n",
    "                )\n",
    "\n",
    "                df_b = pd.concat(\n",
    "                    [df_base[df_base[cluster_col] == c] for c in sampled_clusters],\n",
    "                    ignore_index=True\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    res_b = smf.logit(formula, data=df_b).fit(disp=False)\n",
    "\n",
    "                    for var in ame_point.keys():\n",
    "                        df1 = df_b.copy()\n",
    "                        df0 = df_b.copy()\n",
    "                        df1[var] = 1\n",
    "                        df0[var] = 0\n",
    "\n",
    "                        p1 = res_b.predict(df1)\n",
    "                        p0 = res_b.predict(df0)\n",
    "                        ame_boot[var].append(np.mean(p1 - p0))\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            for var in ame_point.keys():\n",
    "                if len(ame_boot[var]) > 1:\n",
    "                    se = np.std(ame_boot[var], ddof=1)\n",
    "                    z = ame_point[var] / se if se > 0 else 0.0\n",
    "                    p = 2 * (1 - norm.cdf(abs(z))) if se > 0 else 0.0\n",
    "                else:\n",
    "                    se, p = 0.0, 0.0\n",
    "\n",
    "                ame_se[var] = se\n",
    "                ame_p[var] = p\n",
    "\n",
    "        # ---------- assemble ----------\n",
    "        rows = []\n",
    "        for var, ame in ame_point.items():\n",
    "            rows.append({\n",
    "                \"term\": var,\n",
    "                \"ame_coef\": ame,\n",
    "                \"ame_se\": ame_se[var],\n",
    "                \"p_value\": ame_p[var]\n",
    "            })\n",
    "\n",
    "        ame_df = pd.DataFrame(rows)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"model\": model_name,\n",
    "            \"fe_label\": fe_label,\n",
    "            \"dwa_fe\": dwa_fe,\n",
    "            \"nobs\": nobs,\n",
    "            \"r2_pseudo\": pr2,\n",
    "            \"r2_adj_pseudo\": adj_pr2,\n",
    "            \"term\": ame_df[\"term\"],\n",
    "            \"ame_coef\": ame_df[\"ame_coef\"],\n",
    "            \"ame_se\": ame_df[\"ame_se\"],\n",
    "            \"p_value\": ame_df[\"p_value\"],\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating AME for {model_name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Regression Runner\n",
    "# ==========================================\n",
    "def filter_on_identifying_variation(\n",
    "    df, group_cols, regressors, dependent_var, min_unique=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Keeps only groups where EACH regressor and the dependent variable varies within the group.\n",
    "    \"\"\"\n",
    "    variables_to_check = regressors + [dependent_var]\n",
    "    def has_variation(g):\n",
    "        return all(g[r].nunique() >= min_unique for r in variables_to_check)\n",
    "\n",
    "    return df.groupby(group_cols, group_keys=False).filter(has_variation)\n",
    "\n",
    "\n",
    "def run_regressions_on(df, dataset_name, dependent_var, regressors, calculate_standard_errors=True):\n",
    "    df = df.copy()\n",
    "    all_cols = regressors + [dependent_var, 'is_exposed', 'num_tasks', 'DWA ID']\n",
    "    existing_cols = [c for c in all_cols if c in df.columns]\n",
    "    \n",
    "    # Convert numeric columns but NOT DWA ID (keep it as categorical)\n",
    "    numeric_cols = [c for c in existing_cols if c != 'DWA ID']\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    base_formula = f'{dependent_var} ~ ' + ' + '.join(regressors)\n",
    "    ame_list = []\n",
    "    models = {}\n",
    "\n",
    "    # ==== WITHOUT DWA Fixed Effects ====\n",
    "    \n",
    "    # 1) No SOC FE, No DWA FE\n",
    "    try:\n",
    "        formula = base_formula + ' + is_exposed + num_tasks'\n",
    "        res = smf.logit(formula, data=df).fit(\n",
    "            disp=False, \n",
    "            cov_type='cluster',\n",
    "            cov_kwds={'groups': df['DWA ID'],\n",
    "                      'use_correction': True}\n",
    "        )\n",
    "        models['no_fe_no_dwa'] = res\n",
    "        ame_list.append(build_ame_df(res, df, dataset_name, 'no_fe_no_dwa', regressors, \n",
    "                                     fe_label=\"None\", dwa_fe=False, formula=formula,\n",
    "                                     calculate_standard_errors=calculate_standard_errors))\n",
    "        # print(f\"[{dataset_name}] No-FE, No-DWA model converged.\")\n",
    "    except Exception as e: \n",
    "        print(f\"[{dataset_name}] No-FE, No-DWA failed: {e}\")\n",
    "\n",
    "    # 2) Major FE, No DWA FE\n",
    "    if 'Major_Group_Code' in df.columns:\n",
    "        try:\n",
    "            formula = base_formula + ' + C(Major_Group_Code) + is_exposed + num_tasks'\n",
    "            # keep only the groups with variation in dependent variable\n",
    "            df_fe_major = df.groupby(\"Major_Group_Code\").filter(lambda g: g[dependent_var].nunique() > 1)\n",
    "            res = smf.logit(formula, data=df_fe_major).fit(\n",
    "                disp=False, \n",
    "                cov_type='cluster',\n",
    "                cov_kwds={'groups': df_fe_major['DWA ID'],\n",
    "                          'use_correction': True}\n",
    "            )\n",
    "            models['major_fe_no_dwa'] = res\n",
    "            ame_list.append(build_ame_df(res, df_fe_major, dataset_name, 'major_fe_no_dwa', regressors, \n",
    "                                         fe_label=\"Major Group\", dwa_fe=False, formula=formula,\n",
    "                                         calculate_standard_errors=calculate_standard_errors))\n",
    "            # print(f\"[{dataset_name}] Major FE, No-DWA model converged.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{dataset_name}] Major FE, No-DWA failed: {e}\")\n",
    "\n",
    "    # 3) Minor FE, No DWA FE\n",
    "    if 'Minor_Group_Code' in df.columns:\n",
    "        try:\n",
    "            formula = base_formula + ' + C(Minor_Group_Code) + is_exposed + num_tasks'\n",
    "            # keep only the groups with variation in dependent variable\n",
    "            df_fe_minor = df.groupby(\"Minor_Group_Code\").filter(lambda g: g[dependent_var].nunique() > 1)\n",
    "            res = smf.logit(formula, data=df_fe_minor).fit(\n",
    "                disp=False, \n",
    "                cov_type='cluster',\n",
    "                cov_kwds={'groups': df_fe_minor['DWA ID'],\n",
    "                          'use_correction': True}\n",
    "            )\n",
    "            models['minor_fe_no_dwa'] = res\n",
    "            ame_list.append(build_ame_df(res, df_fe_minor, dataset_name, 'minor_fe_no_dwa', regressors, \n",
    "                                         fe_label=\"Minor Group\", dwa_fe=False, formula=formula,\n",
    "                                         calculate_standard_errors=calculate_standard_errors))\n",
    "            # print(f\"[{dataset_name}] Minor FE, No-DWA model converged.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{dataset_name}] Minor FE, No-DWA failed: {e}\")\n",
    "\n",
    "    # ==== WITH DWA Fixed Effects ====\n",
    "\n",
    "    # 4) No SOC FE, With DWA FE\n",
    "    try:\n",
    "        formula = base_formula + ' + C(DWA_ID) + is_exposed + num_tasks'\n",
    "        # Rename column to avoid spaces in formula\n",
    "        df['DWA_ID'] = df['DWA ID']\n",
    "        # keep only the groups with variation in dependent variable\n",
    "        df_dwa = df.groupby(\"DWA_ID\").filter(lambda g: g[dependent_var].nunique() > 1)\n",
    "        res = smf.logit(formula, data=df_dwa).fit(\n",
    "            disp=False, \n",
    "            cov_type='cluster',\n",
    "            cov_kwds={'groups': df_dwa['DWA_ID'],\n",
    "                      'use_correction': True}\n",
    "        )\n",
    "        models['no_fe_with_dwa'] = res\n",
    "        ame_list.append(build_ame_df(res, df_dwa, dataset_name, 'no_fe_with_dwa', regressors, \n",
    "                                     fe_label=\"None\", dwa_fe=True, formula=formula,\n",
    "                                     calculate_standard_errors=calculate_standard_errors))\n",
    "        # print(f\"[{dataset_name}] No-FE, With-DWA model converged.\")\n",
    "    except Exception as e: \n",
    "        print(f\"[{dataset_name}] No-FE, With-DWA failed: {e}\")\n",
    "\n",
    "\n",
    "    # 5) No SOC FE, No DWA FE, With Same DWA Task Counts in Occupation\n",
    "    try:\n",
    "        formula = base_formula + ' + is_exposed + num_tasks + num_tasks_in_dwa_within_occupation'\n",
    "        # Rename column to avoid spaces in formula\n",
    "        df['DWA_ID'] = df['DWA ID']\n",
    "        res = smf.logit(formula, data=df_dwa).fit(\n",
    "            disp=False, \n",
    "            cov_type='cluster',\n",
    "            cov_kwds={'groups': df_dwa['DWA_ID'],\n",
    "                      'use_correction': True}\n",
    "        )\n",
    "        models['no_fe_no_dwa_withTaskDWACount'] = res\n",
    "        ame_list.append(build_ame_df(res, df_dwa, dataset_name, 'no_fe_no_dwa_withTaskDWACount', regressors, \n",
    "                                     fe_label=\"None\", dwa_fe=False, formula=formula,\n",
    "                                     calculate_standard_errors=calculate_standard_errors))\n",
    "        # print(f\"[{dataset_name}] No-FE, With-DWA model converged.\")\n",
    "    except Exception as e: \n",
    "        print(f\"[{dataset_name}] No-FE, With-DWA failed: {e}\")\n",
    "\n",
    "    \n",
    "    # 6) No SOC FE, With DWA FE, With Same DWA Task Counts in Occupation\n",
    "    try:\n",
    "        formula = base_formula + ' + C(DWA_ID) + is_exposed + num_tasks + num_tasks_in_dwa_within_occupation'\n",
    "        # Rename column to avoid spaces in formula\n",
    "        df['DWA_ID'] = df['DWA ID']\n",
    "        # keep only the groups with variation in dependent variable\n",
    "        df_dwa = df.groupby(\"DWA_ID\").filter(lambda g: g[dependent_var].nunique() > 1)\n",
    "        res = smf.logit(formula, data=df_dwa).fit(\n",
    "            disp=False, \n",
    "            cov_type='cluster',\n",
    "            cov_kwds={'groups': df_dwa['DWA_ID'],\n",
    "                      'use_correction': True}\n",
    "        )\n",
    "        models['no_fe_with_dwa_withTaskDWACount'] = res\n",
    "        ame_list.append(build_ame_df(res, df_dwa, dataset_name, 'no_fe_with_dwa_withTaskDWACount', regressors, \n",
    "                                     fe_label=\"None\", dwa_fe=True, formula=formula,\n",
    "                                     calculate_standard_errors=calculate_standard_errors))\n",
    "        # print(f\"[{dataset_name}] No-FE, With-DWA model converged.\")\n",
    "    except Exception as e: \n",
    "        print(f\"[{dataset_name}] No-FE, With-DWA failed: {e}\")\n",
    "\n",
    "\n",
    "    # Combine results\n",
    "    combined = pd.concat(ame_list, ignore_index=True) if ame_list else pd.DataFrame()\n",
    "\n",
    "    # Save results to CSV\n",
    "    out_path = f'{output_data_path}/regression_summaries_{dependent_var}'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    combined.to_csv(f'{out_path}/regression_ame_results_{dataset_name}.csv', index=False)\n",
    "\n",
    "    return models, combined\n",
    "\n",
    "# ==========================================\n",
    "# 3. LaTeX Table Generator (MODIFIED FOR 6 COLUMNS)\n",
    "# ==========================================\n",
    "from io import StringIO\n",
    "\n",
    "def generate_latex_table(df_results, regression_table_name):\n",
    "    if df_results.empty:\n",
    "        raise ValueError(\"The input DataFrame is empty. Cannot generate LaTeX table.\")\n",
    "\n",
    "    buf = StringIO()\n",
    "    w = lambda s: buf.write(s + \"\\n\")\n",
    "\n",
    "    # Filter for one dataset\n",
    "    dataset_to_show = df_results['dataset'].unique()[0]\n",
    "    subset = df_results[df_results['dataset'] == dataset_to_show].copy()\n",
    "\n",
    "    w(f\"% --- LaTeX Table for {dataset_to_show} ---\")\n",
    "\n",
    "    # --- Formatting ---\n",
    "    def fmt(row):\n",
    "        stars = \"\"\n",
    "        p = row['p_value']\n",
    "        if pd.notna(p):\n",
    "            if p < 0.01: stars = \"***\"\n",
    "            elif p < 0.05: stars = \"**\"\n",
    "            elif p < 0.10: stars = \"*\"\n",
    "        return f\"{row['ame_coef']:.2f}{stars}\", f\"({row['ame_se']:.2f})\"\n",
    "\n",
    "    formatted = subset.apply(fmt, axis=1, result_type='expand')\n",
    "    subset['coef_str'] = formatted[0]\n",
    "    subset['se_str'] = formatted[1]\n",
    "\n",
    "    # Pivot\n",
    "    pivot_coef = subset.pivot(index='term', columns='model', values='coef_str')\n",
    "    pivot_se = subset.pivot(index='term', columns='model', values='se_str')\n",
    "\n",
    "    # Ordering\n",
    "    valid_vars = [v for v in TARGET_REGS if v in pivot_coef.index]\n",
    "    pivot_coef = pivot_coef.reindex(valid_vars)\n",
    "    pivot_se = pivot_se.reindex(valid_vars)\n",
    "\n",
    "    model_order = [\n",
    "        'no_fe_no_dwa', 'major_fe_no_dwa', 'minor_fe_no_dwa',\n",
    "        'no_fe_with_dwa',\n",
    "        'no_fe_no_dwa_withTaskDWACount', 'no_fe_with_dwa_withTaskDWACount'\n",
    "    ]\n",
    "    valid_models = [m for m in model_order if m in pivot_coef.columns]\n",
    "\n",
    "    stats = subset[\n",
    "        ['model', 'nobs', 'r2_pseudo', 'r2_adj_pseudo', 'fe_label', 'dwa_fe']\n",
    "    ].drop_duplicates('model').set_index('model')\n",
    "\n",
    "    # --- LaTeX ---\n",
    "    col_def = \"l\" + \"c\" * len(valid_models)\n",
    "    w(f\"\\\\begin{{tabular}}{{{col_def}}}\")\n",
    "    w(r\"\\toprule\")\n",
    "\n",
    "    header_nums = [f\"({i+1})\" for i in range(len(valid_models))]\n",
    "    w(\"Specification & \" + \" & \".join(header_nums) + r\" \\\\\")\n",
    "    w(r\"\\midrule\")\n",
    "\n",
    "    # Body\n",
    "    for var in valid_vars:\n",
    "        label = VAR_LABELS.get(var, var.replace('_', ' '))\n",
    "        c_vals = [pivot_coef.loc[var, m] if m in pivot_coef.columns else \"\" for m in valid_models]\n",
    "        s_vals = [pivot_se.loc[var, m] if m in pivot_se.columns else \"\" for m in valid_models]\n",
    "\n",
    "        w(f\"{label} & \" + \" & \".join(c_vals) + r\" \\\\\")\n",
    "        w(\" & \" + \" & \".join(s_vals) + r\" \\\\\")\n",
    "        w(r\"\\addlinespace\")\n",
    "\n",
    "    w(r\"\\midrule\")\n",
    "\n",
    "    # Footer\n",
    "    w(\"Pseudo $R^2$ & \" + \" & \".join(\n",
    "        f\"{stats.loc[m, 'r2_pseudo']:.3f}\" if m in stats.index else \"\"\n",
    "        for m in valid_models\n",
    "    ) + r\" \\\\\")\n",
    "\n",
    "    # w(\"Adj. Pseudo $R^2$ & \" + \" & \".join(\n",
    "    #     f\"{stats.loc[m, 'r2_adj_pseudo']:.3f}\" if m in stats.index else \"\"\n",
    "    #     for m in valid_models\n",
    "    # ) + r\" \\\\\")\n",
    "\n",
    "    w(\"Observations & \" + \" & \".join(\n",
    "        f\"{int(stats.loc[m, 'nobs']):,}\" if m in stats.index else \"\"\n",
    "        for m in valid_models\n",
    "    ) + r\" \\\\\")\n",
    "\n",
    "    fe_vals = []\n",
    "    for m in valid_models:\n",
    "        if m in stats.index:\n",
    "            label = stats.loc[m, 'fe_label']\n",
    "            if pd.isna(label) or str(label) == \"None\":\n",
    "                fe_vals.append(\"\")\n",
    "            elif \"Major\" in str(label):\n",
    "                fe_vals.append(\"Major\")\n",
    "            elif \"Minor\" in str(label):\n",
    "                fe_vals.append(\"Minor\")\n",
    "            else:\n",
    "                fe_vals.append(str(label))\n",
    "        else:\n",
    "            fe_vals.append(\"\")\n",
    "    w(\"SOC Group FE & \" + \" & \".join(fe_vals) + r\" \\\\\")\n",
    "\n",
    "    dwa_vals = []\n",
    "    for m in valid_models:\n",
    "        if m in stats.index:\n",
    "            if stats.loc[m, 'dwa_fe']:\n",
    "                dwa_vals.append(\"Yes\")\n",
    "            else:\n",
    "                dwa_vals.append(\"\")\n",
    "        else:\n",
    "            dwa_vals.append(\"\")\n",
    "    w(\"DWA FE & \" + \" & \".join(dwa_vals) + r\" \\\\\")\n",
    "\n",
    "    num_tasks_in_dwa_within_occupation_vals = []\n",
    "    for m in valid_models:\n",
    "        if m in stats.index:\n",
    "            if \"withTaskDWACount\" in m:\n",
    "                num_tasks_in_dwa_within_occupation_vals.append(\"Yes\")\n",
    "            else:\n",
    "                num_tasks_in_dwa_within_occupation_vals.append(\"\")\n",
    "        else:\n",
    "            num_tasks_in_dwa_within_occupation_vals.append(\"\")\n",
    "\n",
    "    w(\"NumTasks in DWA-Occupation Control & \" + \" & \".join(num_tasks_in_dwa_within_occupation_vals) + r\" \\\\\")\n",
    "\n",
    "    w(r\"\\bottomrule\")\n",
    "    w(r\"\\footnotesize{Clustered standard errors in parentheses. *** p$<$0.01, ** p$<$0.05, * p$<$0.1}\")\n",
    "    w(r\"\\end{tabular}\")\n",
    "\n",
    "\n",
    "    table_text = buf.getvalue()\n",
    "\n",
    "    out_path = Path(output_data_path) / \"tables\"\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(f\"{out_path}/{regression_table_name}\", \"w\") as f:\n",
    "        f.write(table_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69796c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regressions and compare results\n",
    "print(\">>> Running Regressions on All Eligible Tasks...\")\n",
    "models_full, res_full = run_regressions_on(merged_data, 'full_0', dependent_var, TARGET_REGS)\n",
    "generate_latex_table(res_full, f'allTasks_{dependent_var[3:]}.tex')\n",
    "\n",
    "# print(\"\\n>>> Running Regressions on original ONET DWAs that have the Ideal Setup...\")\n",
    "# models_filt, res_filt = run_regressions_on(original_ONET_ideal_DWAs_data, 'ideal_DWAs', dependent_var, TARGET_REGS)\n",
    "# generate_latex_table(res_filt)\n",
    "\n",
    "# print(\"\\n>>> Running Regressions on Filtered data where Tasks in Occupations with Multiple Tasks in DWAs are Filtered Out...\")\n",
    "# models_filt, res_filt = run_regressions_on(merged_data_without_repetitive_occupation_tasks, 'all_DWAs_onlyUniqueTasks', dependent_var, TARGET_REGS)\n",
    "# generate_latex_table(res_filt)\n",
    "\n",
    "print(\"\\n>>> Running Regressions on GPT-filtered data...\")\n",
    "models_filt, res_filt = run_regressions_on(filtered_data, 'filtered_0', dependent_var, TARGET_REGS)\n",
    "generate_latex_table(res_filt, f'GPT_{dependent_var[3:]}.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ======================================================\n",
    "# 1. Helper: Extract AME directly (No Transformation)\n",
    "# ======================================================\n",
    "def results_to_dict(df_results):\n",
    "    \"\"\"\n",
    "    Reads the dataframe output from run_regressions_on and converts\n",
    "    the 'ame_coef' column into a dictionary structure. Also returns\n",
    "    'ame_se' values as a separate dict.\n",
    "    Returns: (coef_dict, se_dict) with structure coef_dict[spec][term]\n",
    "    \"\"\"\n",
    "    coef_out = {spec: {term: np.nan for term in TARGET_REGS} for spec in SPECS}\n",
    "    se_out = {spec: {term: np.nan for term in TARGET_REGS} for spec in SPECS}\n",
    "    \n",
    "    if df_results is None or (hasattr(df_results, 'empty') and df_results.empty):\n",
    "        return coef_out, se_out\n",
    "\n",
    "    for _, row in df_results.iterrows():\n",
    "        spec = row.get('model')\n",
    "        term = row.get('term')\n",
    "        if spec in coef_out and term in coef_out[spec]:\n",
    "            if 'ame_coef' in row and pd.notna(row['ame_coef']):\n",
    "                coef_out[spec][term] = row['ame_coef']\n",
    "            if 'ame_se' in row and pd.notna(row['ame_se']):\n",
    "                se_out[spec][term] = row['ame_se']\n",
    "    return coef_out, se_out\n",
    "\n",
    "# Store observed values\n",
    "obs_dict_full, obs_se_full = results_to_dict(res_full)\n",
    "obs_dict_filt, obs_se_filt = results_to_dict(res_filt)\n",
    "\n",
    "# ======================================================\n",
    "# 2. Reshuffling Loop\n",
    "# ======================================================\n",
    "# Prepare containers for reshuffled AMEs\n",
    "resh_full = {spec: {t: [] for t in TARGET_REGS} for spec in SPECS}\n",
    "resh_filt = {spec: {t: [] for t in TARGET_REGS} for spec in SPECS}\n",
    "\n",
    "# Assuming n_shuffles is defined (e.g., 1000)\n",
    "print(f'Running {n_shuffles} reshuffles to generate Null Distribution of AMEs...')\n",
    "\n",
    "for i in range(n_shuffles):\n",
    "    seed = 42 + i\n",
    "    \n",
    "    # Use '_ame_summary.csv' to avoid loading old cached raw-coef files\n",
    "    fname_full = f\"{output_data_path}/regression_summaries_{dependent_var}/regression_ame_results_full_{i}.csv\"\n",
    "    fname_filt = f\"{output_data_path}/regression_summaries_{dependent_var}/regression_ame_results_filtered_{i}.csv\"\n",
    "\n",
    "    # --- Load or Compute ---\n",
    "    # if Path(fname_full).exists() and Path(fname_filt).exists():\n",
    "    if Path(fname_filt).exists():\n",
    "        # Load existing results (CSV produced by run_regressions_on)\n",
    "        res_shuf_full = pd.read_csv(fname_full)\n",
    "        res_shuf_filt = pd.read_csv(fname_filt)\n",
    "    else:\n",
    "        # Create Shuffled Data\n",
    "        df_shuf = merged_data.copy()\n",
    "        # Shuffle Task Position within O*NET Code\n",
    "        df_shuf['Task Position'] = df_shuf.groupby('O*NET-SOC Code')['Task Position'].transform(\n",
    "            lambda x: x.sample(frac=1, random_state=seed).values\n",
    "        )\n",
    "        \n",
    "        # Re-calculate neighbor flags based on shuffled positions\n",
    "        df_shuf = df_shuf.groupby(group_col, group_keys=False).apply(add_neighbor_flags).reset_index(drop=True)\n",
    "\n",
    "        # Drop rows where ANY neighbor flag is NA\n",
    "        neighbor_cols = ['prev_is_ai', 'prev2_is_ai', 'next_is_ai', 'next2_is_ai']\n",
    "        df_shuf = df_shuf.dropna(subset=neighbor_cols).reset_index(drop=True)\n",
    "\n",
    "        # Convert to int after dropping NAs\n",
    "        for col in neighbor_cols:\n",
    "            df_shuf[col] = df_shuf[col].astype(int)\n",
    "        \n",
    "        \n",
    "        # Run on Full Data\n",
    "        _, res_shuf_full = run_regressions_on(df_shuf, f'full_{i}', dependent_var=dependent_var, regressors=TARGET_REGS, calculate_standard_errors=False)\n",
    "        \n",
    "        # Run on GPT-filtered Data\n",
    "        df_shuf_filt = df_shuf[\n",
    "            (df_shuf['DWA ID'].isin(dwas_varying_exec_types_ids)\n",
    "             & \n",
    "             df_shuf['Task ID'].isin(gpt_filtered_task_ids)\n",
    "            )\n",
    "            ].reset_index(drop=True)\n",
    "        _, res_shuf_filt = run_regressions_on(df_shuf_filt, f'filtered_{i}', dependent_var=dependent_var, regressors=TARGET_REGS, calculate_standard_errors=False)\n",
    "    \n",
    "    # --- Store Results ---\n",
    "    d_full, d_full_se = results_to_dict(res_shuf_full)\n",
    "    d_filt, d_filt_se = results_to_dict(res_shuf_filt)\n",
    "\n",
    "    # Append to lists\n",
    "    for spec in SPECS:\n",
    "        for t in TARGET_REGS:\n",
    "            resh_full[spec][t].append(d_full[spec][t])\n",
    "            resh_filt[spec][t].append(d_filt[spec][t])\n",
    "\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(f'  Completed {i+1}/{n_shuffles}')\n",
    "\n",
    "print('Reshuffles complete; Marginal Effects stored in resh_full and resh_filt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55545e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os # Ensure os is imported for makedirs\n",
    "\n",
    "# --- Plotting: distributions of Marginal Effects (AME) ---\n",
    "def plot_comparison_hist(resh_dict, obs_dict, obs_se_dict, title, out_name, plot_title_variable, bins=30):\n",
    "    \"\"\"Create the multi-row comparison histogram and also save each row (spec) as a separate image.\n",
    "\n",
    "    Args:\n",
    "        resh_dict: dict of reshuffled AMEs per spec and term\n",
    "        obs_dict: dict of observed AMEs per spec and term\n",
    "        obs_se_dict: dict of observed AME standard errors per spec and term\n",
    "        title: title string to include in saved figures\n",
    "        out_name: filename for the full multi-row figure\n",
    "        plot_title_variable: human-readable dependent var name for titles\n",
    "        bins: histogram bins\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Calculate Global Bounds for X-Axis ---\n",
    "    all_resh_vals = [v \n",
    "                     for inner_dict in resh_dict.values()\n",
    "                     for values_list in inner_dict.values()\n",
    "                     for v in values_list if not np.isnan(v)]\n",
    "    \n",
    "    all_obs_vals = [v \n",
    "                    for inner_dict in obs_dict.values()\n",
    "                    for v in inner_dict.values() if not np.isnan(v)]\n",
    "    \n",
    "    total_vals = all_resh_vals + all_obs_vals\n",
    "    if not total_vals:\n",
    "        print(\"Warning: No valid data found to plot.\")\n",
    "        return\n",
    "\n",
    "    g_min, g_max = min(total_vals), max(total_vals)\n",
    "    symmetric_bound = max(abs(g_min), abs(g_max))\n",
    "    span = 2 * symmetric_bound\n",
    "    if span == 0: span = 0.1\n",
    "    x_limit_min = -symmetric_bound - (span * 0.01)\n",
    "    x_limit_max = symmetric_bound + (span * 0.01)\n",
    "\n",
    "    # --- 2. Setup Plot ---\n",
    "    colors = [plt.cm.tab10(i % 10) for i in range(len(SPECS))]\n",
    "    fig, axes = plt.subplots(nrows=len(SPECS), ncols=len(TARGET_REGS), \n",
    "                             figsize=(6*len(TARGET_REGS), 5*len(SPECS)), \n",
    "                             sharey='col')\n",
    "\n",
    "    if len(SPECS) == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for r, spec in enumerate(SPECS):\n",
    "        color_row = colors[r]\n",
    "        for c, term in enumerate(TARGET_REGS):\n",
    "            ax = axes[r, c]\n",
    "            vals = np.array(resh_dict[spec][term], dtype=float)\n",
    "            vals_clean = vals[~np.isnan(vals)]\n",
    "\n",
    "            if len(vals_clean):\n",
    "                # # Shade reshuffle 95% CI behind the histogram\n",
    "                # lo, hi = np.percentile(vals_clean, [2.5, 97.5])\n",
    "                # # Clip to x-limits\n",
    "                # lo = max(lo, x_limit_min)\n",
    "                # hi = min(hi, x_limit_max)\n",
    "                # if hi <= lo:\n",
    "                #     eps = 1e-8 if abs(lo) > 0 else 1e-4\n",
    "                #     lo, hi = lo - eps, hi + eps\n",
    "                # ax.axvspan(lo, hi, color=color_row, alpha=0.12, zorder=0)\n",
    "\n",
    "                # Draw histogram on top of the shaded CI\n",
    "                ax.hist(vals_clean, bins=bins, color=color_row, alpha=0.7, edgecolor='k', label='Task Position Reshuffled AMEs', zorder=2)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'no estimates', ha='center', va='center')\n",
    "\n",
    "            # Observed AME (red dashed) and 1.645*SE (vertical lines/shade)\n",
    "            obs_val = obs_dict.get(spec, {}).get(term, np.nan)\n",
    "            if not np.isnan(obs_val):\n",
    "                obs_se = obs_se_dict.get(spec, {}).get(term, np.nan)\n",
    "                if not np.isnan(obs_se):\n",
    "                    se_band = 1.645 * obs_se\n",
    "                    # Shade observed SE band slightly above histogram but under the observed line\n",
    "                    ax.axvspan(obs_val - se_band, obs_val + se_band, color='red', alpha=0.08, zorder=1)\n",
    "                    # Also draw thin boundary lines for the observed SE band\n",
    "                    ax.axvline(obs_val - se_band, color='red', linestyle='--', linewidth=1, alpha=0.9, zorder=3)\n",
    "                    ax.axvline(obs_val + se_band, color='red', linestyle='--', linewidth=1, alpha=0.9, zorder=3)\n",
    "                # Observed center line on top\n",
    "                ax.axvline(obs_val, color='red', linestyle='--', linewidth=3, label=f'Observed = {obs_val:.3f}', zorder=4)\n",
    "\n",
    "            # Baseline (No Effect) set to 0\n",
    "            ax.axvline(0.0, color='black', linestyle='-', linewidth=1.5, alpha=0.5, zorder=4)\n",
    "\n",
    "            # Titles and Labels\n",
    "            if r == 0:\n",
    "                clean_title = VAR_LABELS.get(term, term) if 'VAR_LABELS' in globals() else PLOT_TITLES[c]\n",
    "                ax.set_title(clean_title, fontsize=15, fontweight='bold')\n",
    "            if r == len(SPECS) - 1:\n",
    "                ax.set_xlabel('Average Marginal Effect', fontsize=15)\n",
    "            if c == 0:\n",
    "                clean_spec = spec.replace('fe_', '').replace('_', ' ').title()\n",
    "                ax.set_ylabel(f'{clean_spec}\\nCount', fontsize=15)\n",
    "\n",
    "            # Apply consistent X-limits and grid\n",
    "            ax.set_xlim(x_limit_min, x_limit_max)\n",
    "            ax.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "            ax.legend(loc='best', fontsize=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Ensure output dir exists\n",
    "    Path(output_plot_path).mkdir(parents=True, exist_ok=True)\n",
    "    out_dir = f'{output_plot_path}/{dependent_var}'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = f'{out_dir}/{out_name}'\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    print('Saved full multi-row plot to', out_path)\n",
    "\n",
    "    # --- 3. Save Individual Rows ---\n",
    "    base_name = out_name.rsplit('.', 1)[0]\n",
    "    for r, spec in enumerate(SPECS):\n",
    "        fig_row, axs_row = plt.subplots(nrows=1, ncols=len(TARGET_REGS), figsize=(24, 5), sharey=False)\n",
    "        if len(TARGET_REGS) == 1: axs_row = [axs_row]\n",
    "        color_row = colors[r]\n",
    "        for c, term in enumerate(TARGET_REGS):\n",
    "            axr = axs_row[c]\n",
    "            vals = np.array(resh_dict[spec][term], dtype=float)\n",
    "            vals_clean = vals[~np.isnan(vals)]\n",
    "\n",
    "            if len(vals_clean):\n",
    "                # lo, hi = np.percentile(vals_clean, [2.5, 97.5])\n",
    "                # lo = max(lo, x_limit_min)\n",
    "                # hi = min(hi, x_limit_max)\n",
    "                # if hi <= lo:\n",
    "                #     eps = 1e-8 if abs(lo) > 0 else 1e-4\n",
    "                #     lo, hi = lo - eps, hi + eps\n",
    "                # axr.axvspan(lo, hi, color=color_row, alpha=0.12, zorder=0)\n",
    "                axr.hist(vals_clean, bins=bins, color=color_row, alpha=0.7, edgecolor='k', label='Task Position Reshuffled AMEs', zorder=2)\n",
    "            else:\n",
    "                axr.text(0.5, 0.5, 'no estimates', ha='center', va='center')\n",
    "\n",
    "            obs_val = obs_dict.get(spec, {}).get(term, np.nan)\n",
    "            if not np.isnan(obs_val):\n",
    "                obs_se = obs_se_dict.get(spec, {}).get(term, np.nan)\n",
    "                if not np.isnan(obs_se):\n",
    "                    se_band = 1.645 * obs_se\n",
    "                    axr.axvspan(obs_val - se_band, obs_val + se_band, color='red', alpha=0.08, zorder=1)\n",
    "                    axr.axvline(obs_val - se_band, color='red', linestyle='--', linewidth=1, alpha=0.9, zorder=3)\n",
    "                    axr.axvline(obs_val + se_band, color='red', linestyle='--', linewidth=1, alpha=0.9, zorder=3)\n",
    "                axr.axvline(obs_val, color='red', linestyle='--', linewidth=3, label=f'Observed = {obs_val:.3f}', zorder=4)\n",
    "\n",
    "            # Zero line\n",
    "            axr.axvline(0.0, color='black', linestyle='-', linewidth=1.5, alpha=0.5, zorder=4)\n",
    "            clean_title = VAR_LABELS.get(term, term) if 'VAR_LABELS' in globals() else PLOT_TITLES[c]\n",
    "            axr.set_title(clean_title, fontsize=15)\n",
    "            if c == 0:\n",
    "                axr.set_ylabel('Count', fontsize=15)\n",
    "            axr.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "            axr.set_xlim(x_limit_min, x_limit_max)\n",
    "            axr.set_xlabel('Average Marginal Effect', fontsize=15)\n",
    "            axr.legend(loc='best', fontsize=10)\n",
    "\n",
    "        fig_row.tight_layout()\n",
    "        out_path_row = f'{out_dir}/{base_name}_{spec}.png'\n",
    "        fig_row.savefig(out_path_row, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig_row)\n",
    "        print('Saved row plot to', out_path_row)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "Path(output_plot_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plot_comparison_hist(\n",
    "    resh_full, \n",
    "    obs_dict_full, \n",
    "    obs_se_full, \n",
    "    f'FULL Dataset (n={n_shuffles})', \n",
    "    f'AME_full_{dependent_var}.png', \n",
    "    plot_title_variable\n",
    ")\n",
    "\n",
    "\n",
    "plot_comparison_hist(\n",
    "    resh_filt, \n",
    "    obs_dict_filt, \n",
    "    obs_se_filt, \n",
    "    f'FILTERED Dataset (n={n_shuffles})', \n",
    "    f'AME_filtered_{dependent_var}.png', \n",
    "    plot_title_variable\n",
    ")\n",
    "\n",
    "print('All done: comparative Marginal Effect histogram figures created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
