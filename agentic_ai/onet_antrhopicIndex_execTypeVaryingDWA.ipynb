{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 19, 2025\n",
    "#### Last Edit: Nov 9, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/execTypeVaryingDWA_anthropicIndex'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots/anthropic_AI_index/execTypeVaryingDWA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac02f0",
   "metadata": {},
   "source": [
    "## Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d28924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of reshuffles\n",
    "n_shuffles = 1000\n",
    "\n",
    "\n",
    "# dependent_var = 'is_ai'\n",
    "# plot_title_variable = 'Task is AI'\n",
    "dependent_var = 'is_automated'\n",
    "plot_title_variable = 'Task is Automated'\n",
    "\n",
    "\n",
    "TARGET_REGS = ['prev2_is_ai', 'prev_is_ai', 'next_is_ai', 'next2_is_ai']\n",
    "SPECS = ['no_fe', 'fe_MajorGroup', 'fe_MinorGroup']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887655d",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of DWAs with tasks in multiple occupations\n",
    "dwa_list_path = f\"{input_data_path}/computed_objects/similar_dwa_tasks/similarTasks\"\n",
    "\n",
    "# Read all CSV files\n",
    "import glob\n",
    "dwa_csv_files = glob.glob(os.path.join(dwa_list_path, \"*.csv\"))\n",
    "print(f\"Found {len(dwa_csv_files)} DWA CSV files.\")\n",
    "\n",
    "# Load them into DataFrames, skipping 1-row files\n",
    "dwa_dfs = []\n",
    "skipped_files_count = 0\n",
    "for f in dwa_csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if len(df) > 1: # Skip if DWA contains only one task\n",
    "        dwa_dfs.append(df)\n",
    "    else:\n",
    "        skipped_files_count += 1\n",
    "print(f\"Skipped {skipped_files_count} DWA files with only one task.\")\n",
    "    \n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_all = pd.concat(dwa_dfs, ignore_index=True)\n",
    "repetitive_dwa_task_ids = df_all['Task ID'].unique().tolist()\n",
    "repetitive_dwa_task_titles = df_all['Task Title'].unique().tolist()\n",
    "print(f\"Found {len(repetitive_dwa_task_ids)} tasks related to these DWAs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DWA-level dataset with number of tasks and occupations per DWA, as well as fraction of manual, automation, and augmentation tasks per DWA\n",
    "merged_data = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\")\n",
    "merged_data['is_manual'] = merged_data['label'] == 'Manual'\n",
    "merged_data['is_automation'] = merged_data['label'] == 'Automation'\n",
    "merged_data['is_augmentation'] = merged_data['label'] == 'Augmentation'\n",
    "\n",
    "\n",
    "# Merge back DWA ID and DWA Titles to the merged_data\n",
    "dwa_task_mapping = pd.read_csv(f\"{input_data_path}/computed_objects/similar_dwa_tasks/dwa_task_mapping.csv\")\n",
    "print(f'Length of merged_data before merging DWA info: {merged_data.shape[0]}')\n",
    "merged_data = merged_data.merge(dwa_task_mapping, on=['Task ID', 'Task Title', 'O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "print(f'Length of merged_data after merging DWA info: {merged_data.shape[0]}')\n",
    "\n",
    "\n",
    "# Aggregate to get fractions\n",
    "dwa_grouped = merged_data.groupby(['DWA ID', 'DWA Title']).agg(\n",
    "    num_tasks = ('Task ID', 'nunique'),\n",
    "    num_occupations = ('O*NET-SOC Code', 'nunique'),\n",
    "    fraction_manual = ('is_manual', 'mean'),\n",
    "    fraction_automation = ('is_automation', 'mean'),\n",
    "    fraction_augmentation = ('is_augmentation', 'mean'),\n",
    ").reset_index()\n",
    "print(f\"Created DWA-level dataset with {dwa_grouped.shape[0]} DWAs.\")\n",
    "\n",
    "# Keep only DWAs with variation in terms of execution type across occupations\n",
    "dwa_grouped_filtered = dwa_grouped[\n",
    "     (dwa_grouped['num_occupations'] > 1) & (dwa_grouped['fraction_manual'] > 0) & (dwa_grouped['fraction_manual'] < 1)\n",
    "].copy()\n",
    "display(dwa_grouped_filtered)\n",
    "\n",
    "# Create list of DWAs with varying execution types\n",
    "dwas_varying_exec_types_ids = dwa_grouped_filtered['DWA ID'].unique().tolist()\n",
    "dwas_varying_exec_types_titles = dwa_grouped_filtered['DWA Title'].unique().tolist()\n",
    "print(f\"Identified {len(dwas_varying_exec_types_ids)} DWAs with varying execution types across occupations.\")\n",
    "\n",
    "# Save output\n",
    "dwa_grouped_filtered.to_csv(f\"{output_data_path}/dwas_varying_execution_types.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a84d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged data\n",
    "merged_data = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\")\n",
    "merged_data = merged_data[['O*NET-SOC Code', 'Occupation Title', 'Task ID', 'Task Title',\n",
    "       'Task Position', 'Task Type', \n",
    "       'Major_Group_Code', 'Major_Group_Title', \n",
    "       'Minor_Group_Code', 'Minor_Group_Title',\n",
    "       'Broad_Occupation_Code', 'Broad_Occupation_Title',\n",
    "       'Detailed_Occupation_Code', 'Detailed_Occupation_Title',\n",
    "       'gpt4_exposure', 'human_labels', \n",
    "       'automation', 'augmentation', 'label']]\n",
    "\n",
    "\n",
    "# Create is_ai and is_automated flags in merged_data\n",
    "merged_data['is_ai'] = merged_data['label'].isin(['Augmentation','Automation']).astype(int)\n",
    "merged_data['is_automated'] = merged_data['label'].isin(['Automation']).astype(int)\n",
    "\n",
    "\n",
    "# # Step 0: Randomize Task Position for sanity check\n",
    "# merged_data[\"Task Position\"] = (\n",
    "#     merged_data.groupby(\"O*NET-SOC Code\")[\"Task Position\"]\n",
    "#     .transform(lambda x: x.sample(frac=1, random_state=42).values))\n",
    "\n",
    "\n",
    "# Step 1: Create flags for previous/next tasks is AI within occupation groups\n",
    "# Sort by occupation and position when possible\n",
    "merged_data['Task Position'] = pd.to_numeric(merged_data['Task Position'], errors='coerce')\n",
    "merged_data = merged_data.sort_values(['O*NET-SOC Code', 'Task Position']).reset_index(drop=True)\n",
    "group_col = 'O*NET-SOC Code'\n",
    "\n",
    "# Compute neighbor flags (prev/next) within occupation groups when possible\n",
    "merged_data['prev_is_ai'] = 0\n",
    "merged_data['prev2_is_ai'] = 0\n",
    "merged_data['next_is_ai'] = 0\n",
    "merged_data['next2_is_ai'] = 0\n",
    "pos_col = 'Task Position'\n",
    "\n",
    "def add_neighbor_flags(df):\n",
    "    df = df.copy()\n",
    "    df['Task Position'] = pd.to_numeric(df['Task Position'], errors='coerce')\n",
    "    df = df.sort_values(['O*NET-SOC Code','Task Position']).reset_index(drop=True)\n",
    "    def _add_flags(g):\n",
    "        g = g.sort_values('Task Position')\n",
    "        g['prev_is_ai'] = g['is_ai'].shift(1).fillna(0).astype(int)\n",
    "        g['prev2_is_ai'] = g['is_ai'].shift(2).fillna(0).astype(int)\n",
    "        # g['prev2_is_ai'] = ((g['prev2_is_ai'] == 1) & (g['prev_is_ai'] == 1)).astype(int)\n",
    "        g['next_is_ai'] = g['is_ai'].shift(-1).fillna(0).astype(int)\n",
    "        g['next2_is_ai'] = g['is_ai'].shift(-2).fillna(0).astype(int)\n",
    "        # g['next2_is_ai'] = ((g['next2_is_ai'] == 1) & (g['next_is_ai'] == 1)).astype(int)\n",
    "        return g\n",
    "    return df.groupby('O*NET-SOC Code', group_keys=False).apply(_add_flags).reset_index(drop=True)\n",
    "merged_data = merged_data.groupby(group_col, group_keys=False).apply(add_neighbor_flags).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Add back DWA info\n",
    "# Merge back DWA ID and DWA Titles to the merged_data\n",
    "dwa_task_mapping = pd.read_csv(f\"{input_data_path}/computed_objects/similar_dwa_tasks/dwa_task_mapping.csv\")\n",
    "merged_data = merged_data.merge(dwa_task_mapping, on=['Task ID', 'Task Title', 'O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "# Note that the merge might map multiple DWAs to the same task\n",
    "\n",
    "\n",
    "# Step 3: Flag tasks whose DWA ID appears in dwa_execTypeVarying_id_list\n",
    "merged_data['dwa_execType_varying'] = merged_data.get('DWA ID', pd.Series()).isin(dwas_varying_exec_types_ids).fillna(False).astype(int)\n",
    "\n",
    "# Remove duplicates in terms of (O*NET-SOC Code, Task ID) if any\n",
    "print(f'Length of merged_data before dropping duplicates: {merged_data.shape[0]}')\n",
    "merged_data = merged_data.drop_duplicates(subset=['O*NET-SOC Code', 'Task ID'])\n",
    "print(f'Length of merged_data after dropping duplicates: {merged_data.shape[0]}')\n",
    "\n",
    "\n",
    "# Summary for flagged DWA rows\n",
    "mask = merged_data['dwa_execType_varying'] == 1\n",
    "n_flagged = int(mask.sum())\n",
    "print(f'\\nNumber of dwa_execType_varying rows: {n_flagged}')\n",
    "if n_flagged > 0:\n",
    "    for c in ['prev2_is_ai', 'prev_is_ai', 'next_is_ai', 'next2_is_ai']:\n",
    "        s = int(merged_data.loc[mask, c].sum())\n",
    "        frac = merged_data.loc[mask, c].mean()\n",
    "        print(f'{c}: {s} of {n_flagged} flagged rows (fraction={frac:.3f})')\n",
    "    try:\n",
    "        display(merged_data.loc[mask].head())\n",
    "    except Exception:\n",
    "        print(merged_data.loc[mask].head().to_string(index=False))\n",
    "else:\n",
    "    print('No flagged rows to summarize.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e805dee",
   "metadata": {},
   "source": [
    "### Run regression of multiple-execution-type DWA tasks against execution type of neighboring tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressions: neighbor flags on is_ai (Logit, no FE + several FE models)\n",
    "# Assumption: dependent variable = 'is_ai' and regressors are the four neighbor flags\n",
    "# (prev2_is_ai, prev_is_ai, next_is_ai, next2_is_ai).\n",
    "# Runs on full `merged_data` and on filtered subset where dwa_execType_varying==1.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from pathlib import Path\n",
    "from statsmodels.stats.sandwich_covariance import cov_hc1\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# ------- helpers -------\n",
    "# Extract params, robust SEs (HC1) and model sample/df info\n",
    "def extract_coef_series(res):\n",
    "    try:\n",
    "        if hasattr(res, 'get_robustcov_results'):\n",
    "            r = res.get_robustcov_results(cov_type='HC1')\n",
    "            nobs = getattr(r, 'nobs', getattr(res, 'nobs', np.nan))\n",
    "            df_resid = getattr(r, 'df_resid', getattr(res, 'df_resid', np.nan))\n",
    "            return r.params, r.bse, r.pvalues, nobs, df_resid\n",
    "        else:\n",
    "            params = res.params\n",
    "            try:\n",
    "                robust_cov = cov_hc1(res)\n",
    "                bse_arr = np.sqrt(np.diag(robust_cov))\n",
    "                z_scores = params.values / bse_arr\n",
    "                pvals = 2 * (1 - norm.cdf(np.abs(z_scores)))\n",
    "                bse = pd.Series(bse_arr, index=params.index)\n",
    "                pvalues = pd.Series(pvals, index=params.index)\n",
    "                nobs = getattr(res, 'nobs', np.nan)\n",
    "                df_resid = getattr(res, 'df_resid', np.nan)\n",
    "                return params, bse, pvalues, nobs, df_resid\n",
    "            except Exception:\n",
    "                bse = getattr(res, 'bse', pd.Series(np.nan, index=params.index))\n",
    "                pvalues = getattr(res, 'pvalues', pd.Series(np.nan, index=params.index))\n",
    "                nobs = getattr(res, 'nobs', np.nan)\n",
    "                df_resid = getattr(res, 'df_resid', np.nan)\n",
    "                return params, bse, pvalues, nobs, df_resid\n",
    "    except Exception:\n",
    "        params = getattr(res, 'params', pd.Series())\n",
    "        bse = getattr(res, 'bse', pd.Series())\n",
    "        pvalues = getattr(res, 'pvalues', pd.Series())\n",
    "        nobs = getattr(res, 'nobs', np.nan)\n",
    "        df_resid = getattr(res, 'df_resid', np.nan)\n",
    "        return params, bse, pvalues, nobs, df_resid\n",
    "\n",
    "# Build tidy coef dataframe for target regs\n",
    "def build_tidy_coefs(res, dataset_name, model_name):\n",
    "    params, bse, pvalues, nobs, df_resid = extract_coef_series(res)\n",
    "    if len(params) == 0:\n",
    "        return pd.DataFrame(columns=['dataset','model','term','coef','std_err','p_value','nobs','df_resid'])\n",
    "    df = pd.DataFrame({\n",
    "        'term': params.index.astype(str),\n",
    "        'coef': params.values,\n",
    "        'std_err': bse.values if hasattr(bse, 'values') else np.array(bse),\n",
    "        'p_value': pvalues.values if hasattr(pvalues, 'values') else np.array(pvalues)\n",
    "    })\n",
    "    df['model'] = model_name\n",
    "    df['dataset'] = dataset_name\n",
    "    df['nobs'] = nobs\n",
    "    df['df_resid'] = df_resid\n",
    "    df = df[df['term'].isin(TARGET_REGS)].reset_index(drop=True)\n",
    "    df = df[['dataset','model','nobs','df_resid','term','coef','std_err','p_value']]\n",
    "    return df\n",
    "\n",
    "# Drop FE groups with no within-group variation in y or too small size\n",
    "def keep_var_groups(df, fe_col, y, min_size=2):\n",
    "    return df.groupby(fe_col).filter(lambda g: g[y].nunique() == 2 and len(g) >= min_size)\n",
    "\n",
    "# Core function to run requested regressions on a DataFrame\n",
    "def run_regressions_on(df, dataset_name, dependent_var, regressors):\n",
    "    df = df.copy()\n",
    "\n",
    "    # make sure regressors and outcome exist and are numeric\n",
    "    regs = TARGET_REGS\n",
    "    for r in regs + [dependent_var]:\n",
    "        if r not in df.columns:\n",
    "            raise KeyError(f'Required column {r} not found in dataset {dataset_name}')\n",
    "    df[regs] = df[regs].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    df[dependent_var] = pd.to_numeric(df[dependent_var], errors='coerce').fillna(0)\n",
    "\n",
    "    regressors_str = ' + '.join(regressors)\n",
    "    base_formula = f'{dependent_var} ~ {regressors_str}'\n",
    "    models = {}\n",
    "    tidy_list = []\n",
    "\n",
    "    # 1) No fixed effects (Logit)\n",
    "    try:\n",
    "        res_no_fe = smf.logit(base_formula, data=df).fit(disp=False)\n",
    "        models['no_fe'] = res_no_fe\n",
    "        tidy_list.append(build_tidy_coefs(res_no_fe, dataset_name, 'no_fe'))\n",
    "    except Exception as e:\n",
    "        print('No-FE logit model failed:', e)\n",
    "\n",
    "    # 2) Fixed effects models (each separately)\n",
    "    fe_cols = [\n",
    "        ('Major_Group_Code','MajorGroup'),\n",
    "        ('Minor_Group_Code','MinorGroup')\n",
    "    ]\n",
    "\n",
    "    for col, short in fe_cols:\n",
    "        formula = base_formula + f' + C({col})'\n",
    "\n",
    "        # Drop problematic FE groups for THIS FE\n",
    "        df_fe = keep_var_groups(df, col, y=dependent_var, min_size=2)\n",
    "        kept = df_fe[col].nunique()\n",
    "        dropped = df[col].nunique() - kept\n",
    "\n",
    "        try:\n",
    "            res = smf.logit(formula, data=df_fe).fit(disp=False)\n",
    "            models[f'fe_{short}'] = res\n",
    "            tidy_list.append(build_tidy_coefs(res, dataset_name, f'fe_{short}'))\n",
    "        except Exception as e:\n",
    "            print(f'FE logit model with {col} failed:', e)\n",
    "\n",
    "    # Combine tidy coeffs for this dataset\n",
    "    if len(tidy_list):\n",
    "        tidy_combined = pd.concat(tidy_list, ignore_index=True)\n",
    "    else:\n",
    "        tidy_combined = pd.DataFrame(columns=['dataset','model','nobs','df_resid','term','coef','std_err','p_value'])\n",
    "\n",
    "    # Save per-dataset four-variable summary\n",
    "    out_dir = Path(f'{output_data_path}/regression_summaries_{dependent_var}')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f'regression_{dataset_name}_fourvars_summary.csv'\n",
    "    tidy_combined.to_csv(out_path, index=False)\n",
    "    # print(f'Saved per-dataset four-variable summary to {out_path}')\n",
    "\n",
    "    return models, tidy_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa80b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regression on original datasets: full and filtered (dwa_execType_varying == 1)\n",
    "print('Starting regressions on full merged_data')\n",
    "print(f'Full dataset has {merged_data.shape[0]} rows.')\n",
    "models_full, coefs_full = run_regressions_on(merged_data, 'full_merged_data', dependent_var=dependent_var, regressors=TARGET_REGS)\n",
    "\n",
    "\n",
    "print('\\nStarting regressions on filtered dataset (dwa_execType_varying == 1)')\n",
    "filtered = merged_data[merged_data['dwa_execType_varying'] == 1].reset_index(drop=True)\n",
    "print(f'Filtered dataset has {filtered.shape[0]} rows.')\n",
    "models_filtered, coefs_filtered = run_regressions_on(filtered, 'filtered_dwaExecTypeVarying', dependent_var=dependent_var, regressors=TARGET_REGS)\n",
    "\n",
    "\n",
    "combined_all = pd.concat([coefs_full, coefs_filtered], ignore_index=True)\n",
    "final_out = Path(output_data_path) / f'regression_fourvars_allDatasets_{dependent_var}.csv'\n",
    "combined_all.to_csv(final_out, index=False)\n",
    "print(f'Combined four-variable CSV saved to {final_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- logistic transformation ---\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x)) if not np.isnan(x) else np.nan\n",
    "\n",
    "# Compute observed (non-reshuffled) coefficients\n",
    "print('Computing observed coefficients (original ordering)')\n",
    "obs_models_full, obs_coefs_full = run_regressions_on(merged_data, 'obs_full', dependent_var=dependent_var, regressors=TARGET_REGS)\n",
    "filtered_obs = merged_data[merged_data['dwa_execType_varying'] == 1].reset_index(drop=True)\n",
    "obs_models_filt, obs_coefs_filt = run_regressions_on(filtered_obs, 'obs_filtered', dependent_var=dependent_var, regressors=TARGET_REGS)\n",
    "\n",
    "# --- convert tidy results to dict of propensities instead of raw coefs ---\n",
    "def tidy_to_dict(tidy_df):\n",
    "    out = {spec: {term: np.nan for term in TARGET_REGS} for spec in SPECS}\n",
    "    for _, row in tidy_df.iterrows():\n",
    "        spec = row['model']\n",
    "        term = row['term']\n",
    "        if spec in out and term in out[spec]:\n",
    "            coef = row['coef']\n",
    "            out[spec][term] = logistic(coef)  # convert to propensity\n",
    "    return out\n",
    "\n",
    "obs_dict_full = tidy_to_dict(obs_coefs_full)\n",
    "obs_dict_filt = tidy_to_dict(obs_coefs_filt)\n",
    "\n",
    "# Prepare containers for reshuffled *propensities*\n",
    "resh_full = {spec: {t: [] for t in TARGET_REGS} for spec in SPECS}\n",
    "resh_filt = {spec: {t: [] for t in TARGET_REGS} for spec in SPECS}\n",
    "\n",
    "out_dir = Path(f\"{output_data_path}/regression_summaries_{dependent_var}\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Running {n_shuffles} reshuffles...')\n",
    "for i in range(n_shuffles):\n",
    "    seed = 42 + i\n",
    "    fname_full = out_dir / f'regression_shuf_full_{i}_{dependent_var}_fourvars_summary.csv'\n",
    "    fname_filt = out_dir / f'regression_shuf_filt_{i}_{dependent_var}_fourvars_summary.csv'\n",
    "\n",
    "    if fname_full.exists() and fname_filt.exists():\n",
    "        coefs_shuf_full = pd.read_csv(fname_full)\n",
    "        coefs_shuf_filt = pd.read_csv(fname_filt)\n",
    "        print(f'  Seed {i}: loaded existing results')\n",
    "    else:\n",
    "        df_shuf = merged_data.copy()\n",
    "        df_shuf['Task Position'] = df_shuf.groupby('O*NET-SOC Code')['Task Position'].transform(\n",
    "            lambda x: x.sample(frac=1, random_state=seed).values\n",
    "        )\n",
    "        df_shuf = add_neighbor_flags(df_shuf)\n",
    "        _, coefs_shuf_full = run_regressions_on(df_shuf, f'shuf_full_{i}', dependent_var=dependent_var, regressors=TARGET_REGS)\n",
    "        \n",
    "        df_shuf_filt = df_shuf[df_shuf['dwa_execType_varying'] == 1].reset_index(drop=True)\n",
    "        _, coefs_shuf_filt = run_regressions_on(df_shuf_filt, f'shuf_filt_{i}', dependent_var=dependent_var, regressors=TARGET_REGS)\n",
    "\n",
    "    # Convert to dict of propensities\n",
    "    d_full = tidy_to_dict(coefs_shuf_full) if not coefs_shuf_full.empty else tidy_to_dict(pd.DataFrame())\n",
    "    d_filt = tidy_to_dict(coefs_shuf_filt) if not coefs_shuf_filt.empty else tidy_to_dict(pd.DataFrame())\n",
    "\n",
    "    for spec in SPECS:\n",
    "        for t in TARGET_REGS:\n",
    "            resh_full[spec][t].append(d_full.get(spec, {}).get(t, np.nan))\n",
    "            resh_filt[spec][t].append(d_filt.get(spec, {}).get(t, np.nan))\n",
    "\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(f'  Completed {i+1}/{n_shuffles}')\n",
    "\n",
    "print('Reshuffles complete; creating comparative plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30477f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting: distributions of propensities ---\n",
    "def plot_comparison_hist(resh_dict, obs_dict, title, out_name, plot_title_variable, bins=30):\n",
    "    \"\"\"Create the multi-row comparison histogram and also save each row (spec) as a separate image.\n",
    "\n",
    "    Args:\n",
    "        resh_dict: dict of reshuffled propensities per spec and term\n",
    "        obs_dict: dict of observed propensities per spec and term\n",
    "        title: title string to include in saved figures\n",
    "        out_name: filename for the full multi-row figure\n",
    "        plot_title_variable: human-readable dependent var name for titles\n",
    "        bins: histogram bins\n",
    "    \"\"\"\n",
    "    colors = [plt.cm.tab10(i % 10) for i in range(len(SPECS))]\n",
    "    fig, axes = plt.subplots(nrows=len(SPECS), ncols=len(TARGET_REGS), figsize=(6*len(TARGET_REGS), 5*len(SPECS)), sharey='col')\n",
    "\n",
    "    for r, spec in enumerate(SPECS):\n",
    "        color_row = colors[r]\n",
    "        for c, term in enumerate(TARGET_REGS):\n",
    "            ax = axes[r, c] if len(SPECS) > 1 else axes[c]\n",
    "            vals = np.array(resh_dict[spec][term], dtype=float)\n",
    "            vals_clean = vals[~np.isnan(vals)]\n",
    "\n",
    "            if len(vals_clean):\n",
    "                ax.hist(vals_clean, bins=bins, color=color_row, alpha=0.7, edgecolor='k')\n",
    "                lo, hi = np.percentile(vals_clean, [2.5, 97.5])\n",
    "                ax.axvline(lo, color=color_row, linestyle=':', alpha=0.8)\n",
    "                ax.axvline(hi, color=color_row, linestyle=':', alpha=0.8)\n",
    "                ax.axvline(np.mean(vals_clean), color=color_row, linestyle='-', alpha=0.9)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'no estimates', ha='center', va='center')\n",
    "\n",
    "            # observed propensity (red dashed)\n",
    "            obs_val = obs_dict.get(spec, {}).get(term, np.nan)\n",
    "            if not np.isnan(obs_val):\n",
    "                ax.axvline(obs_val, color='red', linestyle='--', linewidth=3, label='observed')\n",
    "\n",
    "            # baseline: random (0.5 probability)\n",
    "            ax.axvline(0.5, color='black', linestyle='--', linewidth=2, alpha=0.7, label='0.5 baseline')\n",
    "\n",
    "            if r == 0:\n",
    "                ax.set_title(term, fontsize=12)\n",
    "            if c == 0:\n",
    "                ax.set_ylabel(spec, fontsize=12)\n",
    "            ax.set_xlim(0.42, 0.78)\n",
    "\n",
    "            ax.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "\n",
    "    fig.suptitle(f'Reshuffled vs. Observed Propensity (P[ {plot_title_variable} | Neighbor AI ])\\n\\n{title}',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "\n",
    "    # Ensure output dir exists\n",
    "    Path(output_plot_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save full multi-row figure\n",
    "    out_dir = f'{output_plot_path}/{dependent_var}'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = f'{out_dir}/{out_name}'\n",
    "    fig.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "    print('Saved full multi-row plot to', out_path)\n",
    "\n",
    "    # Also save each row (each spec) as its own figure while keeping the multi-row output\n",
    "    base_name = out_name.rsplit('.', 1)[0]\n",
    "    for r, spec in enumerate(SPECS):\n",
    "        # Create a single-row figure with one column per target reg\n",
    "        fig_row, axs_row = plt.subplots(nrows=1, ncols=len(TARGET_REGS), figsize=(24, 5), sharey=False)\n",
    "        # normalize axs_row to list for consistent indexing\n",
    "        if len(TARGET_REGS) == 1:\n",
    "            axs_row = [axs_row]\n",
    "        color_row = colors[r]\n",
    "        for c, term in enumerate(TARGET_REGS):\n",
    "            axr = axs_row[c]\n",
    "            vals = np.array(resh_dict[spec][term], dtype=float)\n",
    "            vals_clean = vals[~np.isnan(vals)]\n",
    "\n",
    "            if len(vals_clean):\n",
    "                axr.hist(vals_clean, bins=bins, color=color_row, alpha=0.7, edgecolor='k')\n",
    "                lo, hi = np.percentile(vals_clean, [2.5, 97.5])\n",
    "                axr.axvline(lo, color=color_row, linestyle=':', alpha=0.8)\n",
    "                axr.axvline(hi, color=color_row, linestyle=':', alpha=0.8)\n",
    "                axr.axvline(np.mean(vals_clean), color=color_row, linestyle='-', alpha=0.9)\n",
    "            else:\n",
    "                axr.text(0.5, 0.5, 'no estimates', ha='center', va='center')\n",
    "\n",
    "            obs_val = obs_dict.get(spec, {}).get(term, np.nan)\n",
    "            if not np.isnan(obs_val):\n",
    "                axr.axvline(obs_val, color='red', linestyle='--', linewidth=3, label='observed')\n",
    "\n",
    "            axr.axvline(0.5, color='black', linestyle='--', linewidth=2, alpha=0.7, label='0.5 baseline')\n",
    "            axr.set_title(term, fontsize=12)\n",
    "            if c == 0:\n",
    "                axr.set_ylabel(spec, fontsize=12)\n",
    "            axr.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "            ax.set_xlim(0.42, 0.78)\n",
    "\n",
    "        fig_row.suptitle(f'{spec} â€” Reshuffled vs. Observed Propensity (P[{plot_title_variable} | Neighbor AI])\\n\\n{title}', fontsize=14)\n",
    "        fig_row.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        # Save\n",
    "        out_dir = f'{output_plot_path}/{dependent_var}'\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path_row = f'{out_dir}/{base_name}_{spec}.png'\n",
    "        fig_row.savefig(out_path_row, dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig_row)\n",
    "        print('Saved row plot to', out_path_row)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "# --- Run plots ---\n",
    "Path(output_plot_path).mkdir(parents=True, exist_ok=True)\n",
    "plot_comparison_hist(resh_full, obs_dict_full, f'FULL Dataset (n={n_shuffles})', f'propensity_full_{dependent_var}.png', plot_title_variable)\n",
    "plot_comparison_hist(resh_filt, obs_dict_filt, f'FILTERED Dataset (n={n_shuffles})', f'propensity_filtered_{dependent_var}.png', plot_title_variable)\n",
    "\n",
    "print('All done: comparative propensity histogram figures created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
