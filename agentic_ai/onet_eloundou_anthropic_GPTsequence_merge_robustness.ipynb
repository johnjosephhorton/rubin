{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Jan 30, 2026\n",
    "#### Last Edit: Jan 30, 2026\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6359",
   "metadata": {},
   "source": [
    "# 1) Read O*NET Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf72cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing duplicates: 22,310\n",
      "Number of rows after removing duplicates: 17,953\n",
      "Duplicates removed: 4357\n",
      "Number of rows in ONET dataset: 17,953\n"
     ]
    }
   ],
   "source": [
    "# Read O*NET data\n",
    "ONET = pd.read_csv(f'{input_data_path}/computed_objects/ONET_cleaned_tasks.csv')\n",
    "\n",
    "# Drop  columns to avoid double counting\n",
    "# Note: In ~4k instances, the same task is mapped to multiple DWAs\n",
    "ONET = ONET.drop(columns=['DWA ID', 'DWA Title'])\n",
    "\n",
    "# Remove duplicate rows\n",
    "rows_before = len(ONET)\n",
    "print(f\"Number of rows before removing duplicates: {rows_before:,}\")\n",
    "ONET = ONET.drop_duplicates().reset_index(drop=True)\n",
    "rows_after = len(ONET)\n",
    "print(f\"Number of rows after removing duplicates: {rows_after:,}\")\n",
    "print(f\"Duplicates removed: {rows_before - rows_after}\")\n",
    "\n",
    "# Print length of dataset\n",
    "print(f\"Number of rows in ONET dataset: {len(ONET):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50850b83",
   "metadata": {},
   "source": [
    "# 2) Read and Merge with Eloundou et al.'s AI Exposure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f3fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in GPTs full labels dataset: 19,265\n"
     ]
    }
   ],
   "source": [
    "# Load GPTs are GPTs full label dataset\n",
    "gpts_full_labels = pd.read_csv(f'{input_data_path}/GPTs-are-GPTs-main/data/full_labelset.tsv', sep=\"\\t\")\n",
    "\n",
    "# Keep relevant columns only\n",
    "gpts_full_labels = gpts_full_labels[['O*NET-SOC Code', 'Task ID', 'Task', 'Task Type', 'Title', 'gpt4_exposure', 'human_labels']]\n",
    "\n",
    "# Convert Task ID to integer\n",
    "gpts_full_labels['Task ID'] = gpts_full_labels['Task ID'].astype(int)\n",
    "\n",
    "# Remove apostrophes for consistency\n",
    "gpts_full_labels = gpts_full_labels.applymap(lambda x: x.replace(\"'\", \"\") if isinstance(x, str) else x)\n",
    "\n",
    "# Rename columns\n",
    "gpts_full_labels = gpts_full_labels.rename(columns={\n",
    "    'Task': 'Task Title',\n",
    "    'Title': 'Occupation Title'\n",
    "})\n",
    "\n",
    "# Print length of dataset\n",
    "print(f\"Number of rows in GPTs full labels dataset: {len(gpts_full_labels):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fceda80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched tasks: 791\n"
     ]
    }
   ],
   "source": [
    "# Merge with ONET dataset to get hierarchical codes and titles\n",
    "ONET = ONET.merge(gpts_full_labels, on=['O*NET-SOC Code', 'Occupation Title', 'Task ID', 'Task Title', 'Task Type'], how='left')\n",
    "\n",
    "# Check how many tasks were not matched\n",
    "unmatched_tasks = ONET[ONET['gpt4_exposure'].isna()]\n",
    "print(f\"Number of unmatched tasks: {len(unmatched_tasks):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a891bdd",
   "metadata": {},
   "source": [
    "# 3) Read and Merge with Anthropic's AI Use Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c12ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before filtering: 3,364\n",
      "Number of rows after filtering: 2,298\n"
     ]
    }
   ],
   "source": [
    "# Merge with Anthropic exposure data\n",
    "anthropic_exposure = pd.read_csv(f'{input_data_path}/Anthropic_EconomicIndex/automation_vs_augmentation_by_task.csv')\n",
    "\n",
    "# Remove if all entries are filtered:\n",
    "print(f\"Number of rows before filtering: {len(anthropic_exposure):,}\")\n",
    "anthropic_exposure = anthropic_exposure[anthropic_exposure['filtered'] != 1].reset_index(drop=True)\n",
    "print(f\"Number of rows after filtering: {len(anthropic_exposure):,}\")\n",
    "\n",
    "# Create new columns:\n",
    "# Sum feedback loop and directive into Automation\n",
    "# Sum validation, iteration, and learning into Augmentation\n",
    "anthropic_exposure['automation'] = anthropic_exposure.apply(lambda row: row['feedback_loop'] + row['directive'], axis=1)\n",
    "anthropic_exposure['augmentation'] = anthropic_exposure.apply(lambda row: row['validation'] + row['task_iteration'] + row['learning'], axis=1)\n",
    "\n",
    "# Assign labels: take the max of automation, augmentation, manual and assign the corresponding label\n",
    "def assign_label(row):\n",
    "    max_value = max(row['automation'], row['augmentation'])\n",
    "    if max_value == row['automation']:\n",
    "        return 'Automation'\n",
    "    elif max_value == row['augmentation']:\n",
    "        return 'Augmentation'\n",
    "\n",
    "anthropic_exposure['label'] = anthropic_exposure.apply(assign_label, axis=1)\n",
    "\n",
    "# Filter to only keep the relevant columns\n",
    "anthropic_exposure = anthropic_exposure[['task_name', 'automation', 'augmentation', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51666860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tasks in ONET dataset: 16,913\n",
      "Number of unique tasks in Anthropic exposure dataset: 2,298\n",
      "\n",
      "Distribution of labels after filling NaN values with 'Manual':\n",
      "label\n",
      "Manual          15605\n",
      "Augmentation     1626\n",
      "Automation        722\n",
      "Name: count, dtype: int64\n",
      "Total tasks: 17,953\n"
     ]
    }
   ],
   "source": [
    "# Print number of unique tasks in ONET dataset\n",
    "print(f\"Number of unique tasks in ONET dataset: {ONET['Task Title'].nunique():,}\")\n",
    "\n",
    "# Print number of unique tasks in Anthropic exposure dataset\n",
    "print(f\"Number of unique tasks in Anthropic exposure dataset: {anthropic_exposure['task_name'].nunique():,}\")\n",
    "\n",
    "\n",
    "# Add normalized task title to ONET for merging\n",
    "ONET[\"task_normalized\"] = ONET[\"Task Title\"].str.lower().str.strip()\n",
    "\n",
    "\n",
    "# Merge ONET and Anthropic exposure data on the normalized task title\n",
    "merged_data = pd.merge(ONET, anthropic_exposure[['task_name', 'automation', 'augmentation', 'label']], left_on=\"task_normalized\", right_on=\"task_name\", how=\"left\")\n",
    "\n",
    "# Fill the NaN values of the label column in merged dataset as Manual\n",
    "merged_data['label'] = merged_data['label'].fillna('Manual')\n",
    "\n",
    "# Print distribution after filling NaN values\n",
    "print(f\"\\nDistribution of labels after filling NaN values with 'Manual':\")\n",
    "print(merged_data['label'].value_counts())\n",
    "print(f\"Total tasks: {len(merged_data):,}\")\n",
    "\n",
    "# Drop temporary columns used for merging\n",
    "merged_data = merged_data.drop(columns=['task_normalized', 'task_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959307e",
   "metadata": {},
   "source": [
    "# 4) Read and Merge with GPT 5-o Mini's Task Sequence Data *from robustness prompts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee41ced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prompt_0] Task sequence data shape: (17868, 5)\n",
      "[prompt_0] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_0.csv\n",
      "\n",
      "[prompt_1] Task sequence data shape: (17828, 5)\n",
      "[prompt_1] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_1.csv\n",
      "\n",
      "[prompt_2] Task sequence data shape: (17866, 5)\n",
      "[prompt_2] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_2.csv\n",
      "\n",
      "[prompt_3] Task sequence data shape: (17826, 5)\n",
      "[prompt_3] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_3.csv\n",
      "\n",
      "[prompt_4] Task sequence data shape: (17857, 5)\n",
      "[prompt_4] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_4.csv\n",
      "\n",
      "[prompt_5] Task sequence data shape: (17847, 5)\n",
      "[prompt_5] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_5.csv\n",
      "\n",
      "[prompt_6] Task sequence data shape: (17810, 5)\n",
      "[prompt_6] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_6.csv\n",
      "\n",
      "[prompt_7] Task sequence data shape: (17857, 5)\n",
      "[prompt_7] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_7.csv\n",
      "\n",
      "[prompt_8] Task sequence data shape: (17830, 5)\n",
      "[prompt_8] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_8.csv\n",
      "\n",
      "[prompt_9] Task sequence data shape: (17851, 5)\n",
      "[prompt_9] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_9.csv\n",
      "\n",
      "[prompt_10] Task sequence data shape: (16088, 5)\n",
      "[prompt_10] Saved: ../data/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_10.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(11):\n",
    "    prompt_dir = f\"{input_data_path}/computed_objects/tasks_sequences_robustness_restructured/prompt_{x}\"\n",
    "    task_sequence_files = [f for f in os.listdir(prompt_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "    if not task_sequence_files:\n",
    "        print(f\"[prompt_{x}] No CSV files found in {prompt_dir}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    task_sequence = pd.concat(\n",
    "        [pd.read_csv(os.path.join(prompt_dir, file)) for file in task_sequence_files],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(f\"[prompt_{x}] Task sequence data shape: {task_sequence.shape}\")\n",
    "    # print(f\"[prompt_{x}] Columns in task sequence: {list(task_sequence.columns)}\")\n",
    "\n",
    "    # Work on a fresh copy each iteration (avoid accumulating merges across prompts)\n",
    "    md = merged_data.copy()\n",
    "\n",
    "    # Merge task sequence data with merged_data\n",
    "    md = pd.merge(\n",
    "        md,\n",
    "        task_sequence[[\"O*NET-SOC Code\", \"Task ID\", \"Task Position\"]],\n",
    "        on=[\"O*NET-SOC Code\", \"Task ID\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    md = md[md[\"Task Position\"].notna()].reset_index(drop=True)\n",
    "    md[\"Task Position\"] = pd.to_numeric(md[\"Task Position\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Bring the \"Task Position\" column next to \"Task Title\" (as in your code)\n",
    "    task_position_col = md.pop(\"Task Position\")\n",
    "    md.insert(md.columns.get_loc(\"Task Title\") + 1, \"Task Position\", task_position_col)\n",
    "\n",
    "    # Sort values before saving\n",
    "    md = md.sort_values(by=[\"O*NET-SOC Code\", \"Task Position\"])\n",
    "\n",
    "    # Save merged dataset\n",
    "    out_file = os.path.join(output_data_path, f\"ONET_Eloundou_Anthropic_GPT_{x}.csv\")\n",
    "    md.to_csv(out_file, index=False)\n",
    "    print(f\"[prompt_{x}] Saved: {out_file}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
