{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Nov 7, 2025\n",
    "#### Last Edit: Nov 14, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e03cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "my_onet_level = 'detailed'\n",
    "onet_occupation_code_var = 'Detailed_Occupation_Code'\n",
    "onet_occupation_title_var = 'Detailed_Occupation_Title'\n",
    "\n",
    "ai_exposure_var = 'human_E1_fraction'\n",
    "\n",
    "FREQUENT_TASKS = False  # Whether to use only frequent tasks or all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/fragmentationIndex_frequent' if FREQUENT_TASKS else f'{input_data_path}/computed_objects/fragmentationIndex'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2038cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged data\n",
    "if not FREQUENT_TASKS:\n",
    "    input_file_path = f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\" \n",
    "    merged_data = pd.read_csv(input_file_path)\n",
    "\n",
    "        # Remove occupations with three or less frequent tasks\n",
    "    frequent_tasks_per_occupation_threshold = 3\n",
    "    occupation_task_counts = merged_data.groupby('O*NET-SOC Code')['Task ID'].nunique()\n",
    "    valid_occupations = occupation_task_counts[occupation_task_counts >= frequent_tasks_per_occupation_threshold].index\n",
    "    merged_data = merged_data[merged_data['O*NET-SOC Code'].isin(valid_occupations)].reset_index(drop=True)\n",
    "else:\n",
    "    input_file_path = f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT_frequent/ONET_Eloundou_Anthropic_GPT.csv\"\n",
    "    merged_data = pd.read_csv(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032c8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SOC mappings to merge later\n",
    "# Read original occupation analysis with SOC mappings\n",
    "ONET = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_cleaned_tasks.csv\")\n",
    "\n",
    "# Keep only the relevant \n",
    "SOC_mappings = ONET[['O*NET-SOC Code', 'Occupation Title',\n",
    "                    'Major_Group_Code', 'Major_Group_Title',\n",
    "                    'Minor_Group_Code', 'Minor_Group_Title',\n",
    "                    'Broad_Occupation_Code', 'Broad_Occupation_Title',\n",
    "                    'Detailed_Occupation_Code', 'Detailed_Occupation_Title']].copy()\n",
    "SOC_mappings = SOC_mappings.drop_duplicates(subset=['O*NET-SOC Code', onet_occupation_code_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a3aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_analysis(df, onet_occupation_code_var, onet_occupation_title_var):\n",
    "    # Create occupation-level analysis for scatter plots\n",
    "    # Group by occupation and calculate label fractions and task counts\n",
    "    occupation_stats = []\n",
    "\n",
    "    for (soc_code, occ_title), group in df.groupby([onet_occupation_code_var, onet_occupation_title_var]):\n",
    "        num_tasks = group['Task ID'].nunique()\n",
    "        # num_occupations = group[onet_occupation_code_var].nunique()\n",
    "        total_tasks = len(group)\n",
    "        \n",
    "        manual_fraction = (group['label'] == 'Manual').sum() / total_tasks\n",
    "        augmentation_fraction = (group['label'] == 'Augmentation').sum() / total_tasks  \n",
    "        automation_fraction = (group['label'] == 'Automation').sum() / total_tasks\n",
    "        ai_fraction = augmentation_fraction + automation_fraction\n",
    "        gpt4_E0_fraction = (group['gpt4_exposure'] == 'E0').sum() / total_tasks\n",
    "        gpt4_E1_fraction = (group['gpt4_exposure'] == 'E1').sum() / total_tasks\n",
    "        gpt4_E2_fraction = (group['gpt4_exposure'] == 'E2').sum() / total_tasks\n",
    "        gpt4_aiExposure_fraction = gpt4_E1_fraction + gpt4_E2_fraction\n",
    "        human_E0_fraction = (group['human_labels'] == 'E0').sum() / total_tasks\n",
    "        human_E1_fraction = (group['human_labels'] == 'E1').sum() / total_tasks\n",
    "        human_E2_fraction = (group['human_labels'] == 'E2').sum() / total_tasks\n",
    "        human_aiExposure_fraction = human_E1_fraction + human_E2_fraction\n",
    "\n",
    "        \n",
    "        occupation_stats.append({\n",
    "            f'{onet_occupation_code_var}': soc_code,\n",
    "            f'{onet_occupation_title_var}': occ_title,\n",
    "            'num_tasks': num_tasks,\n",
    "            # 'num_occupations': num_occupations,\n",
    "            'manual_fraction': manual_fraction,\n",
    "            'ai_fraction': ai_fraction,\n",
    "            'augmentation_fraction': augmentation_fraction,\n",
    "            'automation_fraction': automation_fraction,\n",
    "            'gpt4_E0_fraction': gpt4_E0_fraction,\n",
    "            'gpt4_E1_fraction': gpt4_E1_fraction,\n",
    "            'gpt4_E2_fraction': gpt4_E2_fraction,\n",
    "            'gpt4_aiExposure_fraction': gpt4_aiExposure_fraction,\n",
    "            'human_E0_fraction': human_E0_fraction,\n",
    "            'human_E1_fraction': human_E1_fraction,\n",
    "            'human_E2_fraction': human_E2_fraction,\n",
    "            'human_aiExposure_fraction': human_aiExposure_fraction\n",
    "        })\n",
    "\n",
    "    occupation_analysis = pd.DataFrame(occupation_stats)\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "# Create fragmentation index dataframe for different definitions\n",
    "def construct_fragmentation_index(df, desired_definition=1, save_filename=None):\n",
    "    # Definition 1: Separate Augmentation and Automation; AI Chain starts with Automation or Augmentation task and terminates at the first Augmentation task; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 2: Treat all AI tasks similarly; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 3: Same as Definition 1, but use exposure based label (E1) for forming the \"AI-Chain\"s\n",
    "    # Definition 4: Same as Definition 1, but use exposure based label (E1 or E2) for forming the \"AI-Chain\"s\n",
    "    fi_df = df.copy()\n",
    "    \n",
    "    # Definitions 2, 3, and 4, are basically similar in construction, only differing in the labeling of tasks\n",
    "    if desired_definition != 1:\n",
    "        if desired_definition == 2:\n",
    "            # Use AI execution labels\n",
    "            fi_df['is_ai'] = fi_df['label'].isin(['Augmentation', 'Automation']).astype(int)\n",
    "        elif desired_definition == 3:\n",
    "            # Use exposure based label (E1)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1']).astype(int)\n",
    "        elif desired_definition == 4:\n",
    "            # Use exposure based label (E1 or E2)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1', 'E2']).astype(int)\n",
    "        \n",
    "        # Create next_is_ai column within occupation groups\n",
    "        fi_df['next_is_ai'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_ai'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        fi_df.loc[(fi_df['is_ai'] == 1) & (fi_df['next_is_ai'] == 1), 'num_switches'] = 0\n",
    "\n",
    "    else: # Definition 1\n",
    "        fi_df['is_automated'] = fi_df['label'].isin(['Automation']).astype(int)\n",
    "        fi_df['is_augmented'] = fi_df['label'].isin(['Augmentation']).astype(int)\n",
    "\n",
    "        # Create next_is_automated column within occupation groups\n",
    "        fi_df['next_is_automated'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_automated'].shift(-1).fillna(0).astype(int)\n",
    "        fi_df['next_is_augmented'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_augmented'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        ai_chain_indicator = (fi_df['is_automated'] == 1) & ((fi_df['next_is_automated'] == 1) | (fi_df['next_is_augmented'] == 1))\n",
    "        fi_df.loc[ai_chain_indicator, 'num_switches'] = 0\n",
    "\n",
    "    # Now with a counter for number of switches, calculate fragmentation index per occupation as mean of num_switches per occupation\n",
    "    fi_df = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['num_switches'].mean()\n",
    "    fi_df = fi_df.reset_index().rename(columns={'num_switches': 'fragmentation_index'})\n",
    "\n",
    "    # Save fragmentation index data\n",
    "    if save_filename:\n",
    "        fi_df.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    return fi_df\n",
    "\n",
    "\n",
    "\n",
    "# Merge fragmentation data with occupation analysis\n",
    "def merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var, save_filename=None):\n",
    "    # Merge fragmentation index with occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(fi_df, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    # Save occupation analysis with fragmentation index\n",
    "    if save_filename:\n",
    "        occupation_analysis.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    # Merge SOC levels with the occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(SOC_mappings, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate occupation analysis at the level of onet_occupation_code_var\n",
    "def aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var):\n",
    "    occupation_analysis_aggregated = occupation_analysis.groupby(\n",
    "        [onet_occupation_code_var, onet_occupation_title_var]\n",
    "    ).agg({\n",
    "        'fragmentation_index': 'mean',\n",
    "        ai_exposure_var: 'mean',\n",
    "        'ai_fraction': 'mean',\n",
    "        'num_tasks': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge SOC levels for FE\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.merge(\n",
    "        SOC_mappings, on=onet_occupation_code_var, how='left'\n",
    "    )\n",
    "\n",
    "    return occupation_analysis_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617af65e",
   "metadata": {},
   "source": [
    "## Calculate Fragmentation Index for 4 Definitions and Run the Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6363b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = []\n",
    "\n",
    "# Create dataset and run regressions for all 4 definitions\n",
    "for definition in [1, 2, 3, 4]:\n",
    "    # Get occupation data\n",
    "    occupation_analysis = create_occupation_analysis(merged_data, 'O*NET-SOC Code', 'Occupation Title')\n",
    "\n",
    "    # Get fragmentation index\n",
    "    fi_df = construct_fragmentation_index(merged_data, desired_definition=definition, save_filename=f'fragmentationIndex_def{definition}.csv')\n",
    "\n",
    "    # Merge fragmentation data with occupation analysis\n",
    "    occupation_analysis = merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var, save_filename=f'occupation_analysis_with_fragmentationIndex_def{definition}.csv')\n",
    "\n",
    "    # Aggregate data at the onet_occupation_code_var level and add back SOC code information\n",
    "    occupation_analysis_aggregated = aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var)\n",
    "\n",
    "    # Rename regression columns for clarity\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.rename(columns={\n",
    "        ai_exposure_var: 'ai_exposure'\n",
    "    })\n",
    "\n",
    "    # Run regressions, clustering standard errors at the onet_occupation_code_var level\n",
    "    # --- Model A: no FE ---\n",
    "    mod_noFE = smf.ols(\n",
    "        formula=f'ai_fraction ~ fragmentation_index + ai_exposure',\n",
    "        data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\",\n",
    "            cov_kwds={\"groups\": occupation_analysis_aggregated[onet_occupation_code_var],\n",
    "                        \"use_correction\": True,\n",
    "                        \"df_correction\": True}\n",
    "                        )\n",
    "    regression_results.append(mod_noFE)\n",
    "\n",
    "    # --- Model B (Major group FE) ---\n",
    "    mod_majorFE = smf.ols(\n",
    "        formula=f'ai_fraction ~ fragmentation_index + ai_exposure + C(Major_Group_Code)',\n",
    "        data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\",\n",
    "            cov_kwds={\"groups\": occupation_analysis_aggregated[onet_occupation_code_var],\n",
    "                        \"use_correction\": True,\n",
    "                        \"df_correction\": True}\n",
    "                        )\n",
    "    regression_results.append(mod_majorFE)\n",
    "\n",
    "    # --- Model C (Minor group FE) ---\n",
    "    mod_minorFE = smf.ols(\n",
    "        formula=f'ai_fraction ~ fragmentation_index + ai_exposure + C(Minor_Group_Code)',\n",
    "        data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\",\n",
    "            cov_kwds={\"groups\": occupation_analysis_aggregated[onet_occupation_code_var],\n",
    "                        \"use_correction\": True,\n",
    "                        \"df_correction\": True}\n",
    "                        )\n",
    "    regression_results.append(mod_minorFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee0f9cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{}\n",
      "\\label{}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lllllll}\n",
      "\\hline\n",
      " & \\\\multicolumn{3}{c}{FI v1} & \\\\multicolumn{3}{c}{FI v2} \\\\\n",
      "                        & (1)      & (2)      & (3)      & (4)      & (5)      & (6)       \\\\\n",
      "\\hline\n",
      "ai\\_exposure            & 0.43***  & 0.20***  & 0.15***  & 0.26***  & 0.13***  & 0.11***   \\\\\n",
      "                        & (0.03)   & (0.04)   & (0.05)   & (0.02)   & (0.03)   & (0.03)    \\\\\n",
      "fragmentation\\_index    & -2.58*** & -1.97*** & -1.68*** & -1.48*** & -1.29*** & -1.24***  \\\\\n",
      "                        & (0.12)   & (0.15)   & (0.17)   & (0.05)   & (0.05)   & (0.06)    \\\\\n",
      "\\hline\n",
      "R-squared               & 0.65     & 0.74     & 0.80     & 0.84     & 0.87     & 0.90      \\\\\n",
      "R-squared Adj.          & 0.65     & 0.74     & 0.77     & 0.84     & 0.87     & 0.88      \\\\\n",
      "N                       & 872      & 872      & 872      & 872      & 872      & 872       \\\\\n",
      "SOC Group Fixed Effects &          & Major    & Minor    &          & Major    & Minor     \\\\\n",
      "\\hline\\\\[-1.25em]\n",
      "\\multicolumn{4}{l}{Standard errors in parentheses.} \\\\\n",
      "\\multicolumn{4}{l}{$^{*}:p<0.1$, $^{**}:p<0.05$, $^{***}:p<0.01$} \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table}\n",
      "\\bigskip\n",
      "Standard errors in parentheses. \\newline \n",
      "* p<.1, ** p<.05, ***p<.01\n",
      "\\begin{table}\n",
      "\\caption{}\n",
      "\\label{}\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lllllll}\n",
      "\\hline\n",
      " & \\\\multicolumn{3}{c}{FI v3} & \\\\multicolumn{3}{c}{FI v4} \\\\\n",
      "                        & (7)     & (8)    & (9)    & (10)     & (11)     & (12)      \\\\\n",
      "\\hline\n",
      "ai\\_exposure            & 0.66*** & 0.20** & 0.18** & 0.33***  & 0.14**   & 0.12**    \\\\\n",
      "                        & (0.08)  & (0.08) & (0.08) & (0.06)   & (0.06)   & (0.06)    \\\\\n",
      "fragmentation\\_index    & 0.04    & -0.15  & -0.01  & -0.23*** & -0.21*** & -0.14***  \\\\\n",
      "                        & (0.20)  & (0.16) & (0.16) & (0.03)   & (0.04)   & (0.04)    \\\\\n",
      "\\hline\n",
      "R-squared               & 0.31    & 0.63   & 0.73   & 0.37     & 0.66     & 0.74      \\\\\n",
      "R-squared Adj.          & 0.31    & 0.62   & 0.70   & 0.37     & 0.65     & 0.71      \\\\\n",
      "N                       & 872     & 872    & 872    & 872      & 872      & 872       \\\\\n",
      "SOC Group Fixed Effects &         & Major  & Minor  &          & Major    & Minor     \\\\\n",
      "\\hline\\\\[-1.25em]\n",
      "\\multicolumn{4}{l}{Standard errors in parentheses.} \\\\\n",
      "\\multicolumn{4}{l}{$^{*}:p<0.1$, $^{**}:p<0.05$, $^{***}:p<0.01$} \\\\\n",
      "\\end{tabular}\n",
      "\\end{center}\n",
      "\\end{table}\n",
      "\\bigskip\n",
      "Standard errors in parentheses. \\newline \n",
      "* p<.1, ** p<.05, ***p<.01\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "def latex_table_cleanup(results_table):\n",
    "    latex_table = results_table.as_latex()\n",
    "    \n",
    "    # --- Ensure a single \\hline before R-squared ---\n",
    "    latex_table = latex_table.replace('\\nR-squared', '\\n\\\\hline\\nR-squared')\n",
    "    latex_table = latex_table.replace('\\n\\\\hline\\nR-squared Adj.', '\\nR-squared Adj.')\n",
    "\n",
    "    # --- Add centered note below final \\hline (inside table) ---\n",
    "    note_text = (\n",
    "        '\\\\\\\\[-1.25em]\\n'\n",
    "        '\\\\multicolumn{4}{l}{Standard errors in parentheses.} \\\\\\\\\\n'\n",
    "        '\\\\multicolumn{4}{l}{$^{*}:p<0.1$, $^{**}:p<0.05$, $^{***}:p<0.01$} \\\\\\\\\\n'\n",
    "    )\n",
    "\n",
    "    latex_table = latex_table.replace(\n",
    "        r'\\hline' + '\\n\\\\end{tabular}',\n",
    "        r'\\hline' + note_text + r'\\end{tabular}'\n",
    "    )\n",
    "\n",
    "    return latex_table\n",
    "\n",
    "\n",
    "def insert_group_header(latex_table, group_labels, group_sizes):\n",
    "    \"\"\"Insert a grouped header row into a LaTeX table produced by summary_col.\n",
    "\n",
    "    group_labels: list of strings for each group (e.g., ['FI v1','FI v2'])\n",
    "    group_sizes: list of ints with how many model columns each group spans (e.g., [3,3])\n",
    "    \"\"\"\n",
    "    marker = '\\n\\\\hline\\n'\n",
    "    idx = latex_table.find(marker)\n",
    "    if idx == -1:\n",
    "        return latex_table\n",
    "    insert_pos = idx + len(marker)\n",
    "\n",
    "    # Build the grouped row. Leading empty cell for the variable name column.\n",
    "    parts = []\n",
    "    for label, size in zip(group_labels, group_sizes):\n",
    "        parts.append(r\"\\\\multicolumn{\" + str(size) + r\"}{c}{\" + label + r\"}\")\n",
    "\n",
    "    group_row = ' & ' + ' & '.join(parts) + r' \\\\' + '\\n'\n",
    "\n",
    "    return latex_table[:insert_pos] + group_row + latex_table[insert_pos:]\n",
    "\n",
    "\n",
    "# ---- Combine models into a table ----\n",
    "info_dict = {\n",
    "    'N': lambda x: f\"{int(x.nobs):,}\",\n",
    "    'SOC Group Fixed Effects': lambda x: (\n",
    "        '' if 'C(Major_Group_Code)' not in x.model.formula and 'C(Minor_Group_Code)' not in x.model.formula\n",
    "        else 'Major' if 'C(Major_Group_Code)' in x.model.formula\n",
    "        else 'Minor'\n",
    "    )\n",
    "}\n",
    "\n",
    "regressor_order = ['ai_exposure', 'fragmentation_index']\n",
    "\n",
    "results_table_1 = summary_col(\n",
    "    results=regression_results[:6],\n",
    "    float_format='%0.2f',\n",
    "    stars=True,\n",
    "    model_names=['(1)', '(2)', '(3)', \n",
    "                 '(4)', '(5)', '(6)'],\n",
    "    info_dict=info_dict,\n",
    "    regressor_order=regressor_order,\n",
    "    drop_omitted=True\n",
    ")\n",
    "latex_table_1 = latex_table_cleanup(results_table_1)\n",
    "# Insert grouped header for definitions 1 & 2 (three models each)\n",
    "latex_table_1 = insert_group_header(latex_table_1, ['FI v1', 'FI v2'], [3, 3])\n",
    "\n",
    "# Save LaTeX table to file\n",
    "output_path = f\"{output_data_path}/fragmentation_index_regression_1.tex\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_table_1)\n",
    "print(latex_table_1)\n",
    "\n",
    "\n",
    "\n",
    "results_table_2 = summary_col(\n",
    "    results=regression_results[6:],\n",
    "    float_format='%0.2f',\n",
    "    stars=True,\n",
    "    model_names=['(7)', '(8)', '(9)',\n",
    "                 '(10)', '(11)', '(12)'],\n",
    "    info_dict=info_dict,\n",
    "    regressor_order=regressor_order,\n",
    "    drop_omitted=True\n",
    ")\n",
    "latex_table_2 = latex_table_cleanup(results_table_2)\n",
    "# Insert grouped header for definitions 3 & 4 (three models each)\n",
    "latex_table_2 = insert_group_header(latex_table_2, ['FI v3', 'FI v4'], [3, 3])\n",
    "\n",
    "# Save LaTeX table to file\n",
    "output_path = f\"{output_data_path}/fragmentation_index_regression_2.tex\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_table_2)\n",
    "print(latex_table_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3ebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote combined tabular to: ../data/computed_objects/fragmentationIndex/fragmentation_index_regression.tex\n"
     ]
    }
   ],
   "source": [
    "# Combine the two LaTeX tables into a single tabular with two panels\n",
    "import os\n",
    "import re\n",
    "\n",
    "tex1_path = os.path.join(output_data_path, 'fragmentation_index_regression_1.tex')\n",
    "tex2_path = os.path.join(output_data_path, 'fragmentation_index_regression_2.tex')\n",
    "output_combined_path = os.path.join(output_data_path, 'fragmentation_index_regression.tex')\n",
    "\n",
    "\n",
    "def extract_tabular_block(tex):\n",
    "    m = re.search(r\"\\\\begin\\{tabular\\}\\{.*?\\}(.*?)\\\\end\\{tabular\\}\", tex, flags=re.S)\n",
    "    return m.group(1) if m else ''\n",
    "\n",
    "\n",
    "def extract_rows(block):\n",
    "    # Normalize and split into non-empty lines\n",
    "    lines = [l.strip() for l in block.splitlines() if l.strip() != '']\n",
    "\n",
    "    def find_index_containing(substr):\n",
    "        for i, l in enumerate(lines):\n",
    "            if substr in l:\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    ai_idx = find_index_containing('ai\\\\_exposure')\n",
    "    frag_idx = find_index_containing('fragmentation\\\\_index')\n",
    "\n",
    "    ai_line = lines[ai_idx] if ai_idx is not None else ''\n",
    "    ai_se = lines[ai_idx + 1] if ai_idx is not None and ai_idx + 1 < len(lines) else ''\n",
    "    frag_line = lines[frag_idx] if frag_idx is not None else ''\n",
    "    frag_se = lines[frag_idx + 1] if frag_idx is not None and frag_idx + 1 < len(lines) else ''\n",
    "\n",
    "    def find_starting(prefix):\n",
    "        for l in lines:\n",
    "            if l.startswith(prefix):\n",
    "                return l\n",
    "        return ''\n",
    "\n",
    "    r2 = find_starting('R-squared')\n",
    "    r2adj = find_starting('R-squared Adj.')\n",
    "    N = find_starting('N')\n",
    "    soc = find_starting('SOC Group Fixed Effects')\n",
    "\n",
    "    return {\n",
    "        'ai_line': ai_line,\n",
    "        'ai_se': ai_se,\n",
    "        'frag_line': frag_line,\n",
    "        'frag_se': frag_se,\n",
    "        'R2': r2,\n",
    "        'R2adj': r2adj,\n",
    "        'N': N,\n",
    "        'SOC': soc,\n",
    "    }\n",
    "\n",
    "\n",
    "# Read both files\n",
    "with open(tex1_path, 'r', encoding='utf-8') as f:\n",
    "    tex1 = f.read()\n",
    "with open(tex2_path, 'r', encoding='utf-8') as f:\n",
    "    tex2 = f.read()\n",
    "\n",
    "block1 = extract_tabular_block(tex1)\n",
    "block2 = extract_tabular_block(tex2)\n",
    "rows1 = extract_rows(block1)\n",
    "rows2 = extract_rows(block2)\n",
    "\n",
    "# Build combined tabular block (only the tabular, ready to paste into a .tex file)\n",
    "lines = []\n",
    "lines.append('\\\\begin{tabular}{lcccccc}')\n",
    "lines.append('\\\\multicolumn{7}{l}{Panel (A)} \\\\\\\\')\n",
    "lines.append('\\\\hline')\n",
    "lines.append('                        & \\\\multicolumn{3}{c|}{FI v1} & \\\\multicolumn{3}{c}{FI v2} \\\\\\\\')\n",
    "lines.append(' \\\\cline{2-7}')\n",
    "lines.append('                        & (1)      & (2)      & (3)      & (4)      & (5)      & (6)       \\\\\\\\')\n",
    "lines.append('\\\\hline')\n",
    "# append coefficient lines (assume they already have correct number of columns)\n",
    "lines.append(rows1['ai_line'])\n",
    "lines.append(rows1['ai_se'])\n",
    "lines.append(rows1['frag_line'])\n",
    "lines.append(rows1['frag_se'])\n",
    "lines.append('\\\\hline')\n",
    "lines.append(rows1['R2'])\n",
    "lines.append(rows1['R2adj'])\n",
    "lines.append(rows1['N'])\n",
    "lines.append(rows1['SOC'])\n",
    "lines.append('\\\\\\\\[-0.5em]')\n",
    "lines.append('\\\\multicolumn{7}{l}{Panel (B)} \\\\\\\\')\n",
    "lines.append('\\\\hline')\n",
    "lines.append('                        & \\\\multicolumn{3}{c|}{FI v3} & \\\\multicolumn{3}{c}{FI v4} \\\\\\\\')\n",
    "lines.append(' \\\\cline{2-7}')\n",
    "lines.append('                        & (7)     & (8)    & (9)    & (10)     & (11)     & (12)       \\\\\\\\')\n",
    "lines.append('\\\\hline')\n",
    "lines.append(rows2['ai_line'])\n",
    "lines.append(rows2['ai_se'])\n",
    "lines.append(rows2['frag_line'])\n",
    "lines.append(rows2['frag_se'])\n",
    "lines.append('\\\\hline')\n",
    "lines.append(rows2['R2'])\n",
    "lines.append(rows2['R2adj'])\n",
    "lines.append(rows2['N'])\n",
    "lines.append(rows2['SOC'])\n",
    "lines.append('\\\\hline')\n",
    "lines.append('\\\\\\\\[-1em]')\n",
    "lines.append('\\\\multicolumn{7}{l}{\\\\footnotesize Standard errors in parentheses.} \\\\\\\\')\n",
    "lines.append('\\\\multicolumn{7}{l}{\\\\footnotesize $^{*}:p<0.1$, $^{**}:p<0.05$, $^{***}:p<0.01$} \\\\\\\\')\n",
    "lines.append('\\\\end{tabular}')\n",
    "\n",
    "combined_tabular = '\\n'.join(lines)\n",
    "\n",
    "# Write only the tabular piece to the file (no surrounding table/resizing environment)\n",
    "os.makedirs(os.path.dirname(output_combined_path), exist_ok=True)\n",
    "with open(output_combined_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(combined_tabular)\n",
    "\n",
    "print(f'Wrote combined tabular to: {output_combined_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
