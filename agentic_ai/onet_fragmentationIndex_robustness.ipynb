{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Jan 30, 2026\n",
    "#### Last Edit: Jan 30, 2026\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e03cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "my_onet_level = 'detailed'\n",
    "onet_occupation_code_var = 'Detailed_Occupation_Code'\n",
    "onet_occupation_title_var = 'Detailed_Occupation_Title'\n",
    "\n",
    "ai_exposure_var = 'human_E1_fraction'\n",
    "\n",
    "FREQUENT_TASKS = False  # Whether to use only frequent tasks or all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/fragmentationIndex_robustness'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots/fragmentationIndex_robustness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89302725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_analysis(df, onet_occupation_code_var, onet_occupation_title_var):\n",
    "    # Create occupation-level analysis for scatter plots\n",
    "    # Group by occupation and calculate label fractions and task counts\n",
    "    occupation_stats = []\n",
    "\n",
    "    for (soc_code, occ_title), group in df.groupby([onet_occupation_code_var, onet_occupation_title_var]):\n",
    "        num_tasks = group['Task ID'].nunique()\n",
    "        # num_occupations = group[onet_occupation_code_var].nunique()\n",
    "        total_tasks = len(group)\n",
    "        \n",
    "        manual_fraction = (group['label'] == 'Manual').sum() / total_tasks\n",
    "        augmentation_fraction = (group['label'] == 'Augmentation').sum() / total_tasks  \n",
    "        automation_fraction = (group['label'] == 'Automation').sum() / total_tasks\n",
    "        ai_fraction = augmentation_fraction + automation_fraction\n",
    "        gpt4_E0_fraction = (group['gpt4_exposure'] == 'E0').sum() / total_tasks\n",
    "        gpt4_E1_fraction = (group['gpt4_exposure'] == 'E1').sum() / total_tasks\n",
    "        gpt4_E2_fraction = (group['gpt4_exposure'] == 'E2').sum() / total_tasks\n",
    "        gpt4_aiExposure_fraction = gpt4_E1_fraction + gpt4_E2_fraction\n",
    "        human_E0_fraction = (group['human_labels'] == 'E0').sum() / total_tasks\n",
    "        human_E1_fraction = (group['human_labels'] == 'E1').sum() / total_tasks\n",
    "        human_E2_fraction = (group['human_labels'] == 'E2').sum() / total_tasks\n",
    "        human_aiExposure_fraction = human_E1_fraction + human_E2_fraction\n",
    "\n",
    "        \n",
    "        occupation_stats.append({\n",
    "            f'{onet_occupation_code_var}': soc_code,\n",
    "            f'{onet_occupation_title_var}': occ_title,\n",
    "            'num_tasks': num_tasks,\n",
    "            # 'num_occupations': num_occupations,\n",
    "            'manual_fraction': manual_fraction,\n",
    "            'ai_fraction': ai_fraction,\n",
    "            'augmentation_fraction': augmentation_fraction,\n",
    "            'automation_fraction': automation_fraction,\n",
    "            'gpt4_E0_fraction': gpt4_E0_fraction,\n",
    "            'gpt4_E1_fraction': gpt4_E1_fraction,\n",
    "            'gpt4_E2_fraction': gpt4_E2_fraction,\n",
    "            'gpt4_aiExposure_fraction': gpt4_aiExposure_fraction,\n",
    "            'human_E0_fraction': human_E0_fraction,\n",
    "            'human_E1_fraction': human_E1_fraction,\n",
    "            'human_E2_fraction': human_E2_fraction,\n",
    "            'human_aiExposure_fraction': human_aiExposure_fraction\n",
    "        })\n",
    "\n",
    "    occupation_analysis = pd.DataFrame(occupation_stats)\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "# Create fragmentation index dataframe for different definitions\n",
    "def construct_fragmentation_index(df, desired_definition=1, save_filename=None):\n",
    "    # Definition 1: Separate Augmentation and Automation; AI Chain starts with Automation or Augmentation task and terminates at the first Augmentation task; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 2: Treat all AI tasks similarly; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 3: Same as Definition 1, but use exposure based label (E1) for forming the \"AI-Chain\"s\n",
    "    # Definition 4: Same as Definition 1, but use exposure based label (E1 or E2) for forming the \"AI-Chain\"s\n",
    "    fi_df = df.copy()\n",
    "    \n",
    "    # Definitions 2, 3, and 4, are basically similar in construction, only differing in the labeling of tasks\n",
    "    if desired_definition != 1:\n",
    "        if desired_definition == 2:\n",
    "            # Use AI execution labels\n",
    "            fi_df['is_ai'] = fi_df['label'].isin(['Augmentation', 'Automation']).astype(int)\n",
    "        elif desired_definition == 3:\n",
    "            # Use exposure based label (E1)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1']).astype(int)\n",
    "        elif desired_definition == 4:\n",
    "            # Use exposure based label (E1 or E2)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1', 'E2']).astype(int)\n",
    "        \n",
    "        # Create next_is_ai column within occupation groups\n",
    "        fi_df['next_is_ai'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_ai'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        fi_df.loc[(fi_df['is_ai'] == 1) & (fi_df['next_is_ai'] == 1), 'num_switches'] = 0\n",
    "\n",
    "    else: # Definition 1\n",
    "        fi_df['is_automated'] = fi_df['label'].isin(['Automation']).astype(int)\n",
    "        fi_df['is_augmented'] = fi_df['label'].isin(['Augmentation']).astype(int)\n",
    "\n",
    "        # Create next_is_automated column within occupation groups\n",
    "        fi_df['next_is_automated'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_automated'].shift(-1).fillna(0).astype(int)\n",
    "        fi_df['next_is_augmented'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_augmented'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        ai_chain_indicator = (fi_df['is_automated'] == 1) & ((fi_df['next_is_automated'] == 1) | (fi_df['next_is_augmented'] == 1))\n",
    "        fi_df.loc[ai_chain_indicator, 'num_switches'] = 0\n",
    "\n",
    "    # Now with a counter for number of switches, calculate fragmentation index per occupation as mean of num_switches per occupation\n",
    "    fi_df = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['num_switches'].mean()\n",
    "    fi_df = fi_df.reset_index().rename(columns={'num_switches': 'fragmentation_index'})\n",
    "\n",
    "    # Save fragmentation index data\n",
    "    if save_filename:\n",
    "        fi_df.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    return fi_df\n",
    "\n",
    "\n",
    "\n",
    "# Merge fragmentation data with occupation analysis\n",
    "def merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var, save_filename=None):\n",
    "    # Merge fragmentation index with occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(fi_df, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    # Save occupation analysis with fragmentation index\n",
    "    if save_filename:\n",
    "        occupation_analysis.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    # Merge SOC levels with the occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(SOC_mappings, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate occupation analysis at the level of onet_occupation_code_var\n",
    "def aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var):\n",
    "    occupation_analysis_aggregated = occupation_analysis.groupby(\n",
    "        [onet_occupation_code_var, onet_occupation_title_var]\n",
    "    ).agg({\n",
    "        'fragmentation_index': 'mean',\n",
    "        ai_exposure_var: 'mean',\n",
    "        'ai_fraction': 'mean',\n",
    "        'num_tasks': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge SOC levels for FE\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.merge(\n",
    "        SOC_mappings, on=onet_occupation_code_var, how='left', suffixes=('', '_drop')\n",
    "    )\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.loc[:, ~occupation_analysis_aggregated.columns.str.endswith('_drop')]\n",
    "\n",
    "    return occupation_analysis_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc928850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SOC mappings to merge later\n",
    "# Read original occupation analysis with SOC mappings\n",
    "ONET = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_cleaned_tasks.csv\")\n",
    "\n",
    "# Keep only the relevant \n",
    "SOC_mappings = ONET[['O*NET-SOC Code', 'Occupation Title',\n",
    "                    'Major_Group_Code', 'Major_Group_Title',\n",
    "                    'Minor_Group_Code', 'Minor_Group_Title',\n",
    "                    'Broad_Occupation_Code', 'Broad_Occupation_Title',\n",
    "                    'Detailed_Occupation_Code', 'Detailed_Occupation_Title']].copy()\n",
    "SOC_mappings = SOC_mappings.drop_duplicates(subset=['O*NET-SOC Code', onet_occupation_code_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2038cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "KEEP_TERMS = [\"fragmentation_index\", \"ai_exposure\"]\n",
    "\n",
    "def results_to_tidy_df(res, *, x, definition, model_name, keep_terms=KEEP_TERMS):\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": res.params.index,\n",
    "        \"coef\": res.params.values,\n",
    "        \"se\": res.bse.values,\n",
    "        \"t\": res.tvalues.values,\n",
    "        \"p\": res.pvalues.values,\n",
    "    })\n",
    "\n",
    "    # keep only the coefficients you care about\n",
    "    out = out[out[\"term\"].isin(keep_terms)].copy()\n",
    "\n",
    "    out[\"prompt\"] = x\n",
    "    out[\"definition\"] = definition\n",
    "    out[\"model\"] = model_name\n",
    "    out[\"nobs\"] = int(res.nobs)\n",
    "    out[\"r2\"] = float(res.rsquared)\n",
    "    out[\"r2_adj\"] = float(res.rsquared_adj)\n",
    "    return out\n",
    "\n",
    "\n",
    "master_results = []   # list of DataFrames, concat once at end\n",
    "\n",
    "\n",
    "# Initialize the input file with the original data\n",
    "input_file_path_list = [f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\"]\n",
    "\n",
    "for x in range(1, 11): # Ignore 0 as it's the repetition of the original prompts\n",
    "    input_file_path_list.append(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_{x}.csv\")\n",
    "\n",
    "\n",
    "for x, input_file_path in enumerate(input_file_path_list):\n",
    "    merged_data = pd.read_csv(input_file_path)\n",
    "\n",
    "    # # filter occupations\n",
    "    # frequent_tasks_per_occupation_threshold = 3\n",
    "    # occupation_task_counts = merged_data.groupby('O*NET-SOC Code')['Task ID'].nunique()\n",
    "    # valid_occupations = occupation_task_counts[occupation_task_counts >= frequent_tasks_per_occupation_threshold].index\n",
    "    # merged_data = merged_data[merged_data['O*NET-SOC Code'].isin(valid_occupations)].reset_index(drop=True)\n",
    "\n",
    "    for definition in [1, 2, 3, 4]:\n",
    "        occupation_analysis = create_occupation_analysis(merged_data, 'O*NET-SOC Code', 'Occupation Title')\n",
    "        fi_df = construct_fragmentation_index(merged_data, desired_definition=definition)\n",
    "        occupation_analysis = merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var)\n",
    "        occupation_analysis_aggregated = aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var)\n",
    "\n",
    "        occupation_analysis_aggregated = occupation_analysis_aggregated.rename(columns={ai_exposure_var: 'ai_exposure'})\n",
    "        groups = occupation_analysis_aggregated[onet_occupation_code_var]\n",
    "\n",
    "        # Model A\n",
    "        mod_noFE = smf.ols(\n",
    "            \"ai_fraction ~ fragmentation_index + ai_exposure\",\n",
    "            data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups, \"use_correction\": True, \"df_correction\": True})\n",
    "        master_results.append(results_to_tidy_df(mod_noFE, x=x, definition=definition, model_name=\"noFE\"))\n",
    "\n",
    "        # Model B\n",
    "        mod_majorFE = smf.ols(\n",
    "            \"ai_fraction ~ fragmentation_index + ai_exposure + C(Major_Group_Code)\",\n",
    "            data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups, \"use_correction\": True, \"df_correction\": True})\n",
    "        master_results.append(results_to_tidy_df(mod_majorFE, x=x, definition=definition, model_name=\"majorFE\"))\n",
    "\n",
    "        # Model C\n",
    "        mod_minorFE = smf.ols(\n",
    "            \"ai_fraction ~ fragmentation_index + ai_exposure + C(Minor_Group_Code)\",\n",
    "            data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups, \"use_correction\": True, \"df_correction\": True})\n",
    "        master_results.append(results_to_tidy_df(mod_minorFE, x=x, definition=definition, model_name=\"minorFE\"))\n",
    "\n",
    "# one master df\n",
    "master_df = pd.concat(master_results, ignore_index=True)\n",
    "master_df.to_csv(f\"{output_data_path}/fragmentation_index_robustness_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5250260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "DEF_TO_EFI = {1: 3, 2: 4, 3: 1, 4: 2}\n",
    "\n",
    "TERM_LABEL = {\n",
    "    \"ai_exposure\": \"AI Exposure\",\n",
    "    \"fragmentation_index\": \"Empirical Fragmentation Index\",\n",
    "}\n",
    "\n",
    "MODEL_LABEL = {\n",
    "    \"noFE\": \"No FE\",\n",
    "    \"majorFE\": \"Major SOC Code FE\",\n",
    "    \"minorFE\": \"Minor SOC Code FE\",\n",
    "}\n",
    "\n",
    "# colors by term (purple-ish for fragmentation)\n",
    "TERM_COLOR = {\n",
    "    \"ai_exposure\": \"#1f77b4\",            # default-ish blue\n",
    "    \"fragmentation_index\": \"#6a0dad\",    # purple\n",
    "}\n",
    "\n",
    "def plot_six_coeffs(master_df, definition=1,\n",
    "                    models=(\"noFE\",\"majorFE\",\"minorFE\"),\n",
    "                    terms=(\"ai_exposure\", \"fragmentation_index\")):\n",
    "\n",
    "    df = master_df.copy()\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(int)\n",
    "\n",
    "    df = df[(df[\"definition\"] == definition) &\n",
    "            (df[\"term\"].isin(terms)) &\n",
    "            (df[\"model\"].isin(models))].copy()\n",
    "\n",
    "    df = df.sort_values([\"term\", \"model\", \"prompt\"])\n",
    "\n",
    "    # --- compute common y-lims per row (term), INCLUDING CI, and FORCE include 0 ---\n",
    "    row_ylims = {}\n",
    "    for term in terms:\n",
    "        dterm = df[df[\"term\"] == term]\n",
    "        if dterm.empty:\n",
    "            row_ylims[term] = (None, None)\n",
    "            continue\n",
    "\n",
    "        lo = (dterm[\"coef\"] - 1.96 * dterm[\"se\"]).min()\n",
    "        hi = (dterm[\"coef\"] + 1.96 * dterm[\"se\"]).max()\n",
    "\n",
    "        # padding based on span (fallback if flat)\n",
    "        span = hi - lo\n",
    "        pad = 0.05 * span if span > 0 else 0.1\n",
    "\n",
    "        ymin = lo - pad\n",
    "        ymax = hi + pad\n",
    "\n",
    "        # (2) force 0 inside the range\n",
    "        ymin = min(ymin, 0.0)\n",
    "        ymax = max(ymax, 0.0)\n",
    "\n",
    "        # small extra breathing room if 0 is exactly at edge\n",
    "        if np.isclose(ymin, 0.0):\n",
    "            ymin -= 0.05 * (ymax - ymin if ymax > ymin else 1.0)\n",
    "        if np.isclose(ymax, 0.0):\n",
    "            ymax += 0.05 * (ymax - ymin if ymax > ymin else 1.0)\n",
    "\n",
    "        row_ylims[term] = (ymin, ymax)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(terms), ncols=len(models),\n",
    "                             figsize=(8*len(models), 4*len(terms)), sharex=True)\n",
    "\n",
    "    if len(terms) == 1:\n",
    "        axes = np.array([axes])\n",
    "    if len(models) == 1:\n",
    "        axes = axes.reshape(len(terms), 1)\n",
    "\n",
    "    efi_num = DEF_TO_EFI.get(definition, definition)\n",
    "\n",
    "    for r, term in enumerate(terms):\n",
    "        color = TERM_COLOR.get(term, \"#1f77b4\")\n",
    "        for c, model in enumerate(models):\n",
    "            ax = axes[r, c]\n",
    "            d = df[(df[\"term\"] == term) & (df[\"model\"] == model)].sort_values(\"prompt\")\n",
    "\n",
    "            ci = 1.645 * d[\"se\"].to_numpy()\n",
    "            x = d[\"prompt\"].to_numpy()\n",
    "            y = d[\"coef\"].to_numpy()\n",
    "\n",
    "            # (1) term-specific coloring (purple for fragmentation)\n",
    "            ax.plot(x, y, marker=\"o\", color=color, label='Robustness Prompts')\n",
    "            ax.fill_between(x, y - ci, y + ci, alpha=0.20, color=color, label='90% CI')\n",
    "\n",
    "            # zero reference line\n",
    "            ax.axhline(0, color=\"#2B2D31\", linestyle=\"--\", lw=1, alpha=0.6)\n",
    "\n",
    "            # red dot for prompt == 0\n",
    "            mask0 = (x == 0)\n",
    "            if mask0.any():\n",
    "                ax.scatter(x[mask0], y[mask0], color=\"red\", s=50, zorder=5, label='Original Prompt')\n",
    "\n",
    "            # apply common y scale for this row (guaranteed to include 0)\n",
    "            ymin, ymax = row_ylims[term]\n",
    "            if ymin is not None:\n",
    "                ax.set_ylim(ymin, ymax)\n",
    "\n",
    "            term_disp = TERM_LABEL.get(term, term)\n",
    "            model_disp = MODEL_LABEL.get(model, model)\n",
    "            ax.set_title(f\"{term_disp} (EFI Definition {efi_num}) | {model_disp}\")\n",
    "\n",
    "            ax.set_xlabel(\"Prompt Number\")\n",
    "            ax.set_ylabel(\"Regression Coefficient\")\n",
    "            ax.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_plot_path}/fragmentation_index_robustness_definition_{definition}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# usage\n",
    "plot_six_coeffs(master_df, definition=3)\n",
    "plot_six_coeffs(master_df, definition=4)\n",
    "plot_six_coeffs(master_df, definition=1)\n",
    "plot_six_coeffs(master_df, definition=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
