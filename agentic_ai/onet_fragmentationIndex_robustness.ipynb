{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Jan 30, 2026\n",
    "#### Last Edit: Feb 1, 2026\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e03cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "my_onet_level = 'detailed'\n",
    "onet_occupation_code_var = 'Detailed_Occupation_Code'\n",
    "onet_occupation_title_var = 'Detailed_Occupation_Title'\n",
    "\n",
    "ai_exposure_var = 'human_E1_fraction'\n",
    "\n",
    "FREQUENT_TASKS = False  # Whether to use only frequent tasks or all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/fragmentationIndex_robustness'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots/fragmentationIndex_robustness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89302725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_analysis(df, onet_occupation_code_var, onet_occupation_title_var):\n",
    "    # Create occupation-level analysis for scatter plots\n",
    "    # Group by occupation and calculate label fractions and task counts\n",
    "    occupation_stats = []\n",
    "\n",
    "    for (soc_code, occ_title), group in df.groupby([onet_occupation_code_var, onet_occupation_title_var]):\n",
    "        num_tasks = group['Task ID'].nunique()\n",
    "        # num_occupations = group[onet_occupation_code_var].nunique()\n",
    "        total_tasks = len(group)\n",
    "        \n",
    "        manual_fraction = (group['label'] == 'Manual').sum() / total_tasks\n",
    "        augmentation_fraction = (group['label'] == 'Augmentation').sum() / total_tasks  \n",
    "        automation_fraction = (group['label'] == 'Automation').sum() / total_tasks\n",
    "        ai_fraction = augmentation_fraction + automation_fraction\n",
    "        gpt4_E0_fraction = (group['gpt4_exposure'] == 'E0').sum() / total_tasks\n",
    "        gpt4_E1_fraction = (group['gpt4_exposure'] == 'E1').sum() / total_tasks\n",
    "        gpt4_E2_fraction = (group['gpt4_exposure'] == 'E2').sum() / total_tasks\n",
    "        gpt4_aiExposure_fraction = gpt4_E1_fraction + gpt4_E2_fraction\n",
    "        human_E0_fraction = (group['human_labels'] == 'E0').sum() / total_tasks\n",
    "        human_E1_fraction = (group['human_labels'] == 'E1').sum() / total_tasks\n",
    "        human_E2_fraction = (group['human_labels'] == 'E2').sum() / total_tasks\n",
    "        human_aiExposure_fraction = human_E1_fraction + human_E2_fraction\n",
    "\n",
    "        \n",
    "        occupation_stats.append({\n",
    "            f'{onet_occupation_code_var}': soc_code,\n",
    "            f'{onet_occupation_title_var}': occ_title,\n",
    "            'num_tasks': num_tasks,\n",
    "            # 'num_occupations': num_occupations,\n",
    "            'manual_fraction': manual_fraction,\n",
    "            'ai_fraction': ai_fraction,\n",
    "            'augmentation_fraction': augmentation_fraction,\n",
    "            'automation_fraction': automation_fraction,\n",
    "            'gpt4_E0_fraction': gpt4_E0_fraction,\n",
    "            'gpt4_E1_fraction': gpt4_E1_fraction,\n",
    "            'gpt4_E2_fraction': gpt4_E2_fraction,\n",
    "            'gpt4_aiExposure_fraction': gpt4_aiExposure_fraction,\n",
    "            'human_E0_fraction': human_E0_fraction,\n",
    "            'human_E1_fraction': human_E1_fraction,\n",
    "            'human_E2_fraction': human_E2_fraction,\n",
    "            'human_aiExposure_fraction': human_aiExposure_fraction\n",
    "        })\n",
    "\n",
    "    occupation_analysis = pd.DataFrame(occupation_stats)\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "# Create fragmentation index dataframe for different definitions\n",
    "def construct_fragmentation_index(df, desired_definition=1, save_filename=None):\n",
    "    # Definition 1: Separate Augmentation and Automation; AI Chain starts with Automation or Augmentation task and terminates at the first Augmentation task; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 2: Treat all AI tasks similarly; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 3: Same as Definition 1, but use exposure based label (E1) for forming the \"AI-Chain\"s\n",
    "    # Definition 4: Same as Definition 1, but use exposure based label (E1 or E2) for forming the \"AI-Chain\"s\n",
    "    fi_df = df.copy()\n",
    "    \n",
    "    # Definitions 2, 3, and 4, are basically similar in construction, only differing in the labeling of tasks\n",
    "    if desired_definition != 1:\n",
    "        if desired_definition == 2:\n",
    "            # Use AI execution labels\n",
    "            fi_df['is_ai'] = fi_df['label'].isin(['Augmentation', 'Automation']).astype(int)\n",
    "        elif desired_definition == 3:\n",
    "            # Use exposure based label (E1)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1']).astype(int)\n",
    "        elif desired_definition == 4:\n",
    "            # Use exposure based label (E1 or E2)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1', 'E2']).astype(int)\n",
    "        \n",
    "        # Create next_is_ai column within occupation groups\n",
    "        fi_df['next_is_ai'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_ai'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        fi_df.loc[(fi_df['is_ai'] == 1) & (fi_df['next_is_ai'] == 1), 'num_switches'] = 0\n",
    "\n",
    "    else: # Definition 1\n",
    "        fi_df['is_automated'] = fi_df['label'].isin(['Automation']).astype(int)\n",
    "        fi_df['is_augmented'] = fi_df['label'].isin(['Augmentation']).astype(int)\n",
    "\n",
    "        # Create next_is_automated column within occupation groups\n",
    "        fi_df['next_is_automated'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_automated'].shift(-1).fillna(0).astype(int)\n",
    "        fi_df['next_is_augmented'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_augmented'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        ai_chain_indicator = (fi_df['is_automated'] == 1) & ((fi_df['next_is_automated'] == 1) | (fi_df['next_is_augmented'] == 1))\n",
    "        fi_df.loc[ai_chain_indicator, 'num_switches'] = 0\n",
    "\n",
    "    # Now with a counter for number of switches, calculate fragmentation index per occupation as mean of num_switches per occupation\n",
    "    fi_df = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['num_switches'].mean()\n",
    "    fi_df = fi_df.reset_index().rename(columns={'num_switches': 'fragmentation_index'})\n",
    "\n",
    "    # Save fragmentation index data\n",
    "    if save_filename:\n",
    "        fi_df.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    return fi_df\n",
    "\n",
    "\n",
    "\n",
    "# Merge fragmentation data with occupation analysis\n",
    "def merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var, save_filename=None):\n",
    "    # Merge fragmentation index with occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(fi_df, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    # Save occupation analysis with fragmentation index\n",
    "    if save_filename:\n",
    "        occupation_analysis.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    # Merge SOC levels with the occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(SOC_mappings, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate occupation analysis at the level of onet_occupation_code_var\n",
    "def aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var):\n",
    "    occupation_analysis_aggregated = occupation_analysis.groupby(\n",
    "        [onet_occupation_code_var, onet_occupation_title_var]\n",
    "    ).agg({\n",
    "        'fragmentation_index': 'mean',\n",
    "        ai_exposure_var: 'mean',\n",
    "        'ai_fraction': 'mean',\n",
    "        'num_tasks': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge SOC levels for FE\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.merge(\n",
    "        SOC_mappings, on=onet_occupation_code_var, how='left', suffixes=('', '_drop')\n",
    "    )\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.loc[:, ~occupation_analysis_aggregated.columns.str.endswith('_drop')]\n",
    "\n",
    "    return occupation_analysis_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc928850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SOC mappings to merge later\n",
    "# Read original occupation analysis with SOC mappings\n",
    "ONET = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_cleaned_tasks.csv\")\n",
    "\n",
    "# Keep only the relevant \n",
    "SOC_mappings = ONET[['O*NET-SOC Code', 'Occupation Title',\n",
    "                    'Major_Group_Code', 'Major_Group_Title',\n",
    "                    'Minor_Group_Code', 'Minor_Group_Title',\n",
    "                    'Broad_Occupation_Code', 'Broad_Occupation_Title',\n",
    "                    'Detailed_Occupation_Code', 'Detailed_Occupation_Title']].copy()\n",
    "SOC_mappings = SOC_mappings.drop_duplicates(subset=['O*NET-SOC Code', onet_occupation_code_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2038cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "KEEP_TERMS = [\"fragmentation_index\", \"ai_exposure\"]\n",
    "\n",
    "def results_to_tidy_df(res, *, x, definition, model_name, keep_terms=KEEP_TERMS):\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": res.params.index,\n",
    "        \"coef\": res.params.values,\n",
    "        \"se\": res.bse.values,\n",
    "        \"t\": res.tvalues.values,\n",
    "        \"p\": res.pvalues.values,\n",
    "    })\n",
    "\n",
    "    # keep only the coefficients you care about\n",
    "    out = out[out[\"term\"].isin(keep_terms)].copy()\n",
    "\n",
    "    out[\"prompt\"] = x\n",
    "    out[\"definition\"] = definition\n",
    "    out[\"model\"] = model_name\n",
    "    out[\"nobs\"] = int(res.nobs)\n",
    "    out[\"r2\"] = float(res.rsquared)\n",
    "    out[\"r2_adj\"] = float(res.rsquared_adj)\n",
    "    return out\n",
    "\n",
    "\n",
    "master_results = []   # list of DataFrames, concat once at end\n",
    "\n",
    "\n",
    "# Initialize the input file with the original data\n",
    "input_file_path_list = [f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\"]\n",
    "\n",
    "for x in range(1, 11): # Ignore 0 as it's the repetition of the original prompts\n",
    "    input_file_path_list.append(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT_{x}.csv\")\n",
    "\n",
    "\n",
    "for x, input_file_path in enumerate(input_file_path_list):\n",
    "    merged_data = pd.read_csv(input_file_path)\n",
    "\n",
    "    # # filter occupations\n",
    "    # frequent_tasks_per_occupation_threshold = 3\n",
    "    # occupation_task_counts = merged_data.groupby('O*NET-SOC Code')['Task ID'].nunique()\n",
    "    # valid_occupations = occupation_task_counts[occupation_task_counts >= frequent_tasks_per_occupation_threshold].index\n",
    "    # merged_data = merged_data[merged_data['O*NET-SOC Code'].isin(valid_occupations)].reset_index(drop=True)\n",
    "\n",
    "    for definition in [1, 2, 3, 4]:\n",
    "        occupation_analysis = create_occupation_analysis(merged_data, 'O*NET-SOC Code', 'Occupation Title')\n",
    "        fi_df = construct_fragmentation_index(merged_data, desired_definition=definition)\n",
    "        occupation_analysis = merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var)\n",
    "        occupation_analysis_aggregated = aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var)\n",
    "\n",
    "        occupation_analysis_aggregated = occupation_analysis_aggregated.rename(columns={ai_exposure_var: 'ai_exposure'})\n",
    "        groups = occupation_analysis_aggregated[onet_occupation_code_var]\n",
    "\n",
    "        # Model A\n",
    "        mod_noFE = smf.ols(\n",
    "            \"ai_fraction ~ fragmentation_index + ai_exposure\",\n",
    "            data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups, \"use_correction\": True, \"df_correction\": True})\n",
    "        master_results.append(results_to_tidy_df(mod_noFE, x=x, definition=definition, model_name=\"noFE\"))\n",
    "\n",
    "        # Model B\n",
    "        mod_majorFE = smf.ols(\n",
    "            \"ai_fraction ~ fragmentation_index + ai_exposure + C(Major_Group_Code)\",\n",
    "            data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups, \"use_correction\": True, \"df_correction\": True})\n",
    "        master_results.append(results_to_tidy_df(mod_majorFE, x=x, definition=definition, model_name=\"majorFE\"))\n",
    "\n",
    "        # Model C\n",
    "        mod_minorFE = smf.ols(\n",
    "            \"ai_fraction ~ fragmentation_index + ai_exposure + C(Minor_Group_Code)\",\n",
    "            data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\", cov_kwds={\"groups\": groups, \"use_correction\": True, \"df_correction\": True})\n",
    "        master_results.append(results_to_tidy_df(mod_minorFE, x=x, definition=definition, model_name=\"minorFE\"))\n",
    "\n",
    "# one master df\n",
    "master_df = pd.concat(master_results, ignore_index=True)\n",
    "master_df.to_csv(f\"{output_data_path}/fragmentation_index_robustness_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5250260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_df = pd.read_csv(f\"{output_data_path}/fragmentation_index_robustness_combined.csv\")\n",
    "\n",
    "# Plot results\n",
    "DEF_TO_EFI = {1: 3, 2: 4, 3: 1, 4: 2}\n",
    "\n",
    "TERM_LABEL  = {\"ai_exposure\": \"AI Exposure\", \"fragmentation_index\": \"EFI\"}\n",
    "MODEL_LABEL = {\n",
    "    \"noFE\": \"No Fixed Effects\",\n",
    "    \"majorFE\": \"SOC Major Group Fixed Effects\",\n",
    "    \"minorFE\": \"SOC Minor Group Fixed Effects\",\n",
    "}\n",
    "TERM_COLOR  = {\"ai_exposure\": \"#1f77b4\", \"fragmentation_index\": \"#C49000\"}  # blue, dark yellow\n",
    "\n",
    "\n",
    "def plot_six_coeffs(\n",
    "    master_df,\n",
    "    definition=1,\n",
    "    models=(\"noFE\", \"majorFE\", \"minorFE\"),\n",
    "    terms=(\"ai_exposure\", \"fragmentation_index\"),\n",
    "    ylims_by_def=None,                  # {def: {term: (ymin, ymax)}} or {def: (ymin, ymax)} or {term: (ymin, ymax)}\n",
    "    ci_mult=1.645,\n",
    "    output_plot_path=\".\",\n",
    "    dot_ms=6,\n",
    "    original_ms=7.5,\n",
    "    original_color=\"red\",\n",
    "    zero_color=\"#2b2b2b\",               # darker grey-ish than default black\n",
    "    legend_fs=13,\n",
    "    title_fs=16,\n",
    "    label_fs=14,\n",
    "):\n",
    "    df = master_df.copy()\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(int)\n",
    "\n",
    "    df = df[(df[\"definition\"] == definition) & df[\"term\"].isin(terms) & df[\"model\"].isin(models)].copy()\n",
    "    df = df.sort_values([\"term\", \"model\", \"prompt\"])\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(terms), ncols=len(models),\n",
    "        figsize=(6 * len(models), 4.5 * len(terms)),\n",
    "        sharex=True,\n",
    "    )\n",
    "    if len(terms) == 1:  axes = np.array([axes])\n",
    "    if len(models) == 1: axes = axes.reshape(len(terms), 1)\n",
    "\n",
    "    efi_num = DEF_TO_EFI.get(definition, definition)\n",
    "\n",
    "    def _resolve_term_ylim(term):\n",
    "        \"\"\"Flexible y-lims resolver.\"\"\"\n",
    "        if ylims_by_def is None:\n",
    "            return None\n",
    "        # Case 1: ylims_by_def[definition] exists\n",
    "        if isinstance(ylims_by_def, dict) and definition in ylims_by_def:\n",
    "            v = ylims_by_def[definition]\n",
    "            if isinstance(v, dict):   # {term: (ymin, ymax)}\n",
    "                return v.get(term)\n",
    "            return v                  # (ymin, ymax)\n",
    "        # Case 2: ylims_by_def is {term: (ymin, ymax)}\n",
    "        if isinstance(ylims_by_def, dict):\n",
    "            return ylims_by_def.get(term)\n",
    "        return None\n",
    "\n",
    "    for r, term in enumerate(terms):\n",
    "        term_color = TERM_COLOR.get(term, \"#1f77b4\")\n",
    "        term_disp  = TERM_LABEL.get(term, term)\n",
    "        term_ylim  = _resolve_term_ylim(term)  # compute once per row\n",
    "\n",
    "        for c, model in enumerate(models):\n",
    "            ax = axes[r, c]\n",
    "            d = df[(df[\"term\"] == term) & (df[\"model\"] == model)].sort_values(\"prompt\")\n",
    "\n",
    "            if d.empty:\n",
    "                if r == 0:\n",
    "                    ax.set_title(MODEL_LABEL.get(model, model), fontsize=title_fs)\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "                ax.set_axis_off()\n",
    "                continue\n",
    "\n",
    "            x = d[\"prompt\"].to_numpy()\n",
    "            y = d[\"coef\"].to_numpy()\n",
    "            ci = ci_mult * d[\"se\"].to_numpy()\n",
    "            yerr = np.vstack([ci, ci])\n",
    "\n",
    "            mask0 = (x == 0)\n",
    "            m = ~mask0\n",
    "\n",
    "            # Non-zero prompts\n",
    "            if m.any():\n",
    "                ax.errorbar(\n",
    "                    x[m], y[m], yerr=yerr[:, m],\n",
    "                    fmt=\"o\", ms=dot_ms, color=term_color, ecolor=term_color,\n",
    "                    elinewidth=1.6, capsize=3.5, capthick=1.6, alpha=0.95,\n",
    "                    zorder=3, label=\"Robustness Prompts + 90% CI\",\n",
    "                )\n",
    "\n",
    "            # Prompt 0 highlighted\n",
    "            if mask0.any():\n",
    "                ax.errorbar(\n",
    "                    x[mask0], y[mask0], yerr=yerr[:, mask0],\n",
    "                    fmt=\"o\", ms=original_ms, color=original_color, ecolor=original_color,\n",
    "                    elinewidth=1.8, capsize=3.5, capthick=1.8, alpha=1.0,\n",
    "                    zorder=5, label=\"Main Prompt + 90% CI\",\n",
    "                )\n",
    "\n",
    "            # Mean + zero lines\n",
    "            y_mean = float(np.nanmean(y))\n",
    "            ax.axhline(y_mean, color=term_color, linestyle=\"--\", lw=2, alpha=0.9,\n",
    "                       label=f\"Mean (across prompts) = {y_mean:.2f}\")\n",
    "            ax.axhline(0, color=zero_color, linestyle=\"--\", lw=1.6, alpha=0.85)\n",
    "\n",
    "            # Apply y-lims once per term-row (still OK to call per-ax, but resolved once)\n",
    "            if term_ylim is not None:\n",
    "                ax.set_ylim(*term_ylim)\n",
    "\n",
    "            # Titles/labels\n",
    "            if r == 0:\n",
    "                ax.set_title(MODEL_LABEL.get(model, model), fontsize=title_fs)\n",
    "            else:\n",
    "                ax.set_title(\"\")\n",
    "            \n",
    "            if c == 0:\n",
    "                ax.set_ylabel(f\"Estimated {term_disp} Coefficient\", fontsize=label_fs)\n",
    "            else:\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.tick_params(axis=\"y\", which=\"both\", left=False, labelleft=False)\n",
    "\n",
    "            ax.set_xlabel(\"\" if r == 0 else \"GPT Prompt\", fontsize=label_fs)\n",
    "            ax.legend(loc=\"best\", fontsize=legend_fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_plot_path}/fragmentation_index_robustness_definition_{definition}.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USAGE\n",
    "# =========================\n",
    "YLIMS_BY_DEF = {\n",
    "    1: {\"ai_exposure\": (-3.1, 1.1), \"fragmentation_index\": (-3.1, 1.1)},\n",
    "    2: {\"ai_exposure\": (-3.1, 1.1), \"fragmentation_index\": (-3.1, 1.1)},\n",
    "    3: {\"ai_exposure\": (-1.1, 1.1), \"fragmentation_index\": (-1.1, 1.1)},\n",
    "    4: {\"ai_exposure\": (-1.1, 1.1), \"fragmentation_index\": (-1.1, 1.1)},\n",
    "}\n",
    "\n",
    "for d in (1, 2, 3, 4):\n",
    "    plot_six_coeffs(master_df, definition=d, ylims_by_def=YLIMS_BY_DEF, output_plot_path=output_plot_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
