{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Nov 7, 2025\n",
    "#### Last Edit: Dec 13, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e03cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "my_onet_level = 'detailed'\n",
    "onet_occupation_code_var = 'Detailed_Occupation_Code'\n",
    "onet_occupation_title_var = 'Detailed_Occupation_Title'\n",
    "\n",
    "ai_exposure_var = 'human_E1_fraction'\n",
    "\n",
    "FREQUENT_TASKS = False  # Whether to use only frequent tasks or all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects/fragmentationIndex_frequent' if FREQUENT_TASKS else f'{input_data_path}/computed_objects/fragmentationIndex'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots/fragmentationIndex_frequent\" if FREQUENT_TASKS else f\"{main_folder_path}/writeup/plots/fragmentationIndex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2038cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged data\n",
    "if not FREQUENT_TASKS:\n",
    "    input_file_path = f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\" \n",
    "    merged_data = pd.read_csv(input_file_path)\n",
    "\n",
    "        # Remove occupations with three or less frequent tasks\n",
    "    frequent_tasks_per_occupation_threshold = 3\n",
    "    occupation_task_counts = merged_data.groupby('O*NET-SOC Code')['Task ID'].nunique()\n",
    "    valid_occupations = occupation_task_counts[occupation_task_counts >= frequent_tasks_per_occupation_threshold].index\n",
    "    merged_data = merged_data[merged_data['O*NET-SOC Code'].isin(valid_occupations)].reset_index(drop=True)\n",
    "else:\n",
    "    input_file_path = f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT_frequent/ONET_Eloundou_Anthropic_GPT.csv\"\n",
    "    merged_data = pd.read_csv(input_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032c8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SOC mappings to merge later\n",
    "# Read original occupation analysis with SOC mappings\n",
    "ONET = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_cleaned_tasks.csv\")\n",
    "\n",
    "# Keep only the relevant \n",
    "SOC_mappings = ONET[['O*NET-SOC Code', 'Occupation Title',\n",
    "                    'Major_Group_Code', 'Major_Group_Title',\n",
    "                    'Minor_Group_Code', 'Minor_Group_Title',\n",
    "                    'Broad_Occupation_Code', 'Broad_Occupation_Title',\n",
    "                    'Detailed_Occupation_Code', 'Detailed_Occupation_Title']].copy()\n",
    "SOC_mappings = SOC_mappings.drop_duplicates(subset=['O*NET-SOC Code', onet_occupation_code_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a3aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_analysis(df, onet_occupation_code_var, onet_occupation_title_var):\n",
    "    # Create occupation-level analysis for scatter plots\n",
    "    # Group by occupation and calculate label fractions and task counts\n",
    "    occupation_stats = []\n",
    "\n",
    "    for (soc_code, occ_title), group in df.groupby([onet_occupation_code_var, onet_occupation_title_var]):\n",
    "        num_tasks = group['Task ID'].nunique()\n",
    "        # num_occupations = group[onet_occupation_code_var].nunique()\n",
    "        total_tasks = len(group)\n",
    "        \n",
    "        manual_fraction = (group['label'] == 'Manual').sum() / total_tasks\n",
    "        augmentation_fraction = (group['label'] == 'Augmentation').sum() / total_tasks  \n",
    "        automation_fraction = (group['label'] == 'Automation').sum() / total_tasks\n",
    "        ai_fraction = augmentation_fraction + automation_fraction\n",
    "        gpt4_E0_fraction = (group['gpt4_exposure'] == 'E0').sum() / total_tasks\n",
    "        gpt4_E1_fraction = (group['gpt4_exposure'] == 'E1').sum() / total_tasks\n",
    "        gpt4_E2_fraction = (group['gpt4_exposure'] == 'E2').sum() / total_tasks\n",
    "        gpt4_aiExposure_fraction = gpt4_E1_fraction + gpt4_E2_fraction\n",
    "        human_E0_fraction = (group['human_labels'] == 'E0').sum() / total_tasks\n",
    "        human_E1_fraction = (group['human_labels'] == 'E1').sum() / total_tasks\n",
    "        human_E2_fraction = (group['human_labels'] == 'E2').sum() / total_tasks\n",
    "        human_aiExposure_fraction = human_E1_fraction + human_E2_fraction\n",
    "\n",
    "        \n",
    "        occupation_stats.append({\n",
    "            f'{onet_occupation_code_var}': soc_code,\n",
    "            f'{onet_occupation_title_var}': occ_title,\n",
    "            'num_tasks': num_tasks,\n",
    "            # 'num_occupations': num_occupations,\n",
    "            'manual_fraction': manual_fraction,\n",
    "            'ai_fraction': ai_fraction,\n",
    "            'augmentation_fraction': augmentation_fraction,\n",
    "            'automation_fraction': automation_fraction,\n",
    "            'gpt4_E0_fraction': gpt4_E0_fraction,\n",
    "            'gpt4_E1_fraction': gpt4_E1_fraction,\n",
    "            'gpt4_E2_fraction': gpt4_E2_fraction,\n",
    "            'gpt4_aiExposure_fraction': gpt4_aiExposure_fraction,\n",
    "            'human_E0_fraction': human_E0_fraction,\n",
    "            'human_E1_fraction': human_E1_fraction,\n",
    "            'human_E2_fraction': human_E2_fraction,\n",
    "            'human_aiExposure_fraction': human_aiExposure_fraction\n",
    "        })\n",
    "\n",
    "    occupation_analysis = pd.DataFrame(occupation_stats)\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "# Create fragmentation index dataframe for different definitions\n",
    "def construct_fragmentation_index(df, desired_definition=1, save_filename=None):\n",
    "    # Definition 1: Separate Augmentation and Automation; AI Chain starts with Automation or Augmentation task and terminates at the first Augmentation task; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 2: Treat all AI tasks similarly; Get number of switches between AI chains and Manual tasks\n",
    "    # Definition 3: Same as Definition 1, but use exposure based label (E1) for forming the \"AI-Chain\"s\n",
    "    # Definition 4: Same as Definition 1, but use exposure based label (E1 or E2) for forming the \"AI-Chain\"s\n",
    "    fi_df = df.copy()\n",
    "    \n",
    "    # Definitions 2, 3, and 4, are basically similar in construction, only differing in the labeling of tasks\n",
    "    if desired_definition != 1:\n",
    "        if desired_definition == 2:\n",
    "            # Use AI execution labels\n",
    "            fi_df['is_ai'] = fi_df['label'].isin(['Augmentation', 'Automation']).astype(int)\n",
    "        elif desired_definition == 3:\n",
    "            # Use exposure based label (E1)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1']).astype(int)\n",
    "        elif desired_definition == 4:\n",
    "            # Use exposure based label (E1 or E2)\n",
    "            fi_df['is_ai'] = fi_df['human_labels'].isin(['E1', 'E2']).astype(int)\n",
    "        \n",
    "        # Create next_is_ai column within occupation groups\n",
    "        fi_df['next_is_ai'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_ai'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        fi_df.loc[(fi_df['is_ai'] == 1) & (fi_df['next_is_ai'] == 1), 'num_switches'] = 0\n",
    "\n",
    "    else: # Definition 1\n",
    "        fi_df['is_automated'] = fi_df['label'].isin(['Automation']).astype(int)\n",
    "        fi_df['is_augmented'] = fi_df['label'].isin(['Augmentation']).astype(int)\n",
    "\n",
    "        # Create next_is_automated column within occupation groups\n",
    "        fi_df['next_is_automated'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_automated'].shift(-1).fillna(0).astype(int)\n",
    "        fi_df['next_is_augmented'] = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['is_augmented'].shift(-1).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate FI using incremental counter: only if current task and next task is AI do not increment FI\n",
    "        fi_df['num_switches'] = 1\n",
    "        ai_chain_indicator = (fi_df['is_automated'] == 1) & ((fi_df['next_is_automated'] == 1) | (fi_df['next_is_augmented'] == 1))\n",
    "        fi_df.loc[ai_chain_indicator, 'num_switches'] = 0\n",
    "\n",
    "    # Now with a counter for number of switches, calculate fragmentation index per occupation as mean of num_switches per occupation\n",
    "    fi_df = fi_df.groupby(['O*NET-SOC Code', 'Occupation Title'])['num_switches'].mean()\n",
    "    fi_df = fi_df.reset_index().rename(columns={'num_switches': 'fragmentation_index'})\n",
    "\n",
    "    # Save fragmentation index data\n",
    "    if save_filename:\n",
    "        fi_df.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    return fi_df\n",
    "\n",
    "\n",
    "\n",
    "# Merge fragmentation data with occupation analysis\n",
    "def merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var, save_filename=None):\n",
    "    # Merge fragmentation index with occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(fi_df, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    # Save occupation analysis with fragmentation index\n",
    "    if save_filename:\n",
    "        occupation_analysis.to_csv(f\"{output_data_path}/{save_filename}\", index=False)\n",
    "\n",
    "    # Merge SOC levels with the occupation analysis\n",
    "    occupation_analysis = occupation_analysis.merge(SOC_mappings, on=['O*NET-SOC Code', 'Occupation Title'], how='left')\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate occupation analysis at the level of onet_occupation_code_var\n",
    "def aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var):\n",
    "    occupation_analysis_aggregated = occupation_analysis.groupby(\n",
    "        [onet_occupation_code_var, onet_occupation_title_var]\n",
    "    ).agg({\n",
    "        'fragmentation_index': 'mean',\n",
    "        ai_exposure_var: 'mean',\n",
    "        'ai_fraction': 'mean',\n",
    "        'num_tasks': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge SOC levels for FE\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.merge(\n",
    "        SOC_mappings, on=onet_occupation_code_var, how='left', suffixes=('', '_drop')\n",
    "    )\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.loc[:, ~occupation_analysis_aggregated.columns.str.endswith('_drop')]\n",
    "\n",
    "    return occupation_analysis_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617af65e",
   "metadata": {},
   "source": [
    "## Calculate Fragmentation Index for 4 Definitions and Run the Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6363b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = []\n",
    "\n",
    "# Create dataset and run regressions for all 4 definitions\n",
    "for definition in [1, 2, 3, 4]:\n",
    "    # Get occupation data\n",
    "    occupation_analysis = create_occupation_analysis(merged_data, 'O*NET-SOC Code', 'Occupation Title')\n",
    "\n",
    "    # Get fragmentation index\n",
    "    fi_df = construct_fragmentation_index(merged_data, desired_definition=definition, save_filename=f'fragmentationIndex_def{definition}.csv')\n",
    "\n",
    "    # Merge fragmentation data with occupation analysis\n",
    "    occupation_analysis = merge_fragmentation_with_occupation_analysis(fi_df, occupation_analysis, SOC_mappings, onet_occupation_code_var, save_filename=f'occupation_analysis_with_fragmentationIndex_def{definition}.csv')\n",
    "\n",
    "    # Aggregate data at the onet_occupation_code_var level and add back SOC code information\n",
    "    occupation_analysis_aggregated = aggregate_occupation_analysis(occupation_analysis, onet_occupation_code_var, onet_occupation_title_var, SOC_mappings, ai_exposure_var)\n",
    "\n",
    "    # Rename regression columns for clarity\n",
    "    occupation_analysis_aggregated = occupation_analysis_aggregated.rename(columns={\n",
    "        ai_exposure_var: 'ai_exposure'\n",
    "    })\n",
    "\n",
    "    # Run regressions, clustering standard errors at the onet_occupation_code_var level\n",
    "    # --- Model A: no FE ---\n",
    "    mod_noFE = smf.ols(\n",
    "        formula=f'ai_fraction ~ fragmentation_index + ai_exposure',\n",
    "        data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\",\n",
    "            cov_kwds={\"groups\": occupation_analysis_aggregated[onet_occupation_code_var],\n",
    "                        \"use_correction\": True,\n",
    "                        \"df_correction\": True}\n",
    "                        )\n",
    "    regression_results.append(mod_noFE)\n",
    "\n",
    "    # --- Model B (Major group FE) ---\n",
    "    mod_majorFE = smf.ols(\n",
    "        formula=f'ai_fraction ~ fragmentation_index + ai_exposure + C(Major_Group_Code)',\n",
    "        data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\",\n",
    "            cov_kwds={\"groups\": occupation_analysis_aggregated[onet_occupation_code_var],\n",
    "                        \"use_correction\": True,\n",
    "                        \"df_correction\": True}\n",
    "                        )\n",
    "    regression_results.append(mod_majorFE)\n",
    "\n",
    "    # --- Model C (Minor group FE) ---\n",
    "    mod_minorFE = smf.ols(\n",
    "        formula=f'ai_fraction ~ fragmentation_index + ai_exposure + C(Minor_Group_Code)',\n",
    "        data=occupation_analysis_aggregated\n",
    "        ).fit(cov_type=\"cluster\",\n",
    "            cov_kwds={\"groups\": occupation_analysis_aggregated[onet_occupation_code_var],\n",
    "                        \"use_correction\": True,\n",
    "                        \"df_correction\": True}\n",
    "                        )\n",
    "    regression_results.append(mod_minorFE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
