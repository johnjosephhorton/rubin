{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 14, 2025\n",
    "#### Last Edit: Oct 21, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252758cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsl import QuestionFreeText, Scenario, Model, Survey\n",
    "from textwrap import dedent\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_task_sequence(occupation, tasks_data, output_data_path):\n",
    "    \"\"\"\n",
    "    Extract task sequence for an occupation using EDSL workflow.\n",
    "    Returns the ordered sequence of tasks.\n",
    "    \"\"\"\n",
    "    # Check if output file already exists\n",
    "    safe_title = occupation.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    output_folder = f'{output_data_path}/tasks_sequences'\n",
    "    output_file = os.path.join(output_folder, f\"{safe_title}.csv\")\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        return output_file, True  # Return file path and flag indicating it already existed\n",
    "    \n",
    "    # Check if we have tasks for this occupation\n",
    "    if tasks_data.empty:\n",
    "        print(f\"⚠️  Warning: No tasks found for occupation '{occupation}' - skipping\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "    \n",
    "    # Create task mappings\n",
    "    task_id_mapping = dict(zip(tasks_data['Task Title'], tasks_data['Task ID']))\n",
    "    soc_code_mapping = dict(zip(tasks_data['Task Title'], tasks_data['O*NET-SOC Code']))\n",
    "    \n",
    "    # Format tasks as numbered list\n",
    "    tasks_list = tasks_data['Task Title'].tolist()\n",
    "    tasks_text = \"\\n\".join([f\"{i}. {task}\" for i, task in enumerate(tasks_list, 1)])\n",
    "    num_tasks = len(tasks_list)\n",
    "    max_tokens = 32000\n",
    "    \n",
    "    print(f\"   • {num_tasks} tasks, using {max_tokens} max tokens\")\n",
    "\n",
    "    # Create scenario\n",
    "    scenario = Scenario({\n",
    "        \"occupation\": occupation,\n",
    "        \"tasks_list\": tasks_text,\n",
    "        \"num_tasks\": num_tasks\n",
    "    })\n",
    "\n",
    "    # Create question for task sequencing\n",
    "    q_sequence = QuestionFreeText(\n",
    "        question_name=\"task_sequence\",\n",
    "        question_text=dedent(\"\"\"\\\n",
    "            You are an expert in workflow analysis for the occupation: {{ occupation }}.\n",
    "            Below is a list of {{ num_tasks }} tasks that are part of this occupation:\n",
    "            {{ tasks_list }}\n",
    "            Provide the typical sequential order in which these tasks are performed in a real-world workflow.\n",
    "            Return your answer as a JSON array where each element has:\n",
    "            - \"Task Position\": the sequence number (1, 2, 3, etc.)\n",
    "            - \"Task Title\": the exact task text from the list above\n",
    "            Format: [{\"Task Position\": 1, \"Task Title\": \"...\"}, {\"Task Position\": 2, \"Task Title\": \"...\"}, ...]\n",
    "            Only return the JSON array, nothing else.\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create model using openai_v2 for reasoning capabilities\n",
    "        model = Model(\"gpt-5-mini\", service_name=\"openai_v2\", temperature=0.0, max_tokens=max_tokens)\n",
    "        \n",
    "        # Run sequence question\n",
    "        sequence_results = q_sequence.by(model).by([scenario]).run(progress_bar=False)\n",
    "        sequence_df = sequence_results.to_pandas()\n",
    "        sequence_json = sequence_df['answer.task_sequence'][0]\n",
    "        \n",
    "        # Debug: Print the raw response before cleaning\n",
    "        print(f\"   • Raw JSON length: {len(str(sequence_json))}\")\n",
    "        print(f\"   • Raw JSON preview: {str(sequence_json)[:50]}...\")\n",
    "        \n",
    "        # Clean the JSON response by removing markdown code blocks if present\n",
    "        if isinstance(sequence_json, str):\n",
    "            # Simple string replacement approach\n",
    "            cleaned_json = sequence_json\n",
    "            if '```json' in cleaned_json:\n",
    "                cleaned_json = cleaned_json.replace('```json', '')\n",
    "            if '```' in cleaned_json:\n",
    "                cleaned_json = cleaned_json.replace('```', '')\n",
    "            sequence_json = cleaned_json.strip()\n",
    "            print(f\"   • Cleaned JSON preview: {sequence_json[:50]}...\")\n",
    "        \n",
    "        # Check if the response is valid\n",
    "        if pd.isna(sequence_json) or not isinstance(sequence_json, str):\n",
    "            print(f\"❌ Error: Invalid response for '{occupation}' - got {type(sequence_json)} instead of string\")\n",
    "            return None, True  # Treat as already processed to skip\n",
    "        \n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            sequence_data = json.loads(sequence_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ JSON parsing failed, trying to clean response further...\")\n",
    "            print(f\"   Original error: {e}\")\n",
    "            print(f\"   Response starts with: {sequence_json[:100]}...\")\n",
    "            # Try additional cleanup\n",
    "            if sequence_json.startswith('```'):\n",
    "                lines = sequence_json.split('\\n')\n",
    "                if lines[0].strip() in ['```', '```json']:\n",
    "                    lines = lines[1:]  # Remove first line\n",
    "                if lines[-1].strip() == '```':\n",
    "                    lines = lines[:-1]  # Remove last line\n",
    "                sequence_json = '\\n'.join(lines).strip()\n",
    "                print(f\"   Cleaned response starts with: {sequence_json[:100]}...\")\n",
    "                try:\n",
    "                    sequence_data = json.loads(sequence_json)\n",
    "                    print(f\"   ✅ Successfully parsed after additional cleanup\")\n",
    "                except json.JSONDecodeError as e2:\n",
    "                    print(f\"   ❌ Still failed after cleanup: {e2}\")\n",
    "                    raise e  # Re-raise original error\n",
    "            else:\n",
    "                raise e  # Re-raise original error\n",
    "                \n",
    "        ordered_sequence_df = pd.DataFrame(sequence_data)\n",
    "        \n",
    "        # Add metadata columns\n",
    "        ordered_sequence_df['Occupation Title'] = occupation\n",
    "        ordered_sequence_df['Task ID'] = ordered_sequence_df['Task Title'].map(task_id_mapping)\n",
    "        ordered_sequence_df['O*NET-SOC Code'] = ordered_sequence_df['Task Title'].map(soc_code_mapping)\n",
    "        \n",
    "        # Reorder columns\n",
    "        ordered_sequence_df = ordered_sequence_df[['Task Position', 'Task Title', 'Task ID', 'O*NET-SOC Code', 'Occupation Title']]\n",
    "\n",
    "        # Save to file\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        ordered_sequence_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"   ✅ Successfully processed and saved task sequence\")\n",
    "        return output_file, False  # Return file path and flag indicating it was newly created\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSON Error for '{occupation}': {e}\")\n",
    "        print(f\"   Raw response: {sequence_json}\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error for '{occupation}': {e}\")\n",
    "        return None, True  # Treat as already processed to skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load O*NET data and extract unique occupation titles\n",
    "ONET = pd.read_csv(f'{output_data_path}/ONET_cleaned_tasks.csv')\n",
    "\n",
    "# Get all unique occupation titles from the dataset\n",
    "occupations_list = sorted(ONET['Occupation Title'].unique().tolist())\n",
    "print(f\"Found {len(occupations_list)} unique occupations in the dataset:\")\n",
    "\n",
    "# Set seed for reproducible random sampling\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# # Randomly sample 10% of occupations\n",
    "# sample_size = max(1, int(len(occupations_list) * 0.10))  # Ensure at least 1 occupation\n",
    "# sampled_occupations = random.sample(occupations_list, sample_size)\n",
    "# print(f\"Randomly selected {len(sampled_occupations)} occupations (5% of total) for processing:\")\n",
    "# print(f\"Sample: {sampled_occupations[:5]}...\" if len(sampled_occupations) > 5 else f\"Sample: {sampled_occupations}\")\n",
    "sampled_occupations = occupations_list\n",
    "\n",
    "# Process each occupation\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "\n",
    "\n",
    "for i, occupation in enumerate(sampled_occupations, 1):\n",
    "    # Filter data for this occupation\n",
    "    occupation_data = ONET[ONET['Occupation Title'] == occupation].copy()\n",
    "    \n",
    "    # Prepare task data\n",
    "    occupation_task_data = occupation_data[['Task ID', 'Task Title', 'O*NET-SOC Code']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Enhanced progress output\n",
    "    num_tasks = len(occupation_task_data)\n",
    "    print(f\"\\n[{i}/{len(sampled_occupations)}] {occupation}\")\n",
    "    \n",
    "    # Extract task sequence\n",
    "    output_file, already_existed = extract_task_sequence(occupation, occupation_task_data, output_data_path)\n",
    "    \n",
    "    if output_file is None:\n",
    "        error_count += 1\n",
    "    elif already_existed:\n",
    "        print(f\"   ⏭️  Already exists - skipping\")\n",
    "        skipped_count += 1\n",
    "    else:\n",
    "        processed_count += 1\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"PROCESSING COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"• {processed_count} occupations processed\")\n",
    "print(f\"• {skipped_count} occupations skipped (already existed)\")\n",
    "print(f\"• {error_count} occupations failed\")\n",
    "print(f\"• {len(sampled_occupations)} total occupations in sample\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
