{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 29, 2025\n",
    "#### Last Edit: Nov 2, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "# Modify the output path accordingly\n",
    "output_data_path = f'{input_data_path}/computed_objects/BLS_ONET_matchedEmpShares'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots/anthropic_AI_index/BLS_ONET_matchedEmpShares\"\n",
    "output_plot_path_by_BLS_sector = f\"{main_folder_path}/writeup/plots/anthropic_AI_index/BLS_ONET_matchedEmpShares/by_BLS_sector\"\n",
    "output_plot_path_by_ONET_level = f\"{main_folder_path}/writeup/plots/anthropic_AI_index/BLS_ONET_matchedEmpShares/by_ONET_level\"\n",
    "output_plot_path_by_weighting_scheme = f\"{main_folder_path}/writeup/plots/anthropic_AI_index/BLS_ONET_matchedEmpShares/by_weighting_scheme\"\n",
    "output_plot_path_by_dependent_var = f\"{main_folder_path}/writeup/plots/anthropic_AI_index/BLS_ONET_matchedEmpShares/by_dependent_var\"\n",
    "\n",
    "# Toggle: if True, randomly reassign occ_totalEmpShare weights in the merged master_df\n",
    "# during the merge_industry_employment_shares step. Set to False for default behavior.\n",
    "randomize_occ_weights = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a2e933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for path in [output_data_path, output_plot_path, \n",
    "             output_plot_path_by_BLS_sector, output_plot_path_by_ONET_level, \n",
    "             output_plot_path_by_weighting_scheme, output_plot_path_by_dependent_var]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bf72cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged data\n",
    "merged_data = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c0f3fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the supplemental tasks\n",
    "# merged_data = merged_data[merged_data['Task Type'] != 'Supplemental'].reset_index(drop=True)\n",
    "\n",
    "# # Drop rows whose Occupation Title includes 'Teachers, Postsecondary'\n",
    "# merged_data = merged_data[~merged_data['Occupation Title'].str.contains('Teachers, Postsecondary')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814f4c2",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "77ba8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_analysis(my_sector, my_onet_level,\n",
    "                               merged_data, onet_occupation_code_var, onet_occupation_title_var):\n",
    "    # Create occupation-level analysis for scatter plots\n",
    "    # Group by occupation and calculate label fractions and task counts\n",
    "    occupation_stats = []\n",
    "\n",
    "    for (soc_code, occ_title), group in merged_data.groupby([onet_occupation_code_var, onet_occupation_title_var]):\n",
    "        num_tasks = group['Task ID'].nunique()\n",
    "        # num_occupations = group[onet_occupation_code_var].nunique()\n",
    "        total_tasks = len(group)\n",
    "        \n",
    "        manual_fraction = (group['label'] == 'Manual').sum() / total_tasks\n",
    "        augmentation_fraction = (group['label'] == 'Augmentation').sum() / total_tasks  \n",
    "        automation_fraction = (group['label'] == 'Automation').sum() / total_tasks\n",
    "        ai_fraction = augmentation_fraction + automation_fraction\n",
    "        gpt4_E0_fraction = (group['gpt4_exposure'] == 'E0').sum() / total_tasks\n",
    "        gpt4_E1_fraction = (group['gpt4_exposure'] == 'E1').sum() / total_tasks\n",
    "        gpt4_E2_fraction = (group['gpt4_exposure'] == 'E2').sum() / total_tasks\n",
    "        gpt4_aiExposure_fraction = gpt4_E1_fraction + gpt4_E2_fraction\n",
    "        human_E0_fraction = (group['human_labels'] == 'E0').sum() / total_tasks\n",
    "        human_E1_fraction = (group['human_labels'] == 'E1').sum() / total_tasks\n",
    "        human_E2_fraction = (group['human_labels'] == 'E2').sum() / total_tasks\n",
    "        human_aiExposure_fraction = human_E1_fraction + human_E2_fraction\n",
    "\n",
    "        \n",
    "        occupation_stats.append({\n",
    "            f'{onet_occupation_code_var}': soc_code,\n",
    "            f'{onet_occupation_title_var}': occ_title,\n",
    "            'num_tasks': num_tasks,\n",
    "            # 'num_occupations': num_occupations,\n",
    "            'manual_fraction': manual_fraction,\n",
    "            'ai_fraction': ai_fraction,\n",
    "            'augmentation_fraction': augmentation_fraction,\n",
    "            'automation_fraction': automation_fraction,\n",
    "            'gpt4_E0_fraction': gpt4_E0_fraction,\n",
    "            'gpt4_E1_fraction': gpt4_E1_fraction,\n",
    "            'gpt4_E2_fraction': gpt4_E2_fraction,\n",
    "            'gpt4_aiExposure_fraction': gpt4_aiExposure_fraction,\n",
    "            'human_E0_fraction': human_E0_fraction,\n",
    "            'human_E1_fraction': human_E1_fraction,\n",
    "            'human_E2_fraction': human_E2_fraction,\n",
    "            'human_aiExposure_fraction': human_aiExposure_fraction\n",
    "        })\n",
    "\n",
    "    occupation_analysis = pd.DataFrame(occupation_stats)\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "\n",
    "def merge_industry_employment_shares_master(seed,\n",
    "                                            my_sector, my_onet_level,\n",
    "                                            onet_occupation_code_var, onet_occupation_title_var,\n",
    "                                            weight_cols,\n",
    "                                            occupation_analysis):\n",
    "\n",
    "\n",
    "    # Merge industry employment share weights for all NAICS sectors and create a master dataset\n",
    "    bls_sector_shares = pd.read_csv(f'{input_data_path}/computed_objects/BLS_ONET_empShares/bls_{my_sector}_ONET{my_onet_level}_empShares.csv')\n",
    "\n",
    "    # Ensure NAICS is string for consistent merging\n",
    "    bls_sector_shares['NAICS'] = bls_sector_shares['NAICS'].astype(str)\n",
    "\n",
    "    # Start with occupation_analysis as output_df\n",
    "    output_df = occupation_analysis.copy()\n",
    "\n",
    "    # For each weighting scheme, compute the weights and merge into output_df\n",
    "    # 1) occ_totalEmpShare\n",
    "    if 'occ_totalEmpShare' in weight_cols:\n",
    "        bls_occ_totalEmpShares = bls_sector_shares[['OCC_CODE', 'TOT_EMP']].groupby('OCC_CODE').sum().reset_index()\n",
    "\n",
    "        # Convert to % and change variable name\n",
    "        bls_occ_totalEmpShares['TOT_EMP'] = bls_occ_totalEmpShares['TOT_EMP'] / bls_occ_totalEmpShares['TOT_EMP'].sum()\n",
    "        bls_occ_totalEmpShares = bls_occ_totalEmpShares.rename(columns={'TOT_EMP': 'occ_totalEmpShare'})\n",
    "\n",
    "        # Add weight column to output_df\n",
    "        output_df = output_df.merge(bls_occ_totalEmpShares, left_on=onet_occupation_code_var, right_on=['OCC_CODE'], how='left')\n",
    "\n",
    "    # 2) sectorEmpShare\n",
    "    if 'sectorEmpShare' in weight_cols:\n",
    "        sector_weights_df = bls_sector_shares[['NAICS', 'NAICS_TITLE', 'TOT_EMP']].groupby(['NAICS', 'NAICS_TITLE']).sum('TOT_EMP')\n",
    "\n",
    "        # Convert to % and change variable name\n",
    "        sector_weights_df['TOT_EMP'] = sector_weights_df['TOT_EMP'] / sector_weights_df['TOT_EMP'].sum()\n",
    "        sector_weights_df = sector_weights_df.rename(columns={'TOT_EMP': 'sectorEmpShare'})\n",
    "\n",
    "        # Merge back sector weights to bls dataset to get sector-by-sector occupation data with sector weights\n",
    "        bls_sector_weights_df = bls_sector_shares[['NAICS', 'NAICS_TITLE', 'OCC_CODE', 'OCC_TITLE']].merge(sector_weights_df, on='NAICS', how='left')\n",
    "\n",
    "        \n",
    "        # Aggregate sector weights across occupations\n",
    "        bls_sector_weights_occupation_df = bls_sector_weights_df.groupby('OCC_CODE').sum('sectorEmpShare')\n",
    "\n",
    "        # Add weight column to output_df\n",
    "        output_df = output_df.merge(bls_sector_weights_occupation_df, left_on=onet_occupation_code_var, right_on=['OCC_CODE'], how='left')\n",
    "\n",
    "    # 3) occ_sectorEmpShare\n",
    "    if 'occ_sectorEmpShare' in weight_cols:\n",
    "        within_sector_weights_df = bls_sector_shares[['NAICS', 'NAICS_TITLE', 'OCC_CODE', 'OCC_TITLE', 'TOT_EMP']].copy()\n",
    "        within_sector_weights_df['occ_sectorEmpShare'] = within_sector_weights_df['TOT_EMP'] / within_sector_weights_df.groupby(['NAICS', 'NAICS_TITLE'])['TOT_EMP'].transform('sum')\n",
    "\n",
    "        # Calculate sum over all sectors\n",
    "        within_sector_weights = within_sector_weights_df[['OCC_CODE', 'OCC_TITLE', 'occ_sectorEmpShare']].groupby(['OCC_CODE', 'OCC_TITLE']).sum()\n",
    "\n",
    "        # Add weight column to output_df\n",
    "        output_df = output_df.merge(within_sector_weights, left_on=onet_occupation_code_var, right_on=['OCC_CODE'], how='left')\n",
    "\n",
    "    # Drop the 'OCC_CODE' column\n",
    "    output_df = output_df.drop(columns='OCC_CODE')\n",
    "\n",
    "    # Save master dataframe to CSV\n",
    "    output_df.to_csv(f'{output_data_path}/BLS{my_sector}_ONET{my_onet_level}.csv', index=False)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_weighted_regression(my_sector, my_onet_level,\n",
    "                           master_df, \n",
    "                           dependent_vars,\n",
    "                           weight_cols):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for weight_col in weight_cols:\n",
    "        for dependent_var in dependent_vars:\n",
    "            # Data prep\n",
    "            df = master_df[[dependent_var, 'num_tasks', weight_col]].copy()\n",
    "            df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[dependent_var, 'num_tasks', weight_col])\n",
    "\n",
    "            # Coerce numeric\n",
    "            df['num_tasks'] = pd.to_numeric(df['num_tasks'], errors='coerce')\n",
    "            df[dependent_var] = pd.to_numeric(df[dependent_var], errors='coerce')\n",
    "            df[weight_col] = pd.to_numeric(df[weight_col], errors='coerce')\n",
    "\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # Regression\n",
    "            model_wls = smf.wls(f'{dependent_var} ~ num_tasks', data=df, weights=df[weight_col]).fit(cov_type='HC3')\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'BLS_sector_level': my_sector,\n",
    "                'ONET_level': my_onet_level,\n",
    "                'dependent_var': dependent_var,\n",
    "                'weight_col': weight_col,\n",
    "                'model': 'WLS',\n",
    "                'coef_num_tasks': float(model_wls.params.get('num_tasks', np.nan)),\n",
    "                'std_err': float(model_wls.bse.get('num_tasks', np.nan)),\n",
    "                'pvalue': float(model_wls.pvalues.get('num_tasks', np.nan)),\n",
    "                'n_obs': int(model_wls.nobs)\n",
    "            })\n",
    "\n",
    "    reg_df = pd.DataFrame(results)\n",
    "\n",
    "    reg_out_dir = f\"{output_data_path}/regressions\"\n",
    "    os.makedirs(reg_out_dir, exist_ok=True)\n",
    "    out_file = f\"{reg_out_dir}/reg_BLS{my_sector}_ONET{my_onet_level}.csv\"\n",
    "    reg_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "                                  master_df,\n",
    "                                  weight_cols,\n",
    "                                  dependent_var='ai_fraction',\n",
    "                                  regressor='human_E1_fraction'):\n",
    "    \"\"\"\n",
    "    For each weight column, run WLS: ai_fraction ~ human_E1_fraction + num_tasks,\n",
    "    collect the coefficient on human_E1_fraction, and save all results in one CSV.\n",
    "    Returns the results DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for weight_col in weight_cols:\n",
    "        # Clean sample for this regression\n",
    "        df = master_df[[dependent_var, regressor, 'num_tasks', weight_col]].copy()\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.dropna(subset=[dependent_var, regressor, 'num_tasks', weight_col])\n",
    "\n",
    "        model = smf.wls(\n",
    "            f'{dependent_var} ~ {regressor} + num_tasks',\n",
    "            data=df,\n",
    "            weights=df[weight_col]\n",
    "        ).fit(cov_type='HC3')\n",
    "\n",
    "        results.append({\n",
    "            'BLS_sector_level': my_sector,\n",
    "            'ONET_level': my_onet_level,\n",
    "            'weight_col': weight_col,\n",
    "            'model': 'WLS_AIability_on_exposure',\n",
    "            'coef_human_E1_fraction': float(model.params.get(regressor, np.nan)),\n",
    "            'std_err': float(model.bse.get(regressor, np.nan)),\n",
    "            'pvalue': float(model.pvalues.get(regressor, np.nan)),\n",
    "            'n_obs': int(model.nobs),\n",
    "        })\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save one CSV with all weight cols\n",
    "    out_file = f\"{output_data_path}/reg_BLS{my_sector}_ONET{my_onet_level}_AIability_on_exposure.csv\"\n",
    "    res_df.to_csv(out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58762f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bls_sector_levels = ['sector']#, '3-digit', '4-digit', '5-digit', '6-digit']\n",
    "\n",
    "onet_levels = ['major', 'minor', 'broad', 'detailed']\n",
    "onet_occupation_code_vars = ['Major_Group_Code', 'Minor_Group_Code', 'Broad_Occupation_Code', 'Detailed_Occupation_Code']\n",
    "onet_occupation_title_vars = ['Major_Group_Title', 'Minor_Group_Title', 'Broad_Occupation_Title', 'Detailed_Occupation_Title']\n",
    "\n",
    "weight_cols = ['occ_totalEmpShare',         # Weight each occupation by occupation's share of total employment (ignoring sector shares)\n",
    "               'sectorEmpShare',            # Weight each occupation by its sector's share of total employment\n",
    "               'occ_sectorEmpShare']        # Weight each occupation by its share of employment within its sector and weight sectors equally  \n",
    "# plot_title_suffix_list = ['Weighted by Occupation Share of Total Employment',\n",
    "#                           'Weighted by Sector Share of Total Employment',\n",
    "#                           'Weighted by Occupation Employment Share of Sector']\n",
    "# plot_save_name_prefix_list = ['occupationEmpShareWeights',\n",
    "#                               'sectorEmpShareWeights',\n",
    "#                               'occupationEmpShareWithinSectorWeights']\n",
    "\n",
    "dependent_var_list = ['ai_fraction', 'human_E1_fraction']#, 'human_aiExposure_fraction']#, 'gpt4_E1_fraction']\n",
    "# dependent_var_title_list = ['Fraction of AI Tasks (Anthropic)', r'Fraction of Human $\\alpha$ Exposure (Eloundou et al.)']#, r'Fraction of Human \\gamma AI Exposure (Eloundou et al.)']#, r'Fraction of GPT-4 $\\alpha$ Exposure (Eloundou et al.)']\n",
    "# dependent_var_save_name_prefix_list = ['aiFraction', 'humanAiExposureFraction']#, 'humanAiExposureFraction']#, 'gpt4AiExposureFraction']\n",
    "\n",
    "\n",
    "# Run the analysis for each BLS sector level and ONET occupation level\n",
    "for my_sector in bls_sector_levels:\n",
    "    for my_onet_level, onet_occupation_code_var, onet_occupation_title_var in zip(onet_levels, onet_occupation_code_vars, onet_occupation_title_vars):\n",
    "        if my_onet_level != 'detailed':\n",
    "                    continue\n",
    "        \n",
    "        # Get occupation data\n",
    "        occupation_analysis = create_occupation_analysis(my_sector, my_onet_level,\n",
    "                                                         merged_data, onet_occupation_code_var, onet_occupation_title_var)\n",
    "\n",
    "        # Get master dataframe with all weighting schemes merged\n",
    "        master_df = merge_industry_employment_shares_master(0,\n",
    "                                                            my_sector, my_onet_level,\n",
    "                                                            onet_occupation_code_var, onet_occupation_title_var,\n",
    "                                                            weight_cols,\n",
    "                                                            occupation_analysis)\n",
    "        \n",
    "        # Regress for different dependent vars and weighting schemes\n",
    "        run_weighted_regression(my_sector, my_onet_level,\n",
    "                                master_df, \n",
    "                                dependent_var_list,\n",
    "                                weight_cols)\n",
    "        \n",
    "        # Regress exposure on AI ability\n",
    "        regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "                                    master_df,\n",
    "                                    weight_cols,\n",
    "                                    dependent_var='ai_fraction',\n",
    "                                    regressor='human_E1_fraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd0878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 6 regression files into ../data/computed_objects/BLS_ONET_matchedEmpShares/regressions/master_regressions.csv\n",
      "Combined 3 regression files into ../data/computed_objects/BLS_ONET_matchedEmpShares/regression_BLSsector_ONETdetailed/master_regressions.csv\n"
     ]
    }
   ],
   "source": [
    "# After the loops: combine all per-iteration regression outputs into a master file\n",
    "import os, glob\n",
    "paths_list = ['regressions', f'regression_BLS{my_sector}_ONET{my_onet_level}']\n",
    "for path in paths_list:\n",
    "    reg_out_dir = f'{output_data_path}/{path}'\n",
    "    os.makedirs(reg_out_dir, exist_ok=True)\n",
    "    reg_files = glob.glob(os.path.join(reg_out_dir, 'reg_BLS*.csv'))\n",
    "\n",
    "    combined = pd.concat([pd.read_csv(f) for f in reg_files], ignore_index=True)\n",
    "    combined = combined.sort_values(by=['model', 'BLS_sector_level', 'ONET_level', 'weight_col'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    combined = combined[(combined['model'] == 'WLS') | (combined['model'] == 'WLS_exposure_on_AIability')].reset_index(drop=True)\n",
    "    combined = combined.drop(columns=['model'])\n",
    "\n",
    "    try:\n",
    "        # Drop unimportant entries\n",
    "        combined = combined.drop(columns=['plot_prefix'])\n",
    "        combined = combined[combined['ONET_level'] != 'major']\n",
    "        # combined = combined[(combined['weight_col'] != 'ONE') & (combined['weight_col'] != 'occ_sectorEmpShare')].reset_index(drop=True)\n",
    "\n",
    "        # Sort order of entries\n",
    "        # Create mapping dicts\n",
    "        dependent_var_map = {'ai_fraction': 0, 'gpt4_E1_fraction': 2, 'human_E1_fraction': 1}\n",
    "        bls_map = {v: i for i, v in enumerate(bls_sector_levels)}\n",
    "        onet_map = {v: i for i, v in enumerate(onet_levels)}\n",
    "        weight_map = {v: i for i, v in enumerate(weight_cols)}\n",
    "\n",
    "        # Sort by all three with different mappings\n",
    "        combined = combined.sort_values(\n",
    "            by=[\"weight_col\", \"dependent_var\", \"BLS_sector_level\", \"ONET_level\"],\n",
    "            key=lambda col: (\n",
    "                col.map(dependent_var_map) if col.name == \"dependent_var\" else\n",
    "                col.map(weight_map) if col.name == \"weight_col\" else\n",
    "                col.map(bls_map) if col.name == \"BLS_sector_level\" else\n",
    "                col.map(onet_map)\n",
    "            )\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        master_file = os.path.join(reg_out_dir, 'master_regressions.csv')\n",
    "        combined.to_csv(master_file, index=False)\n",
    "        print(f\"Combined {len(reg_files)} regression files into {master_file}\")\n",
    "    except Exception as e:\n",
    "        master_file = os.path.join(reg_out_dir, 'master_regressions.csv')\n",
    "        combined.to_csv(master_file, index=False)\n",
    "        print(f\"Combined {len(reg_files)} regression files into {master_file}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0404f0b",
   "metadata": {},
   "source": [
    "## Placebo Test: Reshuffle Task-Occupation Assignment and Repeat the Same Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8854f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshuffle mode: preserve_unit_counts=True\n",
      "Running full observed (original) pipeline and saving outputs under seed_shuffles/seed_0 ...\n",
      "Saved observed master_regressions under seed_0 -> ../data/computed_objects/BLS_ONET_matchedEmpShares/seed_shuffles/seed_0/regressions/master_regressions.csv (9 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seeds: 100%|██████████| 1000/1000 [00:07<00:00, 128.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found observed master file: ../data/computed_objects/BLS_ONET_matchedEmpShares/seed_shuffles/seed_0/regressions/master_regressions.csv (9 rows)\n",
      "Using coef column 'coef_num_tasks' and keys ['BLS_sector_level', 'ONET_level', 'dependent_var', 'weight_col'] to identify regressions.\n"
     ]
    }
   ],
   "source": [
    "# ---- Begin: 100-seed reshuffle + analysis ----\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of unit columns exactly as you specified\n",
    "unit_cols = [\n",
    "    \"O*NET-SOC Code\",\n",
    "    \"Occupation Title\",\n",
    "    \"Base_SOC_Code\",\n",
    "    \"Major_Group_Code\",\n",
    "    \"Major_Group_Title\",\n",
    "    \"Minor_Group_Code\",\n",
    "    \"Minor_Group_Title\",\n",
    "    \"Broad_Occupation_Code\",\n",
    "    \"Broad_Occupation_Title\",\n",
    "    \"Detailed_Occupation_Code\",\n",
    "    \"Detailed_Occupation_Title\"\n",
    "]\n",
    "\n",
    "def reshuffle_tasks_preserve_unit_counts(merged_df, seed, unit_cols):\n",
    "    \"\"\"\n",
    "    Shuffle task records across units while preserving each unit's number of tasks.\n",
    "    - unit_cols: list of columns that define a unit (will remain as unit identity).\n",
    "    - All other columns (including 'Task ID' and 'Task Title' and other task properties)\n",
    "      are considered task properties and move with the task to a new unit.\n",
    "    Returns a new reshuffled DataFrame with same columns and same number of rows.\n",
    "    \"\"\"\n",
    "    # Defensive copy\n",
    "    df = merged_df.copy()\n",
    "\n",
    "    # Determine task/property columns = all columns except unit columns\n",
    "    task_columns = [c for c in df.columns if c not in unit_cols]\n",
    "\n",
    "    # Compute unit-level counts (preserve order)\n",
    "    unit_counts = df.groupby(unit_cols, sort=False).size().reset_index(name='n_tasks')\n",
    "\n",
    "    # Extract the task pool (task properties only)\n",
    "    task_pool = df[task_columns].sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    assigned_blocks = []\n",
    "    cursor = 0\n",
    "    for _, row in unit_counts.iterrows():\n",
    "        n = int(row['n_tasks'])\n",
    "        # slice of tasks to assign to this unit\n",
    "        tasks_slice = task_pool.iloc[cursor:cursor + n].copy().reset_index(drop=True)\n",
    "        cursor += n\n",
    "\n",
    "        # create a block with unit columns repeated for each assigned task\n",
    "        unit_block = pd.DataFrame([row[unit_cols].to_dict()] * n)\n",
    "        block = pd.concat([unit_block.reset_index(drop=True), tasks_slice.reset_index(drop=True)], axis=1)\n",
    "        assigned_blocks.append(block)\n",
    "\n",
    "    reshuffled = pd.concat(assigned_blocks, ignore_index=True)\n",
    "\n",
    "    # Keep original column order\n",
    "    reshuffled = reshuffled[df.columns]\n",
    "    return reshuffled\n",
    "\n",
    "\n",
    "def reshuffle_tasks_random_assignments(merged_df, seed, unit_cols):\n",
    "    \"\"\"\n",
    "    Randomly reassign tasks to units (occupations) without preserving the\n",
    "    original number of tasks per unit. This draws unit identities at random\n",
    "    (with replacement) for each task, so unit counts will vary across the\n",
    "    reshuffle.\n",
    "\n",
    "    - merged_df: DataFrame with task rows and unit-identifying columns in unit_cols\n",
    "    - seed: integer random seed\n",
    "    - unit_cols: list of columns that define a unit\n",
    "\n",
    "    Returns a DataFrame with the same columns as merged_df but unit columns\n",
    "    reassigned randomly.\n",
    "    \"\"\"\n",
    "    df = merged_df.copy()\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    # Build list of unique unit identity rows (dicts)\n",
    "    unit_id_rows = df[unit_cols].drop_duplicates().to_dict(orient='records')\n",
    "    n_units = len(unit_id_rows)\n",
    "    if n_units == 0:\n",
    "        # nothing to do\n",
    "        return df\n",
    "\n",
    "    # For each task row, sample a unit index (with replacement) so counts can vary\n",
    "    sampled_idxs = rng.randint(0, n_units, size=len(df))\n",
    "    sampled_units = [unit_id_rows[i] for i in sampled_idxs]\n",
    "    sampled_df = pd.DataFrame(sampled_units)\n",
    "\n",
    "    # Overwrite the unit columns in the copy\n",
    "    for col in unit_cols:\n",
    "        if col in sampled_df.columns:\n",
    "            df[col] = sampled_df[col].values\n",
    "\n",
    "    # Keep original column order\n",
    "    df = df[merged_df.columns]\n",
    "    return df\n",
    "\n",
    "# Where to save per-seed outputs (will create seed-specific subfolders under this)\n",
    "base_output_dir = output_data_path  # uses your notebook's variable by default\n",
    "seed_root = os.path.join(base_output_dir, \"seed_shuffles\")\n",
    "os.makedirs(seed_root, exist_ok=True)\n",
    "\n",
    "# Option: preserve the number of tasks per unit (occupation) or not.\n",
    "# Set to True to preserve counts (original behavior). Set to False to allow\n",
    "# arbitrary reassignment of tasks to units (unit counts will change).\n",
    "preserve_unit_counts = True  # <-- change this to False to use non-preserving reshuffle\n",
    "print(f\"Reshuffle mode: preserve_unit_counts={preserve_unit_counts}\")\n",
    "\n",
    "# ------------------ Run the full pipeline on the ORIGINAL (observed) dataset first and save under seed_0 ------------------\n",
    "print(\"Running full observed (original) pipeline and saving outputs under seed_shuffles/seed_0 ...\")\n",
    "seed0_output = os.path.join(seed_root, 'seed_0')\n",
    "os.makedirs(seed0_output, exist_ok=True)\n",
    "_saved_output_data_path = globals().get('output_data_path', None)\n",
    "globals()['output_data_path'] = seed0_output\n",
    "\n",
    "# Run the same loops used elsewhere in the notebook to produce observed regressions and plots\n",
    "for my_sector in bls_sector_levels:\n",
    "    for my_onet_level, onet_occupation_code_var, onet_occupation_title_var in zip(onet_levels, onet_occupation_code_vars, onet_occupation_title_vars):\n",
    "        if my_onet_level != 'detailed':\n",
    "            continue\n",
    "\n",
    "        occupation_analysis = create_occupation_analysis(my_sector, my_onet_level, merged_data, onet_occupation_code_var, onet_occupation_title_var)\n",
    "\n",
    "        for weight_col, plot_title_suffix, plot_save_name_prefix in zip(weight_cols, plot_title_suffix_list, plot_save_name_prefix_list):\n",
    "            for dependent_var, dependent_var_title, dependent_var_save_name_prefix in zip(dependent_var_list, dependent_var_title_list, dependent_var_save_name_prefix_list):\n",
    "                master_df = merge_industry_employment_shares(0, my_sector, my_onet_level, dependent_var, onet_occupation_code_var, onet_occupation_title_var, weight_col, occupation_analysis)\n",
    "                plot_weighted_regression_and_binned_scatter(my_sector, my_onet_level, dependent_var, dependent_var_title, dependent_var_save_name_prefix, master_df, weight_col, plot_title_suffix, plot_save_name_prefix)\n",
    "\n",
    "        # run exposure vs AIability regression per weight_col (once per sector/onet level)\n",
    "        for weight_col in weight_cols:\n",
    "            regress_exposure_on_AIability(my_sector, my_onet_level, onet_occupation_code_var, onet_occupation_title_var, weight_col)\n",
    "\n",
    "# Combine observed regression outputs generated under seed_0 into a master_regressions.csv inside seed_0/regressions\n",
    "reg_files_seed0 = glob.glob(os.path.join(seed0_output, '**', 'reg_BLS*.csv'), recursive=True)\n",
    "if len(reg_files_seed0) > 0:\n",
    "    combined_obs = pd.concat([pd.read_csv(f) for f in reg_files_seed0], ignore_index=True)\n",
    "    combined_obs = combined_obs.sort_values(by=['model', 'BLS_sector_level', 'ONET_level', 'weight_col'], ascending=True).reset_index(drop=True)\n",
    "    combined_obs = combined_obs[(combined_obs['model'] == 'WLS') | (combined_obs['model'] == 'WLS_exposure_on_AIability')].reset_index(drop=True)\n",
    "    if 'plot_prefix' in combined_obs.columns:\n",
    "        combined_obs = combined_obs.drop(columns=['plot_prefix'])\n",
    "    out_dir_obs = os.path.join(seed0_output, 'regressions')\n",
    "    os.makedirs(out_dir_obs, exist_ok=True)\n",
    "    master_file_obs = os.path.join(out_dir_obs, 'master_regressions.csv')\n",
    "    combined_obs.to_csv(master_file_obs, index=False)\n",
    "    print(f\"Saved observed master_regressions under seed_0 -> {master_file_obs} ({len(combined_obs)} rows)\")\n",
    "else:\n",
    "    print(\"Warning: No reg_BLS*.csv files found under seed_0 outputs to combine for the observed run.\")\n",
    "\n",
    "# restore original output_data_path if it existed (seed loop will override it per-seed later)\n",
    "if _saved_output_data_path is None:\n",
    "    globals().pop('output_data_path', None)\n",
    "else:\n",
    "    globals()['output_data_path'] = _saved_output_data_path\n",
    "# ------------------ End observed-to-seed0 pipeline ------------------\n",
    "\n",
    "# Now run reshuffles starting from seed 1\n",
    "n_seeds = 1000\n",
    "np.random.seed(42)  # for reproducibility of seed list\n",
    "seeds = list(range(1, n_seeds + 1))\n",
    "\n",
    "# Keep track of per-seed master_regressions file paths\n",
    "seed_master_files = []\n",
    "\n",
    "# Run the pipeline for each seed\n",
    "for seed in tqdm(seeds, desc=\"Seeds\"):\n",
    "    try:\n",
    "        reshuffled = merged_data.copy()\n",
    "        # # 1) Build reshuffled merged_data for this seed\n",
    "        # if preserve_unit_counts:\n",
    "        #     reshuffled = reshuffle_tasks_preserve_unit_counts(merged_data, seed=seed, unit_cols=unit_cols)\n",
    "        # else:\n",
    "        #     reshuffled = reshuffle_tasks_random_assignments(merged_data, seed=seed, unit_cols=unit_cols)\n",
    "\n",
    "        # 2) Temporarily redirect outputs to seed-specific folder so each seed's regressions don't collide\n",
    "        seed_output_data_path = os.path.join(seed_root, f\"seed_{seed}\")\n",
    "        if not os.path.exists(seed_output_data_path):\n",
    "            os.makedirs(seed_output_data_path, exist_ok=True)\n",
    "        else:\n",
    "            continue  # skip already-done seeds\n",
    "\n",
    "        # Save and restore original output_data_path after seed run\n",
    "        orig_output_data_path = globals().get('output_data_path', None)\n",
    "        globals()['output_data_path'] = seed_output_data_path\n",
    "\n",
    "        # 3) Run the same analysis you do in the notebook, but on `reshuffled`\n",
    "        #    We replicate the part of your main loop that creates occupation_analysis and then merges and runs regressions/plots.\n",
    "        #    Keep to the same ONET level(s) and sector levels you use in the notebook.\n",
    "        #    We'll follow the same loops you have. Adjust if you want fewer runs.\n",
    "        for my_sector in bls_sector_levels:\n",
    "            for my_onet_level, onet_occupation_code_var, onet_occupation_title_var in zip(onet_levels, onet_occupation_code_vars, onet_occupation_title_vars):\n",
    "                if my_onet_level != 'detailed':\n",
    "                    continue\n",
    "\n",
    "                # Use reshuffled for occupation analysis\n",
    "                occupation_analysis = create_occupation_analysis(my_sector, my_onet_level,\n",
    "                                                                 reshuffled, onet_occupation_code_var, onet_occupation_title_var)\n",
    "\n",
    "                for weight_col, plot_title_suffix, plot_save_name_prefix in zip(weight_cols, plot_title_suffix_list, plot_save_name_prefix_list):\n",
    "                    for dependent_var, dependent_var_title, dependent_var_save_name_prefix in zip(dependent_var_list, dependent_var_title_list, dependent_var_save_name_prefix_list):\n",
    "                        master_df = merge_industry_employment_shares(seed,\n",
    "                                                                     my_sector,\n",
    "                                                                    my_onet_level,\n",
    "                                                                    dependent_var,\n",
    "                                                                    onet_occupation_code_var, onet_occupation_title_var,\n",
    "                                                                    weight_col,\n",
    "                                                                    occupation_analysis)\n",
    "\n",
    "                        plot_weighted_regression_and_binned_scatter(my_sector, my_onet_level,\n",
    "                                                                   dependent_var, dependent_var_title, dependent_var_save_name_prefix,\n",
    "                                                                   master_df, weight_col,\n",
    "                                                                   plot_title_suffix, plot_save_name_prefix)\n",
    "                    # run exposure vs AIability regression per weight_col\n",
    "                    regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "                                                  onet_occupation_code_var, onet_occupation_title_var,\n",
    "                                                  weight_col)\n",
    "\n",
    "        # 4) After finishing seed runs, run the same combining code you have that creates master_regressions.csv\n",
    "        #    (Your notebook's combining code expects variables my_onet_level & my_sector from the last loop; to be safe, we'll recompute and call it similarly)\n",
    "        # We'll create combined master_regressions within the seed folder:\n",
    "        # reuse your combining logic but pointing at this seed's output folder\n",
    "        try:\n",
    "            # try to find all reg files under this seed output folder\n",
    "            reg_files = glob.glob(os.path.join(seed_output_data_path, '**', 'reg_BLS*.csv'), recursive=True)\n",
    "            if len(reg_files) == 0:\n",
    "                print(f\"[seed {seed}] No reg files found under {seed_output_data_path}; skipping combine.\")\n",
    "            else:\n",
    "                combined = pd.concat([pd.read_csv(f) for f in reg_files], ignore_index=True)\n",
    "                # Apply the same cleaning/sorting you do in the notebook\n",
    "                combined = combined.sort_values(by=['model', 'BLS_sector_level', 'ONET_level', 'weight_col'], ascending=True).reset_index(drop=True)\n",
    "                combined = combined[(combined['model'] == 'WLS') | (combined['model'] == 'WLS_exposure_on_AIability')].reset_index(drop=True)\n",
    "                # drop model and unneeded cols if present\n",
    "                if 'plot_prefix' in combined.columns:\n",
    "                    combined = combined.drop(columns=['plot_prefix'])\n",
    "                combined = combined[combined['ONET_level'] != 'major'] if 'ONET_level' in combined.columns else combined\n",
    "\n",
    "                out_dir = os.path.join(seed_output_data_path, 'regressions')\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                master_file = os.path.join(out_dir, 'master_regressions.csv')\n",
    "                combined.to_csv(master_file, index=False)\n",
    "                seed_master_files.append(master_file)\n",
    "                if seed % 50 == 0:\n",
    "                    print(f\"[seed {seed}] Combined regressions -> {master_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[seed {seed}] Failed to combine regression files: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[seed {seed}] ERROR during seed processing: {e}\")\n",
    "    # finally:\n",
    "    #     # restore original output_data_path global\n",
    "    #     if orig_output_data_path is None:\n",
    "    #         globals().pop('output_data_path', None)\n",
    "    #     else:\n",
    "    #         globals()['output_data_path'] = orig_output_data_path\n",
    "\n",
    "# ---- Collect coefficients across seeds and plot histograms (one plot per regression id) ----\n",
    "\n",
    "# Prefer the seed_0 observed master file if it exists, otherwise look for any observed master not in seed_shuffles\n",
    "seed0_master = os.path.join(seed_root, 'seed_0', 'regressions', 'master_regressions.csv')\n",
    "if os.path.exists(seed0_master):\n",
    "    observed_master_file = seed0_master\n",
    "else:\n",
    "    observed_master_candidates = glob.glob(os.path.join(base_output_dir, '**', 'master_regressions.csv'), recursive=True)\n",
    "    # prefer one that is not in the seed_shuffles folder\n",
    "    observed_master_candidates = [p for p in observed_master_candidates if 'seed_shuffles' not in p]\n",
    "    observed_master_file = observed_master_candidates[0] if len(observed_master_candidates) > 0 else None\n",
    "\n",
    "if observed_master_file is None:\n",
    "    print(\"WARNING: Could not find an observed master_regressions.csv. Observed value will not be plotted.\")\n",
    "else:\n",
    "    observed_df = pd.read_csv(observed_master_file)\n",
    "    print(f\"Found observed master file: {observed_master_file} ({len(observed_df)} rows)\")\n",
    "\n",
    "# Load per-seed master files into a single DataFrame with 'seed' column\n",
    "seed_dfs = []\n",
    "for seed in seeds:\n",
    "    f = os.path.join(seed_root, f\"seed_{seed}\", \"regressions\", \"master_regressions.csv\")\n",
    "    if os.path.exists(f):\n",
    "        try:\n",
    "            d = pd.read_csv(f)\n",
    "            d['seed'] = seed\n",
    "            seed_dfs.append(d)\n",
    "        except Exception as e:\n",
    "            print(f\"[seed {seed}] failed to read {f}: {e}\")\n",
    "    else:\n",
    "        # don't spam if many missing - print only occasionally\n",
    "        print(f\"[seed {seed}] master_regressions.csv not found at expected path {f}\")\n",
    "\n",
    "if len(seed_dfs) == 0:\n",
    "    raise RuntimeError(\"No per-seed master_regressions found; aborting histogram plotting. Check that the seed runs produced regressions.\")\n",
    "\n",
    "all_seeds_df = pd.concat(seed_dfs, ignore_index=True)\n",
    "\n",
    "# Identify coefficient column (try the commonly used names first)\n",
    "# look in the per-seed combined DataFrame first, then observed if needed\n",
    "coef_col = None\n",
    "candidates = []\n",
    "if 'coef_num_tasks' in all_seeds_df.columns:\n",
    "    candidates.append('coef_num_tasks')\n",
    "if 'coef_human_E1_fraction' in all_seeds_df.columns:\n",
    "    candidates.append('coef_human_E1_fraction')\n",
    "if len(candidates) == 0:\n",
    "    candidates = [c for c in all_seeds_df.columns if str(c).startswith('coef')]\n",
    "if len(candidates) == 0 and 'observed_df' in locals():\n",
    "    # try observed file as a last resort\n",
    "    candidates = [c for c in observed_df.columns if str(c).startswith('coef')]\n",
    "if len(candidates) == 0:\n",
    "    raise RuntimeError(\"Could not find a coefficient column in per-seed master_regressions or observed master. Look for 'coef_num_tasks' or other 'coef_*' columns.\")\n",
    "coef_col = candidates[0]\n",
    "\n",
    "# Determine keys that uniquely identify a regression entry (use string names)\n",
    "candidate_keys = ['BLS_sector_level', 'ONET_level', 'dependent_var', 'weight_col']\n",
    "# prefer keys present in the per-seed dataframe\n",
    "key_cols = [c for c in candidate_keys if c in all_seeds_df.columns]\n",
    "# if observed exists and provides a better set of keys, prefer intersection that is present in both\n",
    "if 'observed_df' in locals():\n",
    "    obs_keys = [c for c in candidate_keys if c in observed_df.columns]\n",
    "    if len(obs_keys) > 0:\n",
    "        inter = [c for c in candidate_keys if c in obs_keys and c in all_seeds_df.columns]\n",
    "        if len(inter) > 0:\n",
    "            key_cols = inter\n",
    "# final fallback: keep any sensible keys present in all_seeds_df\n",
    "if len(key_cols) == 0:\n",
    "    key_cols = [c for c in ['dependent_var', 'weight_col'] if c in all_seeds_df.columns]\n",
    "\n",
    "print(f\"Using coef column '{coef_col}' and keys {key_cols} to identify regressions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved histogram for BLS_sector_level-sector__ONET_level-detailed__dependent_var-human_E1_fraction__weight_col-occ_sectorEmpShare -> ../data/computed_objects/BLS_ONET_matchedEmpShares/shuffle_histograms/hist_BLS_sector_level-sector__ONET_level-detailed__dependent_var-human_E1_fraction__weight_col-occ_sectorEmpShare.png\n",
      "Saved histogram for BLS_sector_level-sector__ONET_level-detailed__dependent_var-ai_fraction__weight_col-occ_sectorEmpShare -> ../data/computed_objects/BLS_ONET_matchedEmpShares/shuffle_histograms/hist_BLS_sector_level-sector__ONET_level-detailed__dependent_var-ai_fraction__weight_col-occ_sectorEmpShare.png\n",
      "Saved histogram for BLS_sector_level-sector__ONET_level-detailed__dependent_var-ai_fraction__weight_col-occ_totalEmpShare -> ../data/computed_objects/BLS_ONET_matchedEmpShares/shuffle_histograms/hist_BLS_sector_level-sector__ONET_level-detailed__dependent_var-ai_fraction__weight_col-occ_totalEmpShare.png\n",
      "Saved histogram for BLS_sector_level-sector__ONET_level-detailed__dependent_var-human_E1_fraction__weight_col-occ_totalEmpShare -> ../data/computed_objects/BLS_ONET_matchedEmpShares/shuffle_histograms/hist_BLS_sector_level-sector__ONET_level-detailed__dependent_var-human_E1_fraction__weight_col-occ_totalEmpShare.png\n",
      "Saved histogram for BLS_sector_level-sector__ONET_level-detailed__dependent_var-human_E1_fraction__weight_col-sectorEmpShare -> ../data/computed_objects/BLS_ONET_matchedEmpShares/shuffle_histograms/hist_BLS_sector_level-sector__ONET_level-detailed__dependent_var-human_E1_fraction__weight_col-sectorEmpShare.png\n",
      "Saved histogram for BLS_sector_level-sector__ONET_level-detailed__dependent_var-ai_fraction__weight_col-sectorEmpShare -> ../data/computed_objects/BLS_ONET_matchedEmpShares/shuffle_histograms/hist_BLS_sector_level-sector__ONET_level-detailed__dependent_var-ai_fraction__weight_col-sectorEmpShare.png\n",
      "Skipping BLS_sector_level-sector__ONET_level-detailed__dependent_var-nan__weight_col-occ_sectorEmpShare: no coef values found across seeds.\n",
      "Skipping BLS_sector_level-sector__ONET_level-detailed__dependent_var-nan__weight_col-occ_totalEmpShare: no coef values found across seeds.\n",
      "Skipping BLS_sector_level-sector__ONET_level-detailed__dependent_var-nan__weight_col-sectorEmpShare: no coef values found across seeds.\n",
      "Done: created 6 histogram(s). Per-seed master_regressions (if produced) were stored under:\n",
      "  ../data/computed_objects/BLS_ONET_matchedEmpShares/seed_shuffles\n",
      "Histograms saved under:\n",
      "  ../data/computed_objects/BLS_ONET_matchedEmpShares/shuffle_histograms\n"
     ]
    }
   ],
   "source": [
    "# Prepare output dir for combined histograms\n",
    "hist_out_dir = os.path.join(base_output_dir, \"shuffle_histograms\")\n",
    "os.makedirs(hist_out_dir, exist_ok=True)\n",
    "\n",
    "# Prefer regressions present in observed master if available, otherwise use union across seeds\n",
    "if len(key_cols) == 0:\n",
    "    # No clear keys available: fall back to plotting one histogram aggregating all seed coefficients\n",
    "    unique_keys_df = pd.DataFrame([{}])\n",
    "else:\n",
    "    if 'observed_df' in locals():\n",
    "        # Use observed regressions if possible (safer to plot what was actually run)\n",
    "        obs_keys_present = [c for c in key_cols if c in observed_df.columns]\n",
    "        if len(obs_keys_present) == len(key_cols):\n",
    "            unique_keys_df = observed_df[key_cols].drop_duplicates().reset_index(drop=True)\n",
    "        else:\n",
    "            # observed missing some key columns: fall back to union across seeds\n",
    "            unique_keys_df = all_seeds_df[key_cols].drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        unique_keys_df = all_seeds_df[key_cols].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "plots_created = 0\n",
    "for idx, key_row in unique_keys_df.iterrows():\n",
    "    # build boolean mask across all seeds; if no key_cols, this stays all True\n",
    "    mask = np.ones(len(all_seeds_df), dtype=bool)\n",
    "    label_parts = []\n",
    "    for col in key_cols:\n",
    "        # key_row may be an empty dict when key_cols == []\n",
    "        val = key_row[col]\n",
    "        mask &= (all_seeds_df[col] == val)\n",
    "        label_parts.append(f\"{val}\")\n",
    "\n",
    "    if len(key_cols) == 0:\n",
    "        regression_id = \"all_regressions\"\n",
    "    else:\n",
    "        regression_id = \"__\".join([f\"{col}-{str(key_row[col])}\" for col in key_cols])\n",
    "\n",
    "    coef_vals = all_seeds_df.loc[mask, coef_col].dropna().astype(float).values\n",
    "\n",
    "    if coef_vals.size == 0:\n",
    "        print(f\"Skipping {regression_id}: no coef values found across seeds.\")\n",
    "        continue\n",
    "\n",
    "    # Plot single histogram that aggregates coefficients from all seeds for this regression\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.hist(coef_vals, bins=min(30, max(5, int(len(coef_vals) / 2))), color='orange', edgecolor='k', alpha=0.9)\n",
    "    plt.xlabel(coef_col)\n",
    "    plt.ylabel('Count')\n",
    "    title = f\"{len(coef_vals)} Reshuffles: {' | '.join(label_parts) if len(label_parts)>0 else 'all'}\\n\\n(Randomized Weights, Fixed Task Assignment)\"\n",
    "    plt.title(title)\n",
    "\n",
    "    # If observed data exists, overlay the observed coefficient as a dashed red line\n",
    "    if 'observed_df' in locals():\n",
    "        # if we have key_cols, try to match observed rows; otherwise use any observed coef available\n",
    "        if len(key_cols) > 0:\n",
    "            mask_obs = np.ones(len(observed_df), dtype=bool)\n",
    "            for col in key_cols:\n",
    "                if col in observed_df.columns:\n",
    "                    mask_obs &= (observed_df[col] == key_row[col])\n",
    "                else:\n",
    "                    # cannot match on this key in observed; set mask_obs all False to skip\n",
    "                    mask_obs &= False\n",
    "            obs_series = observed_df.loc[mask_obs, coef_col].dropna().astype(float) if mask_obs.any() else pd.Series(dtype=float)\n",
    "        else:\n",
    "            obs_series = observed_df[coef_col].dropna().astype(float) if coef_col in observed_df.columns else pd.Series(dtype=float)\n",
    "\n",
    "        if len(obs_series) > 0:\n",
    "            obs_val = float(obs_series.iloc[0])\n",
    "            plt.axvline(obs_val, color='red', linestyle='--', lw=2, label='Observed')\n",
    "            plt.axvline(0, color='black', linestyle='--', lw=1.5)\n",
    "            # annotate percentile: how many seeds are below the observed value\n",
    "            percentile = (coef_vals < obs_val).mean() * 100.0\n",
    "            plt.legend(loc = 'upper right', title=f'Observed (pct below: {percentile:.1f}% )')\n",
    "            plt.xlim(-0.004, 0.004)\n",
    "\n",
    "    out_file = os.path.join(hist_out_dir, f\"hist_{regression_id}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_file, dpi=300)\n",
    "    plt.close()\n",
    "    plots_created += 1\n",
    "    print(f\"Saved histogram for {regression_id} -> {out_file}\")\n",
    "\n",
    "print(f\"Done: created {plots_created} histogram(s). Per-seed master_regressions (if produced) were stored under:\\n  {seed_root}\")\n",
    "print(f\"Histograms saved under:\\n  {hist_out_dir}\")\n",
    "\n",
    "# ---- End cell ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47f486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
