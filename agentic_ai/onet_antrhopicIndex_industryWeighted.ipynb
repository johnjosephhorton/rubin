{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 29, 2025\n",
    "#### Last Edit: Nov 3, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "# Modify the output path accordingly\n",
    "output_data_path = f'{input_data_path}/computed_objects/BLS_ONET_matchedEmpShares'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots/anthropic_AI_index/BLS_ONET_matchedEmpShares\"\n",
    "\n",
    "# Toggle: if True, randomly reassign occ_totalEmpShare weights in the merged master_df\n",
    "# during the merge_industry_employment_shares step. Set to False for default behavior.\n",
    "randomize_occ_weights = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a2e933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814f4c2",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "77ba8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_analysis(df, onet_occupation_code_var, onet_occupation_title_var):\n",
    "    # Create occupation-level analysis for scatter plots\n",
    "    # Group by occupation and calculate label fractions and task counts\n",
    "    occupation_stats = []\n",
    "\n",
    "    for (soc_code, occ_title), group in df.groupby([onet_occupation_code_var, onet_occupation_title_var]):\n",
    "        num_tasks = group['Task ID'].nunique()\n",
    "        # num_occupations = group[onet_occupation_code_var].nunique()\n",
    "        total_tasks = len(group)\n",
    "        \n",
    "        manual_fraction = (group['label'] == 'Manual').sum() / total_tasks\n",
    "        augmentation_fraction = (group['label'] == 'Augmentation').sum() / total_tasks  \n",
    "        automation_fraction = (group['label'] == 'Automation').sum() / total_tasks\n",
    "        ai_fraction = augmentation_fraction + automation_fraction\n",
    "        gpt4_E0_fraction = (group['gpt4_exposure'] == 'E0').sum() / total_tasks\n",
    "        gpt4_E1_fraction = (group['gpt4_exposure'] == 'E1').sum() / total_tasks\n",
    "        gpt4_E2_fraction = (group['gpt4_exposure'] == 'E2').sum() / total_tasks\n",
    "        gpt4_aiExposure_fraction = gpt4_E1_fraction + gpt4_E2_fraction\n",
    "        human_E0_fraction = (group['human_labels'] == 'E0').sum() / total_tasks\n",
    "        human_E1_fraction = (group['human_labels'] == 'E1').sum() / total_tasks\n",
    "        human_E2_fraction = (group['human_labels'] == 'E2').sum() / total_tasks\n",
    "        human_aiExposure_fraction = human_E1_fraction + human_E2_fraction\n",
    "\n",
    "        \n",
    "        occupation_stats.append({\n",
    "            f'{onet_occupation_code_var}': soc_code,\n",
    "            f'{onet_occupation_title_var}': occ_title,\n",
    "            'num_tasks': num_tasks,\n",
    "            # 'num_occupations': num_occupations,\n",
    "            'manual_fraction': manual_fraction,\n",
    "            'ai_fraction': ai_fraction,\n",
    "            'augmentation_fraction': augmentation_fraction,\n",
    "            'automation_fraction': automation_fraction,\n",
    "            'gpt4_E0_fraction': gpt4_E0_fraction,\n",
    "            'gpt4_E1_fraction': gpt4_E1_fraction,\n",
    "            'gpt4_E2_fraction': gpt4_E2_fraction,\n",
    "            'gpt4_aiExposure_fraction': gpt4_aiExposure_fraction,\n",
    "            'human_E0_fraction': human_E0_fraction,\n",
    "            'human_E1_fraction': human_E1_fraction,\n",
    "            'human_E2_fraction': human_E2_fraction,\n",
    "            'human_aiExposure_fraction': human_aiExposure_fraction\n",
    "        })\n",
    "\n",
    "    occupation_analysis = pd.DataFrame(occupation_stats)\n",
    "\n",
    "    return occupation_analysis\n",
    "\n",
    "\n",
    "\n",
    "def merge_industry_employment_shares_master(my_sector, my_onet_level,\n",
    "                                            onet_occupation_code_var,\n",
    "                                            weight_cols,\n",
    "                                            occupation_analysis,\n",
    "                                            save=True,\n",
    "                                            bls_file_path=None,\n",
    "                                            post_match_reshuffle_seed=None):\n",
    "\n",
    "\n",
    "    # Merge industry employment share weights for all NAICS sectors and create a master dataset\n",
    "    if not bls_file_path:\n",
    "        bls_sector_shares = pd.read_csv(f'{input_data_path}/computed_objects/BLS_ONET_empShares/BLS{my_sector}_ONET{my_onet_level}_empShares.csv')\n",
    "    else:\n",
    "        bls_sector_shares = pd.read_csv(bls_file_path)\n",
    "\n",
    "    # Ensure NAICS is string for consistent merging\n",
    "    bls_sector_shares['NAICS'] = bls_sector_shares['NAICS'].astype(str)\n",
    "\n",
    "    # Start with occupation_analysis as output_df\n",
    "    output_df = occupation_analysis.copy()\n",
    "\n",
    "    # For each weighting scheme, compute the weights and merge into output_df\n",
    "    # 1) occ_totalEmpShare\n",
    "    if 'occ_totalEmpShare' in weight_cols:\n",
    "        bls_occ_totalEmpShares = bls_sector_shares[['OCC_CODE', 'TOT_EMP']].groupby('OCC_CODE').sum().reset_index()\n",
    "\n",
    "        # Convert to % and change variable name\n",
    "        bls_occ_totalEmpShares['TOT_EMP'] = bls_occ_totalEmpShares['TOT_EMP'] / bls_occ_totalEmpShares['TOT_EMP'].sum()\n",
    "        bls_occ_totalEmpShares = bls_occ_totalEmpShares.rename(columns={'TOT_EMP': 'occ_totalEmpShare'})\n",
    "\n",
    "        # Add weight column to output_df\n",
    "        output_df = output_df.merge(bls_occ_totalEmpShares, left_on=onet_occupation_code_var, right_on=['OCC_CODE'], how='left')\n",
    "\n",
    "    # 2) sectorEmpShare\n",
    "    if 'sectorEmpShare' in weight_cols:\n",
    "        sector_weights_df = bls_sector_shares[['NAICS', 'NAICS_TITLE', 'TOT_EMP']].groupby(['NAICS', 'NAICS_TITLE']).sum('TOT_EMP')\n",
    "\n",
    "        # Convert to % and change variable name\n",
    "        sector_weights_df['TOT_EMP'] = sector_weights_df['TOT_EMP'] / sector_weights_df['TOT_EMP'].sum()\n",
    "        sector_weights_df = sector_weights_df.rename(columns={'TOT_EMP': 'sectorEmpShare'})\n",
    "\n",
    "        # Merge back sector weights to bls dataset to get sector-by-sector occupation data with sector weights\n",
    "        bls_sector_weights_df = bls_sector_shares[['NAICS', 'NAICS_TITLE', 'OCC_CODE', 'OCC_TITLE']].merge(sector_weights_df, on='NAICS', how='left')\n",
    "\n",
    "        \n",
    "        # Aggregate sector weights across occupations\n",
    "        bls_sector_weights_occupation_df = bls_sector_weights_df.groupby('OCC_CODE').sum('sectorEmpShare')\n",
    "\n",
    "        # Add weight column to output_df\n",
    "        output_df = output_df.merge(bls_sector_weights_occupation_df, left_on=onet_occupation_code_var, right_on=['OCC_CODE'], how='left')\n",
    "\n",
    "    # 3) occ_sectorEmpShare\n",
    "    if 'occ_sectorEmpShare' in weight_cols:\n",
    "        within_sector_weights_df = bls_sector_shares[['NAICS', 'NAICS_TITLE', 'OCC_CODE', 'OCC_TITLE', 'TOT_EMP']].copy()\n",
    "        within_sector_weights_df['occ_sectorEmpShare'] = within_sector_weights_df['TOT_EMP'] / within_sector_weights_df.groupby(['NAICS', 'NAICS_TITLE'])['TOT_EMP'].transform('sum')\n",
    "\n",
    "        # Calculate sum over all sectors\n",
    "        within_sector_weights = within_sector_weights_df[['OCC_CODE', 'OCC_TITLE', 'occ_sectorEmpShare']].groupby(['OCC_CODE', 'OCC_TITLE']).sum()\n",
    "\n",
    "        # Add weight column to output_df\n",
    "        output_df = output_df.merge(within_sector_weights, left_on=onet_occupation_code_var, right_on=['OCC_CODE'], how='left')\n",
    "\n",
    "    # Drop the 'OCC_CODE' column\n",
    "    output_df = output_df.drop(columns='OCC_CODE')\n",
    "\n",
    "    # If randomization is requested, reshuffle the weights across occupations\n",
    "    if post_match_reshuffle_seed is not None:\n",
    "        rng = np.random.default_rng(seed=42 + post_match_reshuffle_seed)\n",
    "\n",
    "        # Stack the 3 columns as a 2D array (n_rows x 3)\n",
    "        triples = output_df[weight_cols].to_numpy()\n",
    "\n",
    "        # Permute row order once (i.e., keep the triples together in each row)\n",
    "        shuffled_triples = rng.permutation(triples)\n",
    "\n",
    "        # Assign back to the same columns\n",
    "        output_df[weight_cols] = shuffled_triples\n",
    "    \n",
    "    # Save master dataframe to CSV\n",
    "    if save:\n",
    "        output_df.to_csv(f'{output_data_path}/BLS{my_sector}_ONET{my_onet_level}.csv', index=False)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "def run_weighted_regression(my_sector, my_onet_level,\n",
    "                           master_df, \n",
    "                           dependent_vars,\n",
    "                           weight_cols,\n",
    "                           out_file=None):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for weight_col in weight_cols:\n",
    "        for dependent_var in dependent_vars:\n",
    "            # Data prep\n",
    "            df = master_df[[dependent_var, 'num_tasks', weight_col]].copy()\n",
    "            df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[dependent_var, 'num_tasks', weight_col])\n",
    "\n",
    "            # Coerce numeric\n",
    "            df['num_tasks'] = pd.to_numeric(df['num_tasks'], errors='coerce')\n",
    "            df[dependent_var] = pd.to_numeric(df[dependent_var], errors='coerce')\n",
    "            df[weight_col] = pd.to_numeric(df[weight_col], errors='coerce')\n",
    "\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # Regression\n",
    "            model_wls = smf.wls(f'{dependent_var} ~ num_tasks', data=df, weights=df[weight_col]).fit(cov_type='HC3')\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'BLS_sector_level': my_sector,\n",
    "                'ONET_level': my_onet_level,\n",
    "                'dependent_var': dependent_var,\n",
    "                'weight_col': weight_col,\n",
    "                'model': 'WLS',\n",
    "                'coef_num_tasks': float(model_wls.params.get('num_tasks', np.nan)),\n",
    "                'std_err': float(model_wls.bse.get('num_tasks', np.nan)),\n",
    "                'pvalue': float(model_wls.pvalues.get('num_tasks', np.nan)),\n",
    "                'n_obs': int(model_wls.nobs)\n",
    "            })\n",
    "\n",
    "    reg_df = pd.DataFrame(results)\n",
    "\n",
    "    if not out_file:\n",
    "        reg_out_dir = f\"{output_data_path}/regressions\"\n",
    "        os.makedirs(reg_out_dir, exist_ok=True)\n",
    "        out_file = f\"{reg_out_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks.csv\"\n",
    "        reg_df.to_csv(out_file, index=False)\n",
    "    else:\n",
    "        reg_df.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "                                  master_df,\n",
    "                                  weight_cols,\n",
    "                                  dependent_var='ai_fraction',\n",
    "                                  regressor='human_E1_fraction',\n",
    "                                  out_file=None):\n",
    "    \"\"\"\n",
    "    For each weight column, run WLS: ai_fraction ~ human_E1_fraction + num_tasks,\n",
    "    collect the coefficient on human_E1_fraction, and save all results in one CSV.\n",
    "    Returns the results DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for weight_col in weight_cols:\n",
    "        # Clean sample for this regression\n",
    "        df = master_df[[dependent_var, regressor, 'num_tasks', weight_col]].copy()\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.dropna(subset=[dependent_var, regressor, 'num_tasks', weight_col])\n",
    "\n",
    "        model = smf.wls(\n",
    "            f'{dependent_var} ~ {regressor} + num_tasks',\n",
    "            data=df,\n",
    "            weights=df[weight_col]\n",
    "        ).fit(cov_type='HC3')\n",
    "\n",
    "        results.append({\n",
    "            'BLS_sector_level': my_sector,\n",
    "            'ONET_level': my_onet_level,\n",
    "            'dependent_var': dependent_var,\n",
    "            'weight_col': weight_col,\n",
    "            'model': 'WLS_AIability_on_exposure',\n",
    "            'coef_human_E1_fraction': float(model.params.get(regressor, np.nan)),\n",
    "            'std_err': float(model.bse.get(regressor, np.nan)),\n",
    "            'pvalue': float(model.pvalues.get(regressor, np.nan)),\n",
    "            'n_obs': int(model.nobs),\n",
    "        })\n",
    "\n",
    "    res_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save one CSV with all weight cols\n",
    "    if not out_file:\n",
    "        reg_out_dir = f\"{output_data_path}/regressions\"\n",
    "        os.makedirs(reg_out_dir, exist_ok=True)\n",
    "        out_file = f\"{reg_out_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_AIability_on_exposure.csv\"\n",
    "        res_df.to_csv(out_file, index=False)\n",
    "    else:\n",
    "        res_df.to_csv(out_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb16f4",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "34839c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the merged data\n",
    "merged_data = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/ONET_Eloundou_Anthropic_GPT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4fef8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the supplemental tasks\n",
    "# merged_data = merged_data[merged_data['Task Type'] != 'Supplemental'].reset_index(drop=True)\n",
    "\n",
    "# # Drop rows whose Occupation Title includes 'Teachers, Postsecondary'\n",
    "# merged_data = merged_data[~merged_data['Occupation Title'].str.contains('Teachers, Postsecondary')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "58762f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define levels and variables\n",
    "bls_sector_levels = ['sector']#, '3-digit', '4-digit', '5-digit', '6-digit']\n",
    "\n",
    "onet_levels = ['major', 'minor', 'broad', 'detailed']\n",
    "onet_occupation_code_vars = ['Major_Group_Code', 'Minor_Group_Code', 'Broad_Occupation_Code', 'Detailed_Occupation_Code']\n",
    "onet_occupation_title_vars = ['Major_Group_Title', 'Minor_Group_Title', 'Broad_Occupation_Title', 'Detailed_Occupation_Title']\n",
    "\n",
    "weight_cols = ['occ_totalEmpShare',         # Weight each occupation by occupation's share of total employment (ignoring sector shares)\n",
    "               'sectorEmpShare',            # Weight each occupation by its sector's share of total employment\n",
    "               'occ_sectorEmpShare']        # Weight each occupation by its share of employment within its sector and weight sectors equally  \n",
    "\n",
    "dependent_var_list = ['ai_fraction', 'human_E1_fraction']#, 'human_aiExposure_fraction']#, 'gpt4_E1_fraction']\n",
    "\n",
    "# Reshuffling iterations count\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "96870a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis for each BLS sector level and ONET occupation level\n",
    "# for my_sector in bls_sector_levels:\n",
    "#     for my_onet_level, onet_occupation_code_var, onet_occupation_title_var in zip(onet_levels, onet_occupation_code_vars, onet_occupation_title_vars):\n",
    "#         if my_onet_level != 'detailed':\n",
    "#                     continue\n",
    "        \n",
    "my_sector = 'sector'\n",
    "my_onet_level = 'detailed'\n",
    "onet_occupation_code_var = 'Detailed_Occupation_Code'\n",
    "onet_occupation_title_var = 'Detailed_Occupation_Title'\n",
    "\n",
    "# Get occupation data\n",
    "occupation_analysis = create_occupation_analysis(merged_data, onet_occupation_code_var, onet_occupation_title_var)\n",
    "\n",
    "# Get master dataframe with all weighting schemes merged\n",
    "master_df = merge_industry_employment_shares_master(my_sector, my_onet_level,\n",
    "                                                    onet_occupation_code_var,\n",
    "                                                    weight_cols,\n",
    "                                                    occupation_analysis)\n",
    "\n",
    "# Regress for different dependent vars and weighting schemes\n",
    "run_weighted_regression(my_sector, my_onet_level,\n",
    "                        master_df, \n",
    "                        dependent_var_list,\n",
    "                        weight_cols)\n",
    "\n",
    "# Regress exposure on AI ability\n",
    "regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "                            master_df,\n",
    "                            weight_cols,\n",
    "                            dependent_var='ai_fraction',\n",
    "                            regressor='human_E1_fraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0404f0b",
   "metadata": {},
   "source": [
    "## Placebo Tests: \n",
    "#### 1 and 2) Reshuffle Task-Occupation Assignment and Repeat the Same Analysis\n",
    "#### 3) Reshuffle BLS Weights and Repeat the Same Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7d47ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Use the preserved count reshuffled datasets to run the same regression of AI ability on exposure\n",
    "# path_suffix = 'taskReshuffled_preserveCounts'\n",
    "# regression_output_dir = f\"{output_data_path}/regressions_{path_suffix}\"\n",
    "# os.makedirs(regression_output_dir, exist_ok=True)\n",
    "\n",
    "# for i in range(iterations):\n",
    "#     file_path = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks_iter{i+1}.csv\"\n",
    "#     if os.path.exists(file_path):\n",
    "#         continue  # Skip iteration if file already exists\n",
    "\n",
    "#     if i % 50 == 0:\n",
    "#         print(f\"Iteration {i+1} of {iterations}\")\n",
    "        \n",
    "#     # Load reshuffled data\n",
    "#     merged_df_reshuffled = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/{path_suffix}/ONET_Eloundou_Anthropic_GPT_iter{i+1}.csv\")\n",
    "    \n",
    "#     # Get occupation data\n",
    "#     occupation_analysis = create_occupation_analysis(merged_df_reshuffled, onet_occupation_code_var, onet_occupation_title_var)\n",
    "\n",
    "#     # Get master dataframe with all weighting schemes merged\n",
    "#     master_df = merge_industry_employment_shares_master(my_sector, my_onet_level,\n",
    "#                                                         onet_occupation_code_var,\n",
    "#                                                         weight_cols,\n",
    "#                                                         occupation_analysis,\n",
    "#                                                         save=False)\n",
    "\n",
    "#     # Regress for different dependent vars and weighting schemes\n",
    "#     run_weighted_regression(my_sector, my_onet_level,\n",
    "#                             master_df, \n",
    "#                             dependent_var_list,\n",
    "#                             weight_cols,\n",
    "#                             out_file = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks_iter{i+1}.csv\")\n",
    "\n",
    "#     # Regress exposure on AI ability\n",
    "#     regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "#                                 master_df,\n",
    "#                                 weight_cols,\n",
    "#                                 dependent_var='ai_fraction',\n",
    "#                                 regressor='human_E1_fraction',\n",
    "#                                 out_file = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_AIability_on_exposure_iter{i+1}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "82821607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) Use the changed counts reshuffled datasets to run the same regression of AI ability on exposure\n",
    "# path_suffix = 'taskReshuffled_changeCounts'\n",
    "# regression_output_dir = f\"{output_data_path}/regressions_{path_suffix}\"\n",
    "# os.makedirs(regression_output_dir, exist_ok=True)\n",
    "\n",
    "# for i in range(iterations):\n",
    "#     file_path = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks_iter{i+1}.csv\"\n",
    "#     if os.path.exists(file_path):\n",
    "#         continue  # Skip iteration if file already exists\n",
    "    \n",
    "#     if i % 50 == 0:\n",
    "#         print(f\"Iteration {i+1} of {iterations}\")\n",
    "        \n",
    "#     # Load reshuffled data\n",
    "#     merged_df_reshuffled = pd.read_csv(f\"{input_data_path}/computed_objects/ONET_Eloundou_Anthropic_GPT/{path_suffix}/ONET_Eloundou_Anthropic_GPT_iter{i+1}.csv\")\n",
    "\n",
    "#     # Get occupation data\n",
    "#     occupation_analysis = create_occupation_analysis(merged_df_reshuffled, onet_occupation_code_var, onet_occupation_title_var)\n",
    "    \n",
    "#     # Get master dataframe with all weighting schemes merged\n",
    "#     master_df = merge_industry_employment_shares_master(my_sector, my_onet_level,\n",
    "#                                                         onet_occupation_code_var,\n",
    "#                                                         weight_cols,\n",
    "#                                                         occupation_analysis,\n",
    "#                                                         save=False)\n",
    "\n",
    "#     # Regress for different dependent vars and weighting schemes\n",
    "#     run_weighted_regression(my_sector, my_onet_level,\n",
    "#                             master_df, \n",
    "#                             dependent_var_list,\n",
    "#                             weight_cols,\n",
    "#                             out_file = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks_iter{i+1}.csv\")\n",
    "\n",
    "#     # Regress exposure on AI ability\n",
    "#     regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "#                                 master_df,\n",
    "#                                 weight_cols,\n",
    "#                                 dependent_var='ai_fraction',\n",
    "#                                 regressor='human_E1_fraction',\n",
    "#                                 out_file = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_AIability_on_exposure_iter{i+1}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2de3dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3) Use the original task assignment but reshuffled BLS weights to run the same regression of AI ability on exposure\n",
    "# path_suffix = 'reshuffledBLSweights'\n",
    "# regression_output_dir = f\"{output_data_path}/regressions_{path_suffix}\"\n",
    "# os.makedirs(regression_output_dir, exist_ok=True)\n",
    "\n",
    "# # Get occupation data\n",
    "# occupation_analysis = create_occupation_analysis(merged_data, onet_occupation_code_var, onet_occupation_title_var)\n",
    "\n",
    "# for i in range(iterations):\n",
    "#     file_path = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks_iter{i+1}.csv\"\n",
    "#     if os.path.exists(file_path):\n",
    "#         continue  # Skip iteration if file already exists\n",
    "    \n",
    "#     if i % 50 == 0:\n",
    "#         print(f\"Iteration {i+1} of {iterations}\")\n",
    "\n",
    "#     # Get master dataframe with all weighting schemes merged\n",
    "#     master_df = merge_industry_employment_shares_master(my_sector, my_onet_level,\n",
    "#                                                         onet_occupation_code_var,\n",
    "#                                                         weight_cols,\n",
    "#                                                         occupation_analysis,\n",
    "#                                                         save=False,\n",
    "#                                                         bls_file_path=f\"{input_data_path}/computed_objects/BLS_ONET_empShares/BLS{my_sector}_ONET{my_onet_level}_reshuffledWeights/BLS{my_sector}_ONET{my_onet_level}_empShares_iter{i+1}.csv\")\n",
    "\n",
    "#     # # Get master dataframe with all weighting schemes merged\n",
    "#     # master_df = merge_industry_employment_shares_master(my_sector, my_onet_level,\n",
    "#     #                                                     onet_occupation_code_var,\n",
    "#     #                                                     weight_cols,\n",
    "#     #                                                     occupation_analysis,\n",
    "#     #                                                     save=False,\n",
    "#     #                                                     bls_file_path=None,\n",
    "#     #                                                     post_match_reshuffle_seed=i)\n",
    "\n",
    "#     # Regress for different dependent vars and weighting schemes\n",
    "#     run_weighted_regression(my_sector, my_onet_level,\n",
    "#                             master_df, \n",
    "#                             dependent_var_list,\n",
    "#                             weight_cols,\n",
    "#                             out_file = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks_iter{i+1}.csv\")\n",
    "\n",
    "#     # Regress exposure on AI ability\n",
    "#     regress_exposure_on_AIability(my_sector, my_onet_level,\n",
    "#                                 master_df,\n",
    "#                                 weight_cols,\n",
    "#                                 dependent_var='ai_fraction',\n",
    "#                                 regressor='human_E1_fraction',\n",
    "#                                 out_file = f\"{regression_output_dir}/reg_BLS{my_sector}_ONET{my_onet_level}_AIability_on_exposure_iter{i+1}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75cd52",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "699fd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dependent_var = 'ai_fraction'\n",
    "observed_coef_df = pd.read_csv(\n",
    "    f\"{output_data_path}/regressions/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks.csv\"\n",
    ")\n",
    "\n",
    "task_allocation_type = ['restrictedTaskMatch', 'noRestrictionTaskMatch']\n",
    "bls_weight_type = ['BLSpostMatchReshuffle', 'BLSoriginalReshuffle']\n",
    "\n",
    "weight_cols = ['occ_totalEmpShare', 'sectorEmpShare', 'occ_sectorEmpShare']\n",
    "\n",
    "for tc in itertools.product(task_allocation_type, bls_weight_type):\n",
    "    task_allocation_type, bls_weight_type = tc\n",
    "    plot_suffix = f'{task_allocation_type}_{bls_weight_type}'\n",
    "\n",
    "    reshuffles = [\n",
    "        (f'taskReshuffled_preserveCounts_{task_allocation_type}', 'green', 'Random Task Assignments (Preserve Occupation Task Counts)'),\n",
    "        (f'taskReshuffled_changeCounts_{task_allocation_type}', 'steelblue', 'Random Task Assignments (Change Occupation Task Counts)'),\n",
    "        (f'reshuffledBLSweights_{bls_weight_type}', 'orange', 'Random BLS Weights')\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Preload all reshuffle results once\n",
    "    reshuffle_data = {}\n",
    "    for reshuffle_type, color, label in reshuffles:\n",
    "        regression_output_dir = f\"{output_data_path}/regressions_{reshuffle_type}\"\n",
    "        csv_files = [\n",
    "            os.path.join(regression_output_dir, f)\n",
    "            for f in os.listdir(regression_output_dir)\n",
    "            if \"on_numTasks\" in f and f.endswith(\".csv\")\n",
    "        ]\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSVs found in {regression_output_dir} with 'on_numTasks' in name.\")\n",
    "        reshuffle_data[reshuffle_type] = pd.concat(\n",
    "            [pd.read_csv(p) for p in csv_files],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(reshuffles), ncols=len(weight_cols), figsize=(8*len(weight_cols), 5*len(reshuffles)),\n",
    "        sharex='col',\n",
    "        constrained_layout=True\n",
    "    )\n",
    "\n",
    "    # Precompute column-wise x-lims (shared across rows)\n",
    "    col_xlim = {}\n",
    "    observed_by_col = {}\n",
    "\n",
    "    for c, weight_col in enumerate(weight_cols):\n",
    "        obs_mask = (observed_coef_df['weight_col'] == weight_col) & (observed_coef_df['dependent_var'] == dependent_var)\n",
    "        if not obs_mask.any():\n",
    "            raise ValueError(f\"No observed coef for weight_col={weight_col}, dep={dependent_var} in observed_coef_df.\")\n",
    "        observed_coef = float(observed_coef_df.loc[obs_mask, 'coef_num_tasks'].iloc[0])\n",
    "        observed_by_col[c] = observed_coef\n",
    "\n",
    "        # Use both reshuffle types to define shared x-range\n",
    "        series_list = []\n",
    "        for reshuffle_type, _, _ in reshuffles:\n",
    "            df_all = reshuffle_data[reshuffle_type]\n",
    "            reg_df = df_all[(df_all['weight_col'] == weight_col) & (df_all['dependent_var'] == dependent_var)]\n",
    "            series_list.append(reg_df['coef_num_tasks'].dropna().values)\n",
    "        all_vals = np.concatenate(series_list) if series_list else np.array([0.0])\n",
    "        xmin, xmax = np.nanmin(all_vals), np.nanmax(all_vals)\n",
    "        xmin = np.nanmin([xmin, observed_coef, 0.0])\n",
    "        xmax = np.nanmax([xmax, observed_coef, 0.0])\n",
    "        span = xmax - xmin if xmax > xmin else 1e-6\n",
    "        xmin -= 0.05 * span\n",
    "        xmax += 0.05 * span\n",
    "        col_xlim[c] = (xmin, xmax)\n",
    "\n",
    "    # Plot\n",
    "    for r, (reshuffle_type, color, row_label) in enumerate(reshuffles):\n",
    "        for c, weight_col in enumerate(weight_cols):\n",
    "            ax = axes[r, c]\n",
    "            df_all = reshuffle_data[reshuffle_type]\n",
    "            reg_df = df_all[(df_all['weight_col'] == weight_col) & (df_all['dependent_var'] == dependent_var)]\n",
    "            vals = reg_df['coef_num_tasks'].dropna().values\n",
    "\n",
    "            # compute bins for THIS subplot over the shared column x-range\n",
    "            # (more bins when the range is wide vs. the data's IQR; capped to stay sensible)\n",
    "            # if len(vals) >= 2:\n",
    "            #     bw = np.histogram_bin_width(vals, bins='fd')  # Freedman–Diaconis\n",
    "            # else:\n",
    "            #     bw = None\n",
    "\n",
    "            # xmin, xmax = col_xlim[c]  # keep shared x-range per column\n",
    "            # span = xmax - xmin if xmax > xmin else 1e-12\n",
    "            bw = 2.0 * np.std(vals) * (len(vals) ** (-1/3)) if len(vals) >= 2 else None  # Scott's rule\n",
    "\n",
    "            if (bw is None) or (not np.isfinite(bw)) or (bw <= 0):\n",
    "                n_bins = 30  # fallback\n",
    "            else:\n",
    "                n_bins = int(np.clip(np.round(span / bw), 20, 80))  # cap between 20 and 80\n",
    "\n",
    "            bins = np.linspace(xmin, xmax, n_bins + 1)\n",
    "        \n",
    "\n",
    "            xmin, xmax = col_xlim[c]\n",
    "            observed_coef = observed_by_col[c]\n",
    "            below_pct = float(np.mean(vals < observed_coef) * 100) if len(vals) else float('nan')\n",
    "\n",
    "            ax.hist(vals, bins=bins, color=color, edgecolor='black', label=f'{row_label}')\n",
    "            ax.axvline(observed_coef, color='red', linestyle='--', linewidth=1.8,\n",
    "                    label=f'Observed (below {below_pct:.1f}%)')\n",
    "            ax.axvline(0, color='black', linestyle=':', linewidth=1.5)\n",
    "\n",
    "            if r == 0:\n",
    "                ax.set_title(f'{weight_col}\\n')\n",
    "            if c == 0:\n",
    "                ax.set_ylabel(\"Frequency\", fontsize=10)\n",
    "            if r == len(reshuffles) - 1:\n",
    "                ax.set_xlabel('(num_tasks) Regression Coefficient')\n",
    "\n",
    "            # ax.set_xlim(xmin, xmax)\n",
    "            ax.legend(loc='best', frameon=False)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Histogram of Regression Coefficients of ({dependent_var}) on (num_tasks) for 1000 Randomizations\\n\\n\"\n",
    "        f\"BLS {my_sector}, ONET {my_onet_level}\\n\",\n",
    "        fontsize=12\n",
    "    )\n",
    "    plt.savefig(f\"{output_plot_path}/regression_coefficients_numTasks_{dependent_var}_BLS{my_sector}_ONET{my_onet_level}_{plot_suffix}.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "0996b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dependent_var = 'ai_fraction'\n",
    "observed_coef_df = pd.read_csv(\n",
    "    f\"{output_data_path}/regressions/reg_BLS{my_sector}_ONET{my_onet_level}_on_numTasks.csv\"\n",
    ")\n",
    "\n",
    "task_allocation_opts = ['restrictedTaskMatch', 'noRestrictionTaskMatch']  # affects GREEN/BLUE only\n",
    "bls_weight_opts = ['BLSpostMatchReshuffle', 'BLSoriginalReshuffle']       # affects ORANGE only\n",
    "\n",
    "weight_cols = ['occ_totalEmpShare', 'sectorEmpShare', 'occ_sectorEmpShare']\n",
    "\n",
    "def load_reshuffle_df(reshuffle_type: str) -> pd.DataFrame:\n",
    "    regression_output_dir = f\"{output_data_path}/regressions_{reshuffle_type}\"\n",
    "    csv_files = [\n",
    "        os.path.join(regression_output_dir, f)\n",
    "        for f in os.listdir(regression_output_dir)\n",
    "        if \"on_numTasks\" in f and f.endswith(\".csv\")\n",
    "    ]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSVs found in {regression_output_dir} with 'on_numTasks' in name.\")\n",
    "    return pd.concat([pd.read_csv(p) for p in csv_files], ignore_index=True)\n",
    "\n",
    "def plot_row(df_all: pd.DataFrame, color: str, row_label: str, suffix: str, reshuffle_type: str):\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=1, ncols=len(weight_cols),\n",
    "        figsize=(6*len(weight_cols), 6),\n",
    "        constrained_layout=True\n",
    "    )\n",
    "    if len(weight_cols) == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    # observed coefs per column\n",
    "    observed_by_col = {}\n",
    "    for c, weight_col in enumerate(weight_cols):\n",
    "        obs_mask = (\n",
    "            (observed_coef_df['weight_col'] == weight_col) &\n",
    "            (observed_coef_df['dependent_var'] == dependent_var)\n",
    "        )\n",
    "        observed_by_col[c] = float(observed_coef_df.loc[obs_mask, 'coef_num_tasks'].iloc[0]) if obs_mask.any() else np.nan\n",
    "\n",
    "    for c, weight_col in enumerate(weight_cols):\n",
    "        ax = axes[c]\n",
    "        reg_df = df_all[\n",
    "            (df_all['weight_col'] == weight_col) &\n",
    "            (df_all['dependent_var'] == dependent_var)\n",
    "        ]\n",
    "        vals = reg_df['coef_num_tasks'].dropna().values\n",
    "        if vals.size == 0:\n",
    "            ax.axis('off')\n",
    "            ax.text(0.5, 0.5, f'No reshuffle data\\n({row_label} • {weight_col})',\n",
    "                    ha='center', va='center')\n",
    "            continue\n",
    "\n",
    "        observed_coef = observed_by_col[c]\n",
    "        # per-subplot x-range (include observed & 0)\n",
    "        local_min, local_max = np.nanmin(vals), np.nanmax(vals)\n",
    "        xmin = np.nanmin([local_min, observed_coef, 0.0])\n",
    "        xmax = np.nanmax([local_max, observed_coef, 0.0])\n",
    "        span = xmax - xmin if xmax > xmin else 1e-6\n",
    "        pad = 0.05 * span\n",
    "        xmin -= pad; xmax += pad\n",
    "\n",
    "        # Scott’s rule for bins (cap)\n",
    "        if vals.size >= 2:\n",
    "            bw = 2.0 * np.std(vals, ddof=1) * (len(vals) ** (-1/3))\n",
    "        else:\n",
    "            bw = None\n",
    "        n_bins = int(np.clip(np.round((xmax - xmin) / bw), 20, 80)) if (bw and np.isfinite(bw) and bw > 0) else 30\n",
    "        bins = np.linspace(xmin, xmax, n_bins + 1)\n",
    "\n",
    "        below_pct = float(np.mean(vals < observed_coef) * 100) if np.isfinite(observed_coef) else np.nan\n",
    "\n",
    "        ax.hist(vals, bins=bins, color=color, edgecolor='black', label='Randomized')\n",
    "        if np.isfinite(observed_coef):\n",
    "            ax.axvline(observed_coef, color='red', linestyle='--', linewidth=1.8,\n",
    "                       label=f'Observed (below {below_pct:.1f}%)')\n",
    "        ax.axvline(0, color='black', linestyle=':', linewidth=1.5)\n",
    "\n",
    "        ax.set_title(weight_col, fontsize=11)\n",
    "        ax.set_xlabel('(num_tasks) Regression Coefficient', fontsize=10)\n",
    "        if c == 0:\n",
    "            ax.set_ylabel('Frequency', fontsize=10)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.legend(loc='best', frameon=False)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Histogram of Regression Coefficients of ({dependent_var}) on (num_tasks)\\n\\n\"\n",
    "        f\"--BLS {my_sector}, ONET {my_onet_level}--\\n\\n\"\n",
    "        f\"{row_label}\\n\\n\"\n",
    "        f\"[{suffix}]\\n\\n\",\n",
    "        fontsize=12\n",
    "    )\n",
    "    out_path = (\n",
    "        f\"{output_plot_path}/regcoef_numTasks_{dependent_var}_BLS{my_sector}\"\n",
    "        f\"_ONET{my_onet_level}_{suffix}_{reshuffle_type}.png\"\n",
    "    )\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# -------- 1) TASK-RESHUFFLE FIGURES (green/blue) — VARY ONLY BY task_allocation --------\n",
    "for task_alloc in task_allocation_opts:\n",
    "    reshuffles_task = [\n",
    "        (f'taskReshuffled_preserveCounts_{task_alloc}', 'green',\n",
    "         'Fixed BLS Weights, Random Task Assignments (Preserve Occupation Task Counts)'),\n",
    "        (f'taskReshuffled_changeCounts_{task_alloc}', 'steelblue',\n",
    "         'Fixed BLS Weights, Random Task Assignments (Change Occupation Task Counts)'),\n",
    "    ]\n",
    "    for reshuffle_type, color, row_label in reshuffles_task:\n",
    "        df_all = load_reshuffle_df(reshuffle_type)\n",
    "        # suffix only reflects task_alloc to avoid duplicate outputs\n",
    "        plot_row(df_all, color, row_label, suffix=task_alloc, reshuffle_type=reshuffle_type)\n",
    "\n",
    "# -------- 2) BLS-WEIGHT-RESHUFFLE FIGURES (orange) — VARY ONLY BY bls_weight --------\n",
    "for bls_weight in bls_weight_opts:\n",
    "    reshuffle_type = f'reshuffledBLSweights_{bls_weight}'\n",
    "    row_label = 'Random BLS Weights, Fixed Task Assignments'\n",
    "    color = 'orange'\n",
    "    df_all = load_reshuffle_df(reshuffle_type)\n",
    "    # suffix only reflects bls_weight to avoid duplicate outputs\n",
    "    plot_row(df_all, color, row_label, suffix=bls_weight, reshuffle_type=reshuffle_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
