{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7fc397",
   "metadata": {},
   "source": [
    "#### By: Peyman Shahidi\n",
    "#### Created: Oct 14, 2025\n",
    "#### Last Edit: Oct 22, 2025\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10366af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random \n",
    "\n",
    "## formatting number to appear comma separated and with two digits after decimal: e.g, 1000 shown as 1,000.00\n",
    "pd.set_option('float_format', \"{:,.2f}\".format)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#from matplotlib.legend import Legend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97af73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "# Install caffeinate package\n",
    "%pip install caffeinate\n",
    "\n",
    "# Use macOS built-in caffeinate command for reliability\n",
    "# This prevents the system from sleeping while the process is running\n",
    "try:\n",
    "    # Start caffeinate in the background\n",
    "    caff_process = subprocess.Popen(['caffeinate', '-d'], \n",
    "                                   stdout=subprocess.DEVNULL, \n",
    "                                   stderr=subprocess.DEVNULL)\n",
    "    print(f\"Caffeinate mode ON ☕ – Device will stay awake (PID: {caff_process.pid})\")\n",
    "    print(\"System sleep is disabled while this process runs.\")\n",
    "    \n",
    "    # Store the process ID for later cleanup\n",
    "    caff_pid = caff_process.pid\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not start caffeinate: {e}\")\n",
    "    print(\"Continuing without caffeinate - system may sleep during long processes.\")\n",
    "    caff_process = None\n",
    "    caff_pid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = \"..\"\n",
    "input_data_path = f\"{main_folder_path}/data\"\n",
    "output_data_path = f'{input_data_path}/computed_objects'\n",
    "output_plot_path = f\"{main_folder_path}/writeup/plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "\n",
    "for path in [output_data_path, output_plot_path]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252758cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsl import QuestionFreeText, Scenario, Model, Survey\n",
    "from textwrap import dedent\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_task_sequence(occupation, tasks_data, output_data_path):\n",
    "    \"\"\"\n",
    "    Extract task sequence for an occupation using EDSL workflow.\n",
    "    Returns the ordered sequence of tasks.\n",
    "    \"\"\"\n",
    "    # Check if output file already exists\n",
    "    safe_title = occupation.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    output_folder = f'{output_data_path}/tasks_sequences'\n",
    "    output_file = os.path.join(output_folder, f\"{safe_title}.csv\")\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        return output_file, True  # Return file path and flag indicating it already existed\n",
    "    \n",
    "    # Check if we have tasks for this occupation\n",
    "    if tasks_data.empty:\n",
    "        print(f\"⚠️  Warning: No tasks found for occupation '{occupation}' - skipping\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "    \n",
    "    # Create task mappings\n",
    "    task_id_mapping = dict(zip(tasks_data['Task Title'], tasks_data['Task ID']))\n",
    "    soc_code_mapping = dict(zip(tasks_data['Task Title'], tasks_data['O*NET-SOC Code']))\n",
    "    \n",
    "    # Format tasks as numbered list\n",
    "    tasks_list = tasks_data['Task Title'].tolist()\n",
    "    tasks_text = \"\\n\".join([f\"{i}. {task}\" for i, task in enumerate(tasks_list, 1)])\n",
    "    num_tasks = len(tasks_list)\n",
    "    max_tokens = 32000\n",
    "    \n",
    "    print(f\"   • {num_tasks} tasks, using {max_tokens} max tokens\")\n",
    "\n",
    "    # Create scenario\n",
    "    scenario = Scenario({\n",
    "        \"occupation\": occupation,\n",
    "        \"tasks_list\": tasks_text,\n",
    "        \"num_tasks\": num_tasks\n",
    "    })\n",
    "\n",
    "    # Create question for task sequencing\n",
    "    q_sequence = QuestionFreeText(\n",
    "        question_name=\"task_sequence\",\n",
    "        question_text=dedent(\"\"\"\\\n",
    "            You are an expert in workflow analysis for the occupation: {{ occupation }}.\n",
    "            Below is a list of {{ num_tasks }} tasks that are part of this occupation:\n",
    "            {{ tasks_list }}\n",
    "            Provide the typical sequential order in which these tasks are performed in a real-world workflow.\n",
    "            Return your answer as a JSON array where each element has:\n",
    "            - \"Task Position\": the sequence number (1, 2, 3, etc.)\n",
    "            - \"Task Title\": the exact task text from the list above\n",
    "            Format: [{\"Task Position\": 1, \"Task Title\": \"...\"}, {\"Task Position\": 2, \"Task Title\": \"...\"}, ...]\n",
    "            Only return the JSON array, nothing else.\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create model using openai_v2 for reasoning capabilities\n",
    "        model = Model(\"gpt-5-mini\", service_name=\"openai_v2\", temperature=0.0, max_tokens=max_tokens)\n",
    "        \n",
    "        # Run sequence question\n",
    "        sequence_results = q_sequence.by(model).by([scenario]).run(progress_bar=False)\n",
    "        sequence_df = sequence_results.to_pandas()\n",
    "        sequence_json = sequence_df['answer.task_sequence'][0]\n",
    "        \n",
    "        # Debug: Print the raw response before cleaning\n",
    "        print(f\"   • Raw JSON length: {len(str(sequence_json))}\")\n",
    "        print(f\"   • Raw JSON preview: {str(sequence_json)[:50]}...\")\n",
    "        \n",
    "        # Clean the JSON response by removing markdown code blocks if present\n",
    "        if isinstance(sequence_json, str):\n",
    "            # Simple string replacement approach\n",
    "            cleaned_json = sequence_json\n",
    "            if '```json' in cleaned_json:\n",
    "                cleaned_json = cleaned_json.replace('```json', '')\n",
    "            if '```' in cleaned_json:\n",
    "                cleaned_json = cleaned_json.replace('```', '')\n",
    "            sequence_json = cleaned_json.strip()\n",
    "            print(f\"   • Cleaned JSON preview: {sequence_json[:50]}...\")\n",
    "        \n",
    "        # Check if the response is valid\n",
    "        if pd.isna(sequence_json) or not isinstance(sequence_json, str):\n",
    "            print(f\"❌ Error: Invalid response for '{occupation}' - got {type(sequence_json)} instead of string\")\n",
    "            return None, True  # Treat as already processed to skip\n",
    "        \n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            sequence_data = json.loads(sequence_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ JSON parsing failed, trying to clean response further...\")\n",
    "            print(f\"   Original error: {e}\")\n",
    "            print(f\"   Response starts with: {sequence_json[:100]}...\")\n",
    "            # Try additional cleanup\n",
    "            if sequence_json.startswith('```'):\n",
    "                lines = sequence_json.split('\\n')\n",
    "                if lines[0].strip() in ['```', '```json']:\n",
    "                    lines = lines[1:]  # Remove first line\n",
    "                if lines[-1].strip() == '```':\n",
    "                    lines = lines[:-1]  # Remove last line\n",
    "                sequence_json = '\\n'.join(lines).strip()\n",
    "                print(f\"   Cleaned response starts with: {sequence_json[:100]}...\")\n",
    "                try:\n",
    "                    sequence_data = json.loads(sequence_json)\n",
    "                    print(f\"   ✅ Successfully parsed after additional cleanup\")\n",
    "                except json.JSONDecodeError as e2:\n",
    "                    print(f\"   ❌ Still failed after cleanup: {e2}\")\n",
    "                    raise e  # Re-raise original error\n",
    "            else:\n",
    "                raise e  # Re-raise original error\n",
    "                \n",
    "        ordered_sequence_df = pd.DataFrame(sequence_data)\n",
    "        \n",
    "        # Add metadata columns\n",
    "        ordered_sequence_df['Occupation Title'] = occupation\n",
    "        ordered_sequence_df['Task ID'] = ordered_sequence_df['Task Title'].map(task_id_mapping)\n",
    "        ordered_sequence_df['O*NET-SOC Code'] = ordered_sequence_df['Task Title'].map(soc_code_mapping)\n",
    "        \n",
    "        # Reorder columns\n",
    "        ordered_sequence_df = ordered_sequence_df[['Task Position', 'Task Title', 'Task ID', 'O*NET-SOC Code', 'Occupation Title']]\n",
    "\n",
    "        # Save to file\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        ordered_sequence_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"   ✅ Successfully processed and saved task sequence\")\n",
    "        return output_file, False  # Return file path and flag indicating it was newly created\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ JSON Error for '{occupation}': {e}\")\n",
    "        print(f\"   Raw response: {sequence_json}\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error for '{occupation}': {e}\")\n",
    "        return None, True  # Treat as already processed to skip\n",
    "\n",
    "\n",
    "# NEW: extract sequence of occupations within a minor SOC group\n",
    "def extract_occupation_sequence(minor_group_title, occupations_df, output_data_path):\n",
    "    \"\"\"\n",
    "    Given a minor SOC group title (e.g., 'Chief Executives') and a DataFrame of occupations within it (with columns\n",
    "    'Occupation Title' and 'O*NET-SOC Code'), query the model to provide a typical sequence of occupations\n",
    "    (e.g., which occupations hand off work to which) within that minor group. Saves a CSV per minor group.\n",
    "\n",
    "    Note: this prompt intentionally does NOT ask for O*NET-SOC Codes to avoid extra token costs; the CSV\n",
    "    will contain Position and Occupation Title only. If you want to add SOC codes later, you can merge\n",
    "    them locally from the ONET dataset.\n",
    "    \"\"\"\n",
    "    safe_title = minor_group_title.replace(',', '')\n",
    "    safe_title = safe_title.replace(' ', '_')\n",
    "    output_folder = f'{output_data_path}/occupation_sequences'\n",
    "    output_file = os.path.join(output_folder, f\"{safe_title}.csv\")\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        return output_file, True\n",
    "\n",
    "    if occupations_df.empty:\n",
    "        print(f\"⚠️  No occupations found for minor group '{minor_group_title}' - skipping\")\n",
    "        return None, True\n",
    "\n",
    "    # Create task mappings\n",
    "    minor_group_mapping = dict(zip(occupations_df['Minor_Group_Title'], occupations_df['Minor_Group_Code']))\n",
    "    occupation_code_mapping = dict(zip(occupations_df['Occupation Title'], occupations_df['O*NET-SOC Code']))\n",
    "    \n",
    "    # build occupations list text\n",
    "    occ_list = occupations_df[['Occupation Title']].drop_duplicates().reset_index(drop=True)\n",
    "    occ_text = '\\n'.join([f\"{i}. {row['Occupation Title']}\" for i, row in occ_list.iterrows()])\n",
    "    num_occs = len(occ_list)\n",
    "\n",
    "    print(f\"   • {num_occs} occupations in minor group {minor_group_title}\")\n",
    "\n",
    "    scenario = Scenario({\n",
    "        'minor_group': minor_group_title,\n",
    "        'occupations_list': occ_text,\n",
    "        'num_occs': num_occs\n",
    "    })\n",
    "\n",
    "    q_occ_sequence = QuestionFreeText(\n",
    "        question_name='occupation_sequence',\n",
    "        question_text=dedent(\"\"\"\\\n",
    "            You are an expert on occupational workflows. Below is a list of occupations within the same minor SOC group ({{ minor_group }}):\n",
    "            {{ occupations_list }}\n",
    "            Provide the typical sequential order in which these occupations interact or hand off work in a multi-step workflow that involves multiple occupations within this group.\n",
    "            Return your answer as a JSON array where each element has:\n",
    "            - \"Occupation Position\": the sequence number (1, 2, 3, ...)\n",
    "            - \"Occupation Title\": the exact occupation title from the list above\n",
    "            Format: [{\"Occupation Position\": 1, \"Occupation Title\": \"...\"}, ...]\n",
    "            Only return the JSON array, nothing else.\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model = Model(\"gpt-5-mini\", service_name=\"openai_v2\", temperature=0.0, max_tokens=16000)\n",
    "        res = q_occ_sequence.by(model).by([scenario]).run(progress_bar=False)\n",
    "        res_df = res.to_pandas()\n",
    "        res_json = res_df['answer.occupation_sequence'][0]\n",
    "\n",
    "        if isinstance(res_json, str):\n",
    "            cleaned = res_json\n",
    "            if '```json' in cleaned:\n",
    "                cleaned = cleaned.replace('```json', '')\n",
    "            if '```' in cleaned:\n",
    "                cleaned = cleaned.replace('```', '')\n",
    "            res_json = cleaned.strip()\n",
    "\n",
    "        seq_data = json.loads(res_json)\n",
    "        seq_df = pd.DataFrame(seq_data)\n",
    "\n",
    "        # Keep only Occupation Position and Occupation Title to save tokens; user can merge SOC codes locally later\n",
    "        if 'Occupation Position' not in seq_df.columns or 'Occupation Title' not in seq_df.columns:\n",
    "            print('⚠️  Unexpected response format; saving raw parsed dataframe for inspection')\n",
    "\n",
    "        # Add metadata columns\n",
    "        seq_df['Minor_Group_Title'] = minor_group_title\n",
    "        seq_df['Minor_Group_Code'] = seq_df['Minor_Group_Title'].map(minor_group_mapping)\n",
    "        seq_df['O*NET-SOC Code'] = seq_df['Occupation Title'].map(occupation_code_mapping)\n",
    "        \n",
    "        # Reorder columns\n",
    "        seq_df = seq_df[['Occupation Position', 'Occupation Title', 'O*NET-SOC Code', 'Minor_Group_Code', 'Minor_Group_Code']]\n",
    "\n",
    "        # Save output\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        seq_df.to_csv(output_file, index=False)\n",
    "        print(f\"   ✅ Saved occupation sequence for {minor_group_title} -> {output_file}\")\n",
    "        return output_file, False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to extract occupation sequence for {minor_group_title}: {e}\")\n",
    "        return None, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fe567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load O*NET data and extract unique occupation titles\n",
    "# ONET = pd.read_csv(f'{output_data_path}/ONET_cleaned_tasks.csv')\n",
    "\n",
    "# # Get all unique occupation titles from the dataset\n",
    "# occupations_list = sorted(ONET['Occupation Title'].unique().tolist())\n",
    "# print(f\"Found {len(occupations_list)} unique occupations in the dataset:\")\n",
    "\n",
    "# # Set seed for reproducible random sampling\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # # Randomly sample 10% of occupations\n",
    "# # sample_size = max(1, int(len(occupations_list) * 0.10))  # Ensure at least 1 occupation\n",
    "# # sampled_occupations = random.sample(occupations_list, sample_size)\n",
    "# # print(f\"Randomly selected {len(sampled_occupations)} occupations (5% of total) for processing:\")\n",
    "# # print(f\"Sample: {sampled_occupations[:5]}...\" if len(sampled_occupations) > 5 else f\"Sample: {sampled_occupations}\")\n",
    "# sampled_occupations = occupations_list\n",
    "\n",
    "# # Process each occupation\n",
    "# processed_count = 0\n",
    "# skipped_count = 0\n",
    "# error_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# for i, occupation in enumerate(sampled_occupations, 1):\n",
    "#     # Filter data for this occupation\n",
    "#     occupation_data = ONET[ONET['Occupation Title'] == occupation].copy()\n",
    "    \n",
    "#     # Prepare task data\n",
    "#     occupation_task_data = occupation_data[['Task ID', 'Task Title', 'O*NET-SOC Code']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "#     # Enhanced progress output\n",
    "#     num_tasks = len(occupation_task_data)\n",
    "#     print(f\"\\n[{i}/{len(sampled_occupations)}] {occupation}\")\n",
    "    \n",
    "#     # Extract task sequence\n",
    "#     output_file, already_existed = extract_task_sequence(occupation, occupation_task_data, output_data_path)\n",
    "    \n",
    "#     if output_file is None:\n",
    "#         error_count += 1\n",
    "#     elif already_existed:\n",
    "#         print(f\"   ⏭️  Already exists - skipping\")\n",
    "#         skipped_count += 1\n",
    "#     else:\n",
    "#         processed_count += 1\n",
    "\n",
    "# # Summary\n",
    "# print(f\"\\n\" + \"=\"*50)\n",
    "# print(f\"PROCESSING COMPLETE\")\n",
    "# print(f\"=\"*50)\n",
    "# print(f\"• {processed_count} occupations processed\")\n",
    "# print(f\"• {skipped_count} occupations skipped (already existed)\")\n",
    "# print(f\"• {error_count} occupations failed\")\n",
    "# print(f\"• {len(sampled_occupations)} total occupations in sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load O*NET data and extract unique occupations and minor groupings\n",
    "ONET = pd.read_csv(f'{output_data_path}/ONET_cleaned_tasks.csv')\n",
    "\n",
    "# Get unique minor groups\n",
    "minor_groups = ONET['Minor_Group_Title'].unique().tolist()\n",
    "minor_groups = [x for x in minor_groups if pd.notna(x)] # Drop \"nan\" instances\n",
    "print(f\"Found {len(minor_groups)} minor SOC groups in the dataset:\")\n",
    "\n",
    "# Process each minor group\n",
    "processed = 0\n",
    "skipped = 0\n",
    "errors = 0\n",
    "\n",
    "for i, minor in enumerate(minor_groups, 1):\n",
    "    occ_df = ONET[ONET['Minor_Group_Title'] == minor][['Occupation Title','O*NET-SOC Code', 'Minor_Group_Code', 'Minor_Group_Title']].drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"\\n[{i}/{len(minor_groups)}] Minor group: {minor} (occupations: {len(occ_df)})\")\n",
    "    out_file, existed = extract_occupation_sequence(minor, occ_df, output_data_path)\n",
    "    if out_file is None:\n",
    "        errors += 1\n",
    "    elif existed:\n",
    "        print('   ⏭️ Already exists - skipping')\n",
    "        skipped += 1\n",
    "    else:\n",
    "        processed += 1\n",
    "\n",
    "# Summary\n",
    "print('\\n' + '='*50)\n",
    "print('PROCESSING COMPLETE')\n",
    "print('='*50)\n",
    "print(f'• {processed} minor groups processed')\n",
    "print(f'• {skipped} minor groups skipped (already existed)')\n",
    "print(f'• {errors} minor groups failed')\n",
    "print(f'• {len(minor_groups)} total minor groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee389c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up caffeinate process\n",
    "try:\n",
    "    if 'caff_process' in globals() and caff_process is not None:\n",
    "        caff_process.terminate()\n",
    "        caff_process.wait()  # Wait for process to terminate\n",
    "        print(\"Caffeinate mode OFF 💡 - System sleep is now enabled.\")\n",
    "    else:\n",
    "        print(\"Caffeinate was not running or already stopped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Caffeinate process may have already ended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
