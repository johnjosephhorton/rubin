{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)\n",
    "\n",
    "with open('functions.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c01952",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('all')\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymansh':\n",
    "    main_folder_path = '/Users/peymansh/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9747ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "onet_data_path = f'{data_path}/data/onet_occupations_yearly.csv'\n",
    "\n",
    "# list of occupations to create DAGs for\n",
    "occupation_list = ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators', \n",
    "                   'dredgeOperators', 'gradersAndSortersForAgriculturalProducts', 'reinforcingIronAndRebarWorkers',\n",
    "                   'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists', \n",
    "                   'personalCareAides', 'proofreadersAndCopyMarkers', 'chiropractors', \n",
    "                   'shippingReceivingAndInventoryClerks', 'cooksShortOrder', 'orthodontists',\n",
    "                   'subwayAndStreetcarOperators', 'packersAndPackagersHand', 'hoistAndWinchOperators', \n",
    "                   'forgingMachineSettersOperatorsAndTenders', 'avionicsTechnicians', 'dishwashers', \n",
    "                   'dispatchersExceptPoliceFireAndAmbulance', 'familyMedicinePhysicians', 'MachineFeedersAndOffbearers'\n",
    "                   ]\n",
    "\n",
    "occupation = 'travelAgents'\n",
    "occupation = 'insuranceUnderwriters'\n",
    "occupation = 'pileDriverOperators'\n",
    "\n",
    "# Generate occupation-specific strings\n",
    "GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bb531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha as AI quality metric\n",
    "n = 100\n",
    "epsilon = 1e-8\n",
    "alpha_list = np.linspace(epsilon, 1-epsilon, n).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40506e",
   "metadata": {},
   "source": [
    "### Initialize input-output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200fa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_M_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_M.csv'\n",
    "\n",
    "# First Last Task DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_FLT_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_FLT.csv'\n",
    "\n",
    "# Conditioned First Last Task DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_CFLT_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_CFLT.csv'\n",
    "\n",
    "# Partitioned DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_P_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_P.csv'\n",
    "\n",
    "# Conditioned Partitioned DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_CP_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_CP.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd525be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Move hand and foot levers of hoisting equipmen...</td>\n",
       "      <td>Drive pilings to provide support for buildings...</td>\n",
       "      <td>The worker driving the pilings needs to know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drive pilings to provide support for buildings...</td>\n",
       "      <td>Move levers and turn valves to activate power ...</td>\n",
       "      <td>The worker operating the levers and valves to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Clean, lubricate, and refill equipment.</td>\n",
       "      <td>The worker responsible for cleaning, lubricati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Move hand and foot levers of hoisting equipmen...</td>\n",
       "      <td>The worker moving hand and foot levers to posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clean, lubricate, and refill equipment.</td>\n",
       "      <td>Move hand and foot levers of hoisting equipmen...</td>\n",
       "      <td>The worker operating the hoisting equipment ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Drive pilings to provide support for buildings...</td>\n",
       "      <td>The worker driving pilings needs to know that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Move levers and turn valves to activate power ...</td>\n",
       "      <td>The worker who is moving levers and turning va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Clean, lubricate, and refill equipment.</td>\n",
       "      <td>\"Target\"</td>\n",
       "      <td>Job Completion Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Move levers and turn valves to activate power ...</td>\n",
       "      <td>\"Target\"</td>\n",
       "      <td>Job Completion Indicator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source  \\\n",
       "0   Move hand and foot levers of hoisting equipmen...   \n",
       "2   Drive pilings to provide support for buildings...   \n",
       "3   Conduct pre-operational checks on equipment to...   \n",
       "4   Conduct pre-operational checks on equipment to...   \n",
       "5             Clean, lubricate, and refill equipment.   \n",
       "6   Conduct pre-operational checks on equipment to...   \n",
       "8   Conduct pre-operational checks on equipment to...   \n",
       "10            Clean, lubricate, and refill equipment.   \n",
       "11  Move levers and turn valves to activate power ...   \n",
       "\n",
       "                                               target  \\\n",
       "0   Drive pilings to provide support for buildings...   \n",
       "2   Move levers and turn valves to activate power ...   \n",
       "3             Clean, lubricate, and refill equipment.   \n",
       "4   Move hand and foot levers of hoisting equipmen...   \n",
       "5   Move hand and foot levers of hoisting equipmen...   \n",
       "6   Drive pilings to provide support for buildings...   \n",
       "8   Move levers and turn valves to activate power ...   \n",
       "10                                           \"Target\"   \n",
       "11                                           \"Target\"   \n",
       "\n",
       "                                              comment  \n",
       "0   The worker driving the pilings needs to know t...  \n",
       "2   The worker operating the levers and valves to ...  \n",
       "3   The worker responsible for cleaning, lubricati...  \n",
       "4   The worker moving hand and foot levers to posi...  \n",
       "5   The worker operating the hoisting equipment ne...  \n",
       "6   The worker driving pilings needs to know that ...  \n",
       "8   The worker who is moving levers and turning va...  \n",
       "10                           Job Completion Indicator  \n",
       "11                           Job Completion Indicator  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read DAG\n",
    "dag_df = pd.read_csv(input_path)\n",
    "\n",
    "# remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "if 'comment' in dag_df.columns:\n",
    "    dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "# get task stats\n",
    "tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "tasks_stats\n",
    "\n",
    "# print stats\n",
    "#tasks_stats.iloc[:,1:].sum()\n",
    "dag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f86c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Move hand and foot levers of hoisting equipment to position piling leads, hoist piling into leads, and position hammers over pilings.',\n",
       " 1: 'Conduct pre-operational checks on equipment to ensure proper functioning.',\n",
       " 2: 'Drive pilings to provide support for buildings or other structures, using heavy equipment with a pile driver head.',\n",
       " 3: 'Move levers and turn valves to activate power hammers, or to raise and lower drophammers that drive piles to required depths.',\n",
       " 4: 'Clean, lubricate, and refill equipment.',\n",
       " 5: '\"Target\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract list of tasks and create a dictionary for indexing tasks\n",
    "tasks_list = tasks_stats['task'].unique()\n",
    "tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "# create numpy array of adjacency matrix\n",
    "adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "for _, row in dag_df.iterrows():\n",
    "    source_index = aux_dict[row['source']]\n",
    "    target_index = aux_dict[row['target']]\n",
    "    adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "tasks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa449c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(adjacency_matrix):\n",
    "    n = adjacency_matrix.shape[0]  # Number of nodes\n",
    "    neighbors_dict = {}\n",
    "\n",
    "    for node in range(n):\n",
    "        neighbors = []\n",
    "        for neighbor in range(n):\n",
    "            if adjacency_matrix[node, neighbor] != 0:  # Check for an edge from node to neighbor\n",
    "                neighbors.append(neighbor)\n",
    "        neighbors_dict[node] = neighbors\n",
    "\n",
    "    return neighbors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8decea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors_undirected(adjacency_matrix):\n",
    "    n = adjacency_matrix.shape[0]  # Number of nodes\n",
    "    neighbors_dict = {}\n",
    "\n",
    "    for node in range(n):\n",
    "        neighbors = set()  # Use a set to avoid duplicate neighbors\n",
    "        # Add neighbors from outgoing edges (row)\n",
    "        for neighbor in range(n):\n",
    "            if adjacency_matrix[node, neighbor] != 0:\n",
    "                neighbors.add(neighbor)\n",
    "        \n",
    "        # Add neighbors from incoming edges (column)\n",
    "        for neighbor in range(n):\n",
    "            if adjacency_matrix[neighbor, node] != 0:\n",
    "                neighbors.add(neighbor)\n",
    "        \n",
    "        neighbors_dict[node] = list(neighbors)\n",
    "\n",
    "    return neighbors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad7ceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [2], 1: [0, 2, 3, 4], 2: [3], 3: [], 4: [0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = find_neighbors(adjacency_matrix[:-1,:-1])\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfaaf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 2, 4], 1: [0, 2, 3, 4], 2: [0, 1, 3], 3: [1, 2], 4: [0, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "neighbors = find_neighbors_undirected(adjacency_matrix[:-1,:-1])\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "effba6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory dict for node 2: {0: [], 1: [], 2: [[3, 2], [2]], 3: [[3]], 4: []}\n",
      "memory dict for node 0: {0: [[2, 0], [2, 3, 0], [0]], 1: [], 2: [[3, 2], [2]], 3: [[3]], 4: []}\n",
      "memory dict for node 4: {0: [[2, 0], [2, 3, 0], [0]], 1: [], 2: [[3, 2], [2]], 3: [[3]], 4: [[2, 0, 4], [2, 3, 0, 4], [0, 4], [4]]}\n",
      "memory dict for node 1: {0: [[2, 0], [2, 3, 0], [0]], 1: [[2, 0, 1], [2, 3, 0, 1], [0, 1], [1], [3, 2, 1], [2, 1], [1], [3, 1], [1], [4, 1], [0, 4, 1], [0, 2, 4, 1], [0, 2, 3, 4, 1], [1]], 2: [[3, 2], [2]], 3: [[3]], 4: [[2, 0, 4], [2, 3, 0, 4], [0, 4], [4]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [[0], [0, 2], [0, 2, 3]],\n",
       " 1: [[1],\n",
       "  [0, 1],\n",
       "  [1, 2],\n",
       "  [1, 3],\n",
       "  [1, 4],\n",
       "  [0, 1, 4],\n",
       "  [0, 1, 2],\n",
       "  [1, 2, 3],\n",
       "  [0, 1, 2, 3],\n",
       "  [0, 1, 2, 4],\n",
       "  [0, 1, 2, 3, 4]],\n",
       " 2: [[3, 2], [2]],\n",
       " 3: [[3]],\n",
       " 4: [[2, 0, 4], [2, 3, 0, 4], [0, 4], [4]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_valid_DAG_subsets(adjacency_matrix):\n",
    "\n",
    "    # get neighbors of each task (excluding Target node)\n",
    "    tasks_neighbors = find_neighbors(adjacency_matrix[:-1,:-1])\n",
    "    #tasks_neighbors = find_neighbors_undirected(adjacency_matrix[:-1,:-1])\n",
    "\n",
    "    # get number of non-Target nodes\n",
    "    n = adjacency_matrix.shape[0] - 1\n",
    "\n",
    "    def valid_subsets_recursive(node):\n",
    "        # if valid subsets for neighbor are already computed, use them\n",
    "        if len(memory_dict[node]) > 0:\n",
    "            return memory_dict[node]\n",
    "        \n",
    "\n",
    "        # if node is terminal node (i.e., has no neighbors) return node itself as singleton\n",
    "        if len(tasks_neighbors[node]) == 0:\n",
    "            memory_dict[node] = [[node]]\n",
    "            return memory_dict[node]\n",
    "        \n",
    "\n",
    "        # initialize list for valid subsets in all neighbors\n",
    "        valid_subsets = []\n",
    "\n",
    "        # for each neighbor get valid subsets\n",
    "        for neighbor in tasks_neighbors[node]:\n",
    "            #print(f'\\nneighbor of {node}: node {neighbor}')\n",
    "            # initialize list for valid subsets in neighbor\n",
    "            valid_neighbor_subsets = []\n",
    "\n",
    "            # if neighbor is terminal node (i.e., has no neighbors) generate valid \"terminal\" subsets \n",
    "            if len(neighbors[neighbor]) == 0:\n",
    "                #print(f'terminal node: {neighbor}')\n",
    "\n",
    "                valid_neighbor_subsets = [[node, neighbor]]\n",
    "\n",
    "                # add node itself as singleton to set of valid subsets\n",
    "                valid_neighbor_subsets.append([node])\n",
    "                \n",
    "                #print(valid_neighbor_subsets)\n",
    "            # if neighbor has neighbors, get valid subsets of neighbor and add node to each subset\n",
    "            else:\n",
    "                # get valid subsets of neighbor\n",
    "                valid_neighbor_subsets = valid_subsets_recursive(neighbor)\n",
    "\n",
    "                # add parent node to valid subsets of neighbor node\n",
    "                valid_neighbor_subsets = [subset + [node] for subset in valid_neighbor_subsets]\n",
    "\n",
    "                # add node itself as singleton to set of valid subsets\n",
    "                valid_neighbor_subsets.append([node])\n",
    "\n",
    "                #print(f'add node {node} to terminal subsets of neighbor node {neighbor}')\n",
    "                #print(valid_neighbor_subsets)\n",
    "\n",
    "            # add valid subsets of neighbor to valid subsets of all neighbors\n",
    "            valid_subsets.extend(valid_neighbor_subsets)\n",
    "\n",
    "        # update memory\n",
    "        memory_dict[node] = valid_subsets\n",
    "        print(f'memory dict for node {node}: {memory_dict}')\n",
    "        \n",
    "        # return unique elements of all_subsets\n",
    "        unique_valid_subsets = [sorted(list(t)) for t in set(tuple(subset) for subset in valid_subsets)]\n",
    "        unique_valid_subsets = sorted(unique_valid_subsets, key=len)\n",
    "        \n",
    "        return unique_valid_subsets\n",
    "    \n",
    "\n",
    "    # initialize dict for valid subsets to act as memory\n",
    "    memory_dict = {key: [] for key in range(n)}\n",
    "\n",
    "    # initialize dictionary for valid subsets origniating from each node\n",
    "    valid_subsets_dict = {}\n",
    "    for node in range(n):\n",
    "        valid_subsets_dict[node] = valid_subsets_recursive(node)\n",
    "\n",
    "    return valid_subsets_dict\n",
    "\n",
    "\n",
    "valid_subsets_dict = get_valid_DAG_subsets(adjacency_matrix)\n",
    "valid_subsets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d7f7d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory dict for node 1: {0: [], 1: [[3, 1], [1]], 2: [], 3: [[3]]}\n",
      "memory dict for node 2: {0: [], 1: [[3, 1], [1]], 2: [[3, 2], [2]], 3: [[3]]}\n",
      "memory dict for node 0: {0: [[1, 0], [1, 3, 0], [0], [2, 0], [2, 3, 0], [0]], 1: [[3, 1], [1]], 2: [[3, 2], [2]], 3: [[3]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [[0], [0, 2], [0, 1], [0, 1, 3], [0, 2, 3]],\n",
       " 1: [[3, 1], [1]],\n",
       " 2: [[3, 2], [2]],\n",
       " 3: [[3]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_adjacency_matrix = np.array([[0, 1, 1, 0, 0],\n",
    "                                     [0, 0, 0, 1, 0],\n",
    "                                     [0, 0, 0, 1, 0],\n",
    "                                     [0, 0, 0, 0, 0],\n",
    "                                     [0, 0, 0, 0, 0]])\n",
    "valid_subsets_dict = get_valid_DAG_subsets(example_adjacency_matrix)\n",
    "valid_subsets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ef3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_combination_valid(combination, n):\n",
    "    # Flatten list of combination\n",
    "    covered_tasks_list = [element for sublist in combination for element in sublist]\n",
    "    \n",
    "    # Create a set of the flattened list\n",
    "    covered_tasks_set = set(covered_tasks_list)\n",
    "    \n",
    "    # Check if the flattened set has exactly n elements and contains all elements from 0 to n-1\n",
    "    if len(covered_tasks_list) == n and covered_tasks_set == set(range(n)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_combinations(valid_subsets_dict, current_key=0, current_combination=None, result=None):\n",
    "    if current_combination is None:\n",
    "        current_combination = []\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    # Base case: if convered all tasks add current combination to the result list\n",
    "    if is_combination_valid(current_combination, len(valid_subsets_dict)):\n",
    "        print(f'********found valid combination: {current_combination}********')\n",
    "        result.append(current_combination)\n",
    "        return result\n",
    "\n",
    "    # Recursive case: iterate through the list of lists at the current key\n",
    "    for subset in valid_subsets_dict[current_key]:\n",
    "        # Create a new combination including the current subset\n",
    "        new_combination = current_combination + [subset]\n",
    "        new_combination_flattened = [element for sublist in new_combination for element in sublist]\n",
    "\n",
    "        # Check which nodes are NOT covered by the new combination. Only need to process these nodes next\n",
    "        uncovered_nodes = list(set(range(len(valid_subsets_dict))) - set(new_combination_flattened))\n",
    "        print(f'new combination: {new_combination}\\n uncovered nodes: {uncovered_nodes}')\n",
    "        #uncovered_nodes_valid_subsets_dict = {key: value for key, value in valid_subsets_dict.items() if key in uncovered_nodes}\n",
    "\n",
    "        if len(uncovered_nodes) == 0:\n",
    "            if is_combination_valid(new_combination, len(valid_subsets_dict)):\n",
    "                print(f'********found valid combination: {new_combination}********\\n')\n",
    "                result.append(new_combination)\n",
    "                return result\n",
    "        else:\n",
    "            # Recursively call the function to process the next key\n",
    "            for nex_key in uncovered_nodes:\n",
    "                generate_combinations(valid_subsets_dict, nex_key, new_combination, result)\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf3c6b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new combination: [[0]]\n",
      " uncovered nodes: [1, 2, 3]\n",
      "new combination: [[0], [3, 1]]\n",
      " uncovered nodes: [2]\n",
      "new combination: [[0], [3, 1], [3, 2]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3, 1], [2]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [3, 1], [2]]********\n",
      "\n",
      "new combination: [[0], [1]]\n",
      " uncovered nodes: [2, 3]\n",
      "new combination: [[0], [1], [3, 2]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [1], [3, 2]]********\n",
      "\n",
      "new combination: [[0], [1], [3]]\n",
      " uncovered nodes: [2]\n",
      "new combination: [[0], [1], [3], [3, 2]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [1], [3], [2]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [1], [3], [2]]********\n",
      "\n",
      "new combination: [[0], [3, 2]]\n",
      " uncovered nodes: [1]\n",
      "new combination: [[0], [3, 2], [3, 1]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3, 2], [1]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [3, 2], [1]]********\n",
      "\n",
      "new combination: [[0], [2]]\n",
      " uncovered nodes: [1, 3]\n",
      "new combination: [[0], [2], [3, 1]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [2], [3, 1]]********\n",
      "\n",
      "new combination: [[0], [2], [3]]\n",
      " uncovered nodes: [1]\n",
      "new combination: [[0], [2], [3], [3, 1]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [2], [3], [1]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [2], [3], [1]]********\n",
      "\n",
      "new combination: [[0], [3]]\n",
      " uncovered nodes: [1, 2]\n",
      "new combination: [[0], [3], [3, 1]]\n",
      " uncovered nodes: [2]\n",
      "new combination: [[0], [3], [3, 1], [3, 2]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3], [3, 1], [2]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3], [1]]\n",
      " uncovered nodes: [2]\n",
      "new combination: [[0], [3], [1], [3, 2]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3], [1], [2]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [3], [1], [2]]********\n",
      "\n",
      "new combination: [[0], [3], [3, 2]]\n",
      " uncovered nodes: [1]\n",
      "new combination: [[0], [3], [3, 2], [3, 1]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3], [3, 2], [1]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3], [2]]\n",
      " uncovered nodes: [1]\n",
      "new combination: [[0], [3], [2], [3, 1]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0], [3], [2], [1]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0], [3], [2], [1]]********\n",
      "\n",
      "new combination: [[0, 2]]\n",
      " uncovered nodes: [1, 3]\n",
      "new combination: [[0, 2], [3, 1]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0, 2], [3, 1]]********\n",
      "\n",
      "new combination: [[0, 2], [3]]\n",
      " uncovered nodes: [1]\n",
      "new combination: [[0, 2], [3], [3, 1]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0, 2], [3], [1]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0, 2], [3], [1]]********\n",
      "\n",
      "new combination: [[0, 1]]\n",
      " uncovered nodes: [2, 3]\n",
      "new combination: [[0, 1], [3, 2]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0, 1], [3, 2]]********\n",
      "\n",
      "new combination: [[0, 1], [3]]\n",
      " uncovered nodes: [2]\n",
      "new combination: [[0, 1], [3], [3, 2]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0, 1], [3], [2]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0, 1], [3], [2]]********\n",
      "\n",
      "new combination: [[0, 1, 3]]\n",
      " uncovered nodes: [2]\n",
      "new combination: [[0, 1, 3], [3, 2]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0, 1, 3], [2]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0, 1, 3], [2]]********\n",
      "\n",
      "new combination: [[0, 2, 3]]\n",
      " uncovered nodes: [1]\n",
      "new combination: [[0, 2, 3], [3, 1]]\n",
      " uncovered nodes: []\n",
      "new combination: [[0, 2, 3], [1]]\n",
      " uncovered nodes: []\n",
      "********found valid combination: [[0, 2, 3], [1]]********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combinations = generate_combinations(valid_subsets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80aaac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[0], [3, 1], [2]],\n",
       " [[0], [1], [3, 2]],\n",
       " [[0], [1], [3], [2]],\n",
       " [[0], [3, 2], [1]],\n",
       " [[0], [2], [3, 1]],\n",
       " [[0], [2], [3], [1]],\n",
       " [[0], [3], [1], [2]],\n",
       " [[0], [3], [2], [1]],\n",
       " [[0, 2], [3, 1]],\n",
       " [[0, 2], [3], [1]],\n",
       " [[0, 1], [3, 2]],\n",
       " [[0, 1], [3], [2]],\n",
       " [[0, 1, 3], [2]],\n",
       " [[0, 2, 3], [1]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(combinations))\n",
    "combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a12a6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[0], [3, 1], [2]],\n",
       " [[0], [1], [3, 2]],\n",
       " [[0], [1], [3], [2]],\n",
       " [[0, 2], [3, 1]],\n",
       " [[0, 2], [3], [1]],\n",
       " [[0, 1], [3, 2]],\n",
       " [[0, 1], [3], [2]],\n",
       " [[0, 1, 3], [2]],\n",
       " [[0, 2, 3], [1]]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_sublist(sublist):\n",
    "    # Sort the elements within each inner list and then sort the entire sublist\n",
    "    return tuple(sorted(tuple(sorted(inner)) for inner in sublist))\n",
    "\n",
    "def unique_lists(input_list):\n",
    "    seen = set()\n",
    "    unique_combinations = []\n",
    "\n",
    "    for sublist in input_list:\n",
    "        normalized = normalize_sublist(sublist)\n",
    "        if normalized not in seen:\n",
    "            seen.add(normalized)\n",
    "            unique_combinations.append(sublist)\n",
    "\n",
    "    return unique_combinations\n",
    "\n",
    "unique_combinations = unique_lists(combinations)\n",
    "print(len(unique_combinations))\n",
    "unique_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc0d8c",
   "metadata": {},
   "source": [
    "### Generate all possible partition schemes for the set of tasks (ignoring structre of the DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b4d6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def partitions(set_):\n",
    "    if not set_:\n",
    "        yield []\n",
    "        return\n",
    "    for i in range(1, len(set_) + 1):\n",
    "        for part in combinations(set_, i):\n",
    "            remaining = set(set_) - set(part)\n",
    "            if not remaining:\n",
    "                yield [list(part)]\n",
    "            else:\n",
    "                for b in partitions(list(remaining)):\n",
    "                    yield [list(part)] + b\n",
    "\n",
    "def generate_unique_partitions(numbers):\n",
    "    all_partitions = set()\n",
    "    for partition in partitions(numbers):\n",
    "        # Create a frozenset of frozensets to make each partition hashable and order-independent\n",
    "        partition_set = frozenset(frozenset(part) for part in partition)\n",
    "        all_partitions.add(partition_set)\n",
    "    \n",
    "    # Convert the frozensets back to lists for the final output\n",
    "    unique_partitions = [list(map(list, partition)) for partition in all_partitions]\n",
    "\n",
    "    # Sort elements\n",
    "    unique_partitions = sorted([sorted(x) for x in unique_partitions], key=len)\n",
    "    return unique_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e0a6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of numbers for non-\"Target\" tasks in occupation\n",
    "tasks_list_numbers = list(range(len(valid_subsets_dict)))\n",
    "\n",
    "# Generate all possible partitioning schemes\n",
    "all_partitions = generate_unique_partitions(tasks_list_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471fb25",
   "metadata": {},
   "source": [
    "### Check if partition scheme is \"valid\" (i.e., if its non-singleton partitions are a connected graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7fb15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_connected(matrix):\n",
    "    # Number of nodes in the matrix\n",
    "    num_nodes = matrix.shape[0]\n",
    "    \n",
    "    # Visited array to keep track of visited nodes\n",
    "    visited = np.zeros(num_nodes, dtype=bool)\n",
    "    \n",
    "    # Helper function to perform DFS\n",
    "    def dfs(node):\n",
    "        visited[node] = True\n",
    "        # Visit all the neighbors of the current node\n",
    "        for neighbor in range(num_nodes):\n",
    "            if matrix[node, neighbor] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "            elif matrix[neighbor, node] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "    \n",
    "    # Start DFS from the first node (node 0)\n",
    "    dfs(0)\n",
    "    \n",
    "    # If all nodes are visited, the matrix is connected\n",
    "    return np.all(visited)\n",
    "\n",
    "\n",
    "def validate_partition_using_connectedness(adjacency_matrix, tasks_list):\n",
    "    # Return valid if Singleton\n",
    "    if len(tasks_list) == 1:\n",
    "        return True\n",
    "    # Check if partition forms connected graph\n",
    "    else:\n",
    "        # Subset original adjacency matrix\n",
    "        subset_matrix = adjacency_matrix[np.ix_(tasks_list, tasks_list)]\n",
    "\n",
    "        # check if subset matrix is a connected graph\n",
    "        subset_matrix_connected = is_connected(subset_matrix)\n",
    "\n",
    "        # return true if connected and false otherwise\n",
    "        return subset_matrix_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a663211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all possible partitioning schemes: 15\n",
      "Number of valid partitioning schemes given DAG structure: 12\n",
      "[[0, 1, 2, 3]]\n",
      "[[0, 2, 3], [1]]\n",
      "[[0, 1], [2, 3]]\n",
      "[[0, 2], [1, 3]]\n",
      "[[0, 1, 2], [3]]\n",
      "[[0], [1, 2, 3]]\n",
      "[[0, 1, 3], [2]]\n",
      "[[0, 2], [1], [3]]\n",
      "[[0], [1, 3], [2]]\n",
      "[[0, 1], [2], [3]]\n",
      "[[0], [1], [2, 3]]\n",
      "[[0], [1], [2], [3]]\n"
     ]
    }
   ],
   "source": [
    "# Get valid partitioning schemes from all possible partitions to cut computation load\n",
    "valid_partitions = []\n",
    "for scheme in all_partitions:\n",
    "    # Set valid partitions count to 0\n",
    "    valid_partition_count = 0\n",
    "    for partition in scheme:\n",
    "        valid_partition = validate_partition_using_connectedness(example_adjacency_matrix, partition)\n",
    "        if valid_partition:\n",
    "            valid_partition_count += 1\n",
    "    \n",
    "    # If number of valid partitions within a partition scheme is equal to \n",
    "    # number of partitions in partition scheme then partition scheme is valid\n",
    "    if valid_partition_count == len(scheme):\n",
    "        valid_partitions.append(scheme)\n",
    "\n",
    "# Print stats\n",
    "print(f'Number of all possible partitioning schemes: {len(all_partitions)}')\n",
    "print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "for partition in valid_partitions:\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcffb4",
   "metadata": {},
   "source": [
    "## Missing in \"Smart\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbb02247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3]]\n",
      "[[0, 1, 2], [3]]\n",
      "[[0], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "def normalize_sublist(sublist):\n",
    "    # Sort the elements within each inner list and then sort the entire sublist\n",
    "    return tuple(sorted(tuple(sorted(inner)) for inner in sublist))\n",
    "\n",
    "def list_difference(list1, list2):\n",
    "    # Normalize both lists\n",
    "    normalized_list1 = {normalize_sublist(sublist) for sublist in list1}\n",
    "    normalized_list2 = {normalize_sublist(sublist) for sublist in list2}\n",
    "    \n",
    "    # Find the difference\n",
    "    difference = normalized_list1 - normalized_list2\n",
    "    \n",
    "    # Convert the normalized tuples back to the original list format\n",
    "    difference_list = []\n",
    "    for norm_sublist in difference:\n",
    "        original_sublist = [list(inner) for inner in norm_sublist]\n",
    "        difference_list.append(original_sublist)\n",
    "    \n",
    "    return difference_list\n",
    "\n",
    "result = list_difference(valid_partitions, unique_combinations)\n",
    "for case in result:\n",
    "    print(case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
