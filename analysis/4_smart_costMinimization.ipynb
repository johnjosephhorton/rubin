{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)\n",
    "\n",
    "with open('functions.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c01952",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('all')\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymansh':\n",
    "    main_folder_path = '/Users/peymansh/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9747ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "onet_data_path = f'{data_path}/data/onet_occupations_yearly.csv'\n",
    "\n",
    "# list of occupations to create DAGs for\n",
    "occupation_list = ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators', \n",
    "                   'dredgeOperators', 'gradersAndSortersForAgriculturalProducts', 'reinforcingIronAndRebarWorkers',\n",
    "                   'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists', \n",
    "                   'personalCareAides', 'proofreadersAndCopyMarkers', 'chiropractors', \n",
    "                   'shippingReceivingAndInventoryClerks', 'cooksShortOrder', 'orthodontists',\n",
    "                   'subwayAndStreetcarOperators', 'packersAndPackagersHand', 'hoistAndWinchOperators', \n",
    "                   'forgingMachineSettersOperatorsAndTenders', 'avionicsTechnicians', 'dishwashers', \n",
    "                   'dispatchersExceptPoliceFireAndAmbulance', 'familyMedicinePhysicians', 'MachineFeedersAndOffbearers'\n",
    "                   ]\n",
    "\n",
    "occupation = 'travelAgents'\n",
    "occupation = 'insuranceUnderwriters'\n",
    "occupation = 'pileDriverOperators'\n",
    "\n",
    "# Generate occupation-specific strings\n",
    "GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49bb531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha as AI quality metric\n",
    "n = 100\n",
    "epsilon = 1e-8\n",
    "alpha_list = np.linspace(epsilon, 1-epsilon, n).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40506e",
   "metadata": {},
   "source": [
    "### Initialize input-output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200fa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_M_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_M.csv'\n",
    "\n",
    "# First Last Task DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_FLT_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_FLT.csv'\n",
    "\n",
    "# Conditioned First Last Task DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_CFLT_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_CFLT.csv'\n",
    "\n",
    "# Partitioned DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_P_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_P.csv'\n",
    "\n",
    "# Conditioned Partitioned DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_CP_GPT_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_CP.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd525be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Move hand and foot levers of hoisting equipmen...</td>\n",
       "      <td>Drive pilings to provide support for buildings...</td>\n",
       "      <td>The worker driving the pilings needs to know t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drive pilings to provide support for buildings...</td>\n",
       "      <td>Move levers and turn valves to activate power ...</td>\n",
       "      <td>The worker operating the levers and valves to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Clean, lubricate, and refill equipment.</td>\n",
       "      <td>The worker responsible for cleaning, lubricati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Move hand and foot levers of hoisting equipmen...</td>\n",
       "      <td>The worker moving hand and foot levers to posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clean, lubricate, and refill equipment.</td>\n",
       "      <td>Move hand and foot levers of hoisting equipmen...</td>\n",
       "      <td>The worker operating the hoisting equipment ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Drive pilings to provide support for buildings...</td>\n",
       "      <td>The worker driving pilings needs to know that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Conduct pre-operational checks on equipment to...</td>\n",
       "      <td>Move levers and turn valves to activate power ...</td>\n",
       "      <td>The worker who is moving levers and turning va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Clean, lubricate, and refill equipment.</td>\n",
       "      <td>\"Target\"</td>\n",
       "      <td>Job Completion Indicator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Move levers and turn valves to activate power ...</td>\n",
       "      <td>\"Target\"</td>\n",
       "      <td>Job Completion Indicator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source  \\\n",
       "0   Move hand and foot levers of hoisting equipmen...   \n",
       "2   Drive pilings to provide support for buildings...   \n",
       "3   Conduct pre-operational checks on equipment to...   \n",
       "4   Conduct pre-operational checks on equipment to...   \n",
       "5             Clean, lubricate, and refill equipment.   \n",
       "6   Conduct pre-operational checks on equipment to...   \n",
       "8   Conduct pre-operational checks on equipment to...   \n",
       "10            Clean, lubricate, and refill equipment.   \n",
       "11  Move levers and turn valves to activate power ...   \n",
       "\n",
       "                                               target  \\\n",
       "0   Drive pilings to provide support for buildings...   \n",
       "2   Move levers and turn valves to activate power ...   \n",
       "3             Clean, lubricate, and refill equipment.   \n",
       "4   Move hand and foot levers of hoisting equipmen...   \n",
       "5   Move hand and foot levers of hoisting equipmen...   \n",
       "6   Drive pilings to provide support for buildings...   \n",
       "8   Move levers and turn valves to activate power ...   \n",
       "10                                           \"Target\"   \n",
       "11                                           \"Target\"   \n",
       "\n",
       "                                              comment  \n",
       "0   The worker driving the pilings needs to know t...  \n",
       "2   The worker operating the levers and valves to ...  \n",
       "3   The worker responsible for cleaning, lubricati...  \n",
       "4   The worker moving hand and foot levers to posi...  \n",
       "5   The worker operating the hoisting equipment ne...  \n",
       "6   The worker driving pilings needs to know that ...  \n",
       "8   The worker who is moving levers and turning va...  \n",
       "10                           Job Completion Indicator  \n",
       "11                           Job Completion Indicator  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read DAG\n",
    "dag_df = pd.read_csv(input_path)\n",
    "\n",
    "# remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "if 'comment' in dag_df.columns:\n",
    "    dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "# get task stats\n",
    "tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "tasks_stats\n",
    "\n",
    "# print stats\n",
    "#tasks_stats.iloc[:,1:].sum()\n",
    "dag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f86c511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Move hand and foot levers of hoisting equipment to position piling leads, hoist piling into leads, and position hammers over pilings.',\n",
       " 1: 'Conduct pre-operational checks on equipment to ensure proper functioning.',\n",
       " 2: 'Drive pilings to provide support for buildings or other structures, using heavy equipment with a pile driver head.',\n",
       " 3: 'Move levers and turn valves to activate power hammers, or to raise and lower drophammers that drive piles to required depths.',\n",
       " 4: 'Clean, lubricate, and refill equipment.',\n",
       " 5: '\"Target\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract list of tasks and create a dictionary for indexing tasks\n",
    "tasks_list = tasks_stats['task'].unique()\n",
    "tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "# create numpy array of adjacency matrix\n",
    "adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "for _, row in dag_df.iterrows():\n",
    "    source_index = aux_dict[row['source']]\n",
    "    target_index = aux_dict[row['target']]\n",
    "    adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "tasks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(adjacency_matrix):\n",
    "    n = adjacency_matrix.shape[0]  # Number of nodes\n",
    "    neighbors_dict = {}\n",
    "\n",
    "    for node in range(n):\n",
    "        neighbors = []\n",
    "        for neighbor in range(n):\n",
    "            if adjacency_matrix[node, neighbor] != 0:  # Check for an edge from node to neighbor\n",
    "                neighbors.append(neighbor)\n",
    "        neighbors_dict[node] = neighbors\n",
    "\n",
    "    # add node itself to set of its neighbors\n",
    "    for key in neighbors_dict.keys():\n",
    "        neighbors_dict[key].append(key)\n",
    "\n",
    "    return neighbors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1733cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_node_inclusive_partitions(arr, element):\n",
    "    # Ensure the element is in the list\n",
    "    if element not in arr:\n",
    "        return []\n",
    "\n",
    "    # Remove the specific element from the list\n",
    "    arr.remove(element)\n",
    "    \n",
    "    # Generate all subsets of the remaining elements\n",
    "    subsets = []\n",
    "    for r in range(len(arr) + 1):\n",
    "        subsets.extend(combinations(arr, r))\n",
    "    \n",
    "    # Add the specific element back to each subset\n",
    "    subsets_with_element = []\n",
    "    for subset in subsets:\n",
    "        subsets_with_element.append((element,) + subset)\n",
    "\n",
    "    subsets_with_element = [list(subset) for subset in subsets_with_element]\n",
    "    return subsets_with_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_unique_partitions(my_list):\n",
    "    unique_values_tuple = set(tuple(sorted(inner_list)) for inner_list in my_list)\n",
    "    unique_values_lists = [list(inner_tuple) for inner_tuple in unique_values_tuple]\n",
    "    unique_values_lists.sort(key=len)\n",
    "    return unique_values_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca00bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsets_with_all_elements(input_list, must_include):\n",
    "    # Generate all possible subsets\n",
    "    all_subsets = []\n",
    "    for r in range(len(input_list) + 1):\n",
    "        subsets = combinations(input_list, r)\n",
    "        all_subsets.extend(subsets)\n",
    "    \n",
    "    # Filter subsets to include only those that contain all elements from must_include except the must_include itself\n",
    "    valid_subsets = [list(subset) for subset in all_subsets if all(elem in subset for elem in must_include) and list(subset) != must_include]\n",
    "    \n",
    "    return valid_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf98048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_DAG_subsets(adjacency_matrix):\n",
    "    # subset adjacency matrix to exclude Target node\n",
    "    non_target_adjacency_matrix = adjacency_matrix[:-1,:-1].copy()\n",
    "\n",
    "    # get neighbors of each task\n",
    "    tasks_neighbors = find_neighbors(non_target_adjacency_matrix)\n",
    "\n",
    "    # get list of tasks with no outgoing edges (i.e., last tasks)\n",
    "    last_tasks_list = [key for key, value in tasks_neighbors.items() if len(value) == 1]\n",
    "\n",
    "    # get number of non-Target nodes\n",
    "    n = non_target_adjacency_matrix.shape[0]\n",
    "\n",
    "    # initialize dict for valid subsets of nodes (and also partitions) to act as memory\n",
    "    memory_dict = {key: [] for key in range(n)}\n",
    "    \n",
    "\n",
    "    # initialize dictionary for valid subsets origniating from each node\n",
    "    valid_subsets_dict = {}\n",
    "\n",
    "    def valid_subsets_recursive(partition):\n",
    "\n",
    "        # if partition already in memory return its value\n",
    "        try:\n",
    "            if len(memory_dict[tuple(sorted(partition))]) > 0:\n",
    "                return memory_dict[tuple(partition)]\n",
    "            \n",
    "        # if partition not in memory, get valid subsets of partition\n",
    "        except KeyError:\n",
    "            # get neighbors of partition\n",
    "            neighbors_list = set(np.where(non_target_adjacency_matrix[partition, :].sum(axis=0) > 0)[0])\n",
    "            neighbors_list = list(neighbors_list.difference(partition)) # remove partition nodes from neighbors list\n",
    "\n",
    "            # if partition is terminal (i.e., has no outgoing edges), return partition + empty list \n",
    "            if len(neighbors_list) == 0:\n",
    "                memory_dict[tuple(sorted(partition))] = [partition]\n",
    "                return [partition]\n",
    "            # if partition is non-terminal, run recursive function on all partitions of neighbors\n",
    "            else:\n",
    "                neighbors_list = set(np.where(non_target_adjacency_matrix[partition, :].sum(axis=0) > 0)[0])\n",
    "                neighbors_list = list(neighbors_list.difference(partition)) # remove partition nodes from neighbors list\n",
    "                # add partition nodes to neighbors list\n",
    "                for node in partition:\n",
    "                    neighbors_list.append(node)\n",
    "                \n",
    "                # get all subsets of neighbors list that contain elements of partition\n",
    "                partitions_of_partition = get_subsets_with_all_elements(neighbors_list, partition)\n",
    "                \n",
    "                partitions_of_partition_valid_subsets = []\n",
    "                for partition_of_partition in partitions_of_partition:\n",
    "                    partition_of_partition_valid_subsets = valid_subsets_recursive(partition_of_partition)\n",
    "\n",
    "                    # add partition nodes to neighbor's valid subsets\n",
    "                    partition_of_partition_valid_subsets = [list(set(partition + subset)) for subset in partition_of_partition_valid_subsets]\n",
    "\n",
    "                    # add partition itself as a valid subset\n",
    "                    partition_of_partition_valid_subsets.append(partition)\n",
    "                    partition_of_partition_valid_subsets = keep_unique_partitions(partition_of_partition_valid_subsets)\n",
    "\n",
    "                    # update memory for partition of current partition\n",
    "                    memory_dict[tuple(sorted(partition_of_partition))] = partition_of_partition_valid_subsets\n",
    "\n",
    "                    partitions_of_partition_valid_subsets.extend(partition_of_partition_valid_subsets)\n",
    "                    partitions_of_partition_valid_subsets = keep_unique_partitions(partitions_of_partition_valid_subsets)\n",
    "\n",
    "                # update memory for partition\n",
    "                memory_dict[tuple(sorted(partition))] = partitions_of_partition_valid_subsets            \n",
    "                return partitions_of_partition_valid_subsets\n",
    "            \n",
    "\n",
    "    for node in range(n):\n",
    "        valid_subsets_dict[node] = valid_subsets_recursive([node])\n",
    "\n",
    "    # modify valid subsets of last tasks\n",
    "    for last_task in last_tasks_list:\n",
    "        last_task_neighbors = [i for i in range(n) if non_target_adjacency_matrix[i, last_task] != 0]\n",
    "        last_task_neighbors.append(last_task)\n",
    "        \n",
    "        node_partitions = get_node_inclusive_partitions(last_task_neighbors, last_task)\n",
    "\n",
    "        # update valid_subsets_dict for last task\n",
    "        # exclude partitions with only two nodes as they already appear in parent node partitions\n",
    "        valid_subsets_dict[last_task] = [partition for partition in node_partitions if len(partition) != 2]\n",
    "\n",
    "\n",
    "    return valid_subsets_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_subsets_dict = get_valid_DAG_subsets(adjacency_matrix)\n",
    "valid_subsets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_combination_valid(combination, n):\n",
    "    # Flatten list of combination\n",
    "    covered_tasks_list = [element for sublist in combination for element in sublist]\n",
    "    \n",
    "    # Create a set of the flattened list\n",
    "    covered_tasks_set = set(covered_tasks_list)\n",
    "    \n",
    "    # Check if the flattened set has exactly n elements and contains all elements from 0 to n-1\n",
    "    if len(covered_tasks_list) == n and covered_tasks_set == set(range(n)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_combinations(valid_subsets_dict, current_key=0, current_combination=None, result=None):\n",
    "    if current_combination is None:\n",
    "        current_combination = []\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    # Base case: if convered all tasks add current combination to the result list\n",
    "    if is_combination_valid(current_combination, len(valid_subsets_dict)):\n",
    "        result.append(current_combination)\n",
    "        return result\n",
    "\n",
    "    # Recursive case: iterate through the list of lists at the current key\n",
    "    for subset in valid_subsets_dict[current_key]:\n",
    "        # Create a new combination including the current subset\n",
    "        new_combination = current_combination + [subset]\n",
    "        new_combination_flattened = [element for sublist in new_combination for element in sublist]\n",
    "\n",
    "        # Check which nodes are NOT covered by the new combination. Only need to process these nodes next\n",
    "        uncovered_nodes = list(set(range(len(valid_subsets_dict))) - set(new_combination_flattened))\n",
    "\n",
    "        if len(uncovered_nodes) == 0:\n",
    "            if is_combination_valid(new_combination, len(valid_subsets_dict)):\n",
    "                result.append(new_combination)\n",
    "                return result\n",
    "        else:\n",
    "            # Recursively call the function to process the next key\n",
    "            for nex_key in uncovered_nodes:\n",
    "                generate_combinations(valid_subsets_dict, nex_key, new_combination, result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for initial_node in range(len(valid_subsets_dict)):\n",
    "    combinations.extend(generate_combinations(valid_subsets_dict, initial_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62091253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sublist(sublist):\n",
    "    # Sort the elements within each inner list and then sort the entire sublist\n",
    "    return tuple(sorted(tuple(sorted(inner)) for inner in sublist))\n",
    "\n",
    "def unique_lists(input_list):\n",
    "    seen = set()\n",
    "    unique_combinations = []\n",
    "\n",
    "    for sublist in input_list:\n",
    "        normalized = normalize_sublist(sublist)\n",
    "        if normalized not in seen:\n",
    "            seen.add(normalized)\n",
    "            unique_combinations.append(sublist)\n",
    "\n",
    "    return unique_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combinations = unique_lists(combinations)\n",
    "print(len(unique_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0af166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
