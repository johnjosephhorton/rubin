{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)\n",
    "\n",
    "with open('functions.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdd9c966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['caffeinate']>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run caffeinate in the background to prevent sleep\n",
    "subprocess.Popen(['caffeinate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymansh':\n",
    "    main_folder_path = '/Users/peymansh/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54980713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tasks(onet_data_path,\n",
    "              occupation_code):\n",
    "\n",
    "    # Load the data\n",
    "    onet = pd.read_csv(onet_data_path)\n",
    "    onet = onet.sort_values(by=['year', 'occ_code', 'occ_title', 'task_id'])\n",
    "    onet = onet[onet['year'] == 2023].reset_index(drop=True)\n",
    "\n",
    "    # Get list of tasks\n",
    "    my_df = onet[(onet.occ_code == f'{occupation_code}') & (onet.year == 2023)]\n",
    "    tasks = my_df['task'].unique().tolist()\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8161f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(adjacency_matrix):\n",
    "    # Get the number of nodes (n) from the shape of the adjacency matrix\n",
    "    n = adjacency_matrix.shape[0]\n",
    "    \n",
    "    # Initialize an empty dictionary to store the neighbors for each node\n",
    "    neighbors = {i: [] for i in range(n)}\n",
    "    \n",
    "    # Loop through each entry in the adjacency matrix\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # If there's an edge from i to j or from j to i, add j to the neighbors of i\n",
    "            if adjacency_matrix[i, j] == 1 or adjacency_matrix[j, i] == 1:\n",
    "                if j not in neighbors[i]:  # Avoid duplicate neighbors\n",
    "                    neighbors[i].append(j)\n",
    "                if i not in neighbors[j]:  # Ensure symmetry in the undirected version\n",
    "                    neighbors[j].append(i)\n",
    "    \n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc8778eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inactive_node_neighbor_subset_combinations(inactive_neighbors_valid_subsets_dict):\n",
    "    # Step 1: Extract lists from dictionary and remove duplicates within lists\n",
    "    all_lists = [list(set(item)) for sublist in inactive_neighbors_valid_subsets_dict.values() for item in sublist]\n",
    "\n",
    "    # Step 2: Create all combinations of lists across keys\n",
    "    output_set = set()\n",
    "    for r in range(1, len(all_lists) + 1):\n",
    "        combinations = itertools.combinations(all_lists, r)\n",
    "        for combo in combinations:\n",
    "            # Flatten the combination of lists\n",
    "            flattened_combo = list(itertools.chain(*combo))\n",
    "            # Remove duplicates within the flattened list and sort for consistency\n",
    "            unique_combo = tuple(sorted(set(flattened_combo)))\n",
    "            # Add the unique combination to the output set\n",
    "            output_set.add(unique_combo)\n",
    "\n",
    "    # Convert the set back to a list of lists\n",
    "    output_list = [list(combo) for combo in output_set]\n",
    "    return sorted(output_list, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f85a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_DAG_subsets(adjacency_matrix):\n",
    "    # subset adjacency matrix to exclude Target node\n",
    "    non_target_adjacency_matrix = adjacency_matrix[:-1,:-1].copy()\n",
    "\n",
    "    # get number of non-Target nodes\n",
    "    n = non_target_adjacency_matrix.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    def valid_subsets_recursive(adjacency_matrix, active_dict, memory_dict, partition):\n",
    "        # if partition already in memory return its value\n",
    "        try:\n",
    "            if len(memory_dict[tuple(sorted(partition))]) > 0:\n",
    "                return memory_dict[tuple(sorted(partition))]\n",
    "            \n",
    "        # if partition not in memory, get valid subsets of partition\n",
    "        except KeyError:\n",
    "            # get inactive neighbors of node\n",
    "            partition_neighbors = find_neighbors(adjacency_matrix)\n",
    "            neighbors_list = []\n",
    "            for node in partition:\n",
    "                neighbors_list.append(partition_neighbors[node])\n",
    "            \n",
    "            # delete repetitions and keep unique neighbors\n",
    "            neighbors_list = list(itertools.chain(*neighbors_list))\n",
    "            neighbors_list = list(set(neighbors_list))\n",
    "\n",
    "            inactive_neighbors_list = [neighbor for neighbor in neighbors_list if active_dict[neighbor] == False]\n",
    "\n",
    "            # if partition has no outgoing edges return partition and empty list\n",
    "            if len(inactive_neighbors_list) == 0:\n",
    "                memory_dict[tuple(sorted(partition))] = [partition, []]\n",
    "                return [partition, []]\n",
    "\n",
    "            # for each inactive neighbor get valid subsets\n",
    "            inactive_neighbors_valid_subsets = {}\n",
    "\n",
    "\n",
    "\n",
    "            ############\n",
    "            # set all neighbors to active and get valid subsets of neighbors\n",
    "            for neighbor in inactive_neighbors_list:\n",
    "                active_dict[neighbor] = True\n",
    "            # print(f'\\n\\n>>>>>>>>>active dict: {active_dict}<<<<<<<<<')\n",
    "            ############\n",
    "\n",
    "\n",
    "            # create all subsets of inactive neighbors to loop over\n",
    "            inactive_neighbor_subsets = []\n",
    "            for r in range(len(inactive_neighbors_list) + 1):\n",
    "                inactive_neighbor_subsets.extend(itertools.combinations(inactive_neighbors_list, r))\n",
    "            inactive_neighbor_subsets = [list(subset) for subset in inactive_neighbor_subsets if len(subset) > 0]\n",
    "\n",
    "\n",
    "            for neighbor_partition in inactive_neighbor_subsets:\n",
    "                ############\n",
    "                # make a copy of active_dict and memory dict for the neighbor contingencies\n",
    "                neighbor_active_dict = active_dict.copy()\n",
    "                neighbor_memory_dict = memory_dict.copy()\n",
    "                ############\n",
    "\n",
    "                valid_subsets = valid_subsets_recursive(adjacency_matrix, neighbor_active_dict, neighbor_memory_dict, neighbor_partition)\n",
    "\n",
    "                # add neighbor partition itself to valid subsets of neighbor\n",
    "                #valid_subsets = [subset + neighbor_partition for subset in valid_subsets]\n",
    "                valid_subsets = sorted(valid_subsets, key=len)\n",
    "\n",
    "                # add valid subsets of neighbor to memory\n",
    "                memory_dict[tuple(sorted(neighbor_partition))] = valid_subsets\n",
    "\n",
    "                # append valid subsets of neighbor to valid subsets of node\n",
    "                inactive_neighbors_valid_subsets[tuple(sorted(neighbor_partition))] = valid_subsets\n",
    "\n",
    "            # create all combinations of valid subsets of inactive neighbors\n",
    "            for key, lists in inactive_neighbors_valid_subsets.items():\n",
    "                #lists.append([])\n",
    "                for inner_list in lists:\n",
    "                    inner_list.extend(partition)\n",
    "            \n",
    "            inactive_neighbors_valid_subsets_combinations = []\n",
    "            for key, value in inactive_neighbors_valid_subsets.items():\n",
    "                inactive_neighbors_valid_subsets_combinations.extend(value)\n",
    "            \n",
    "            # add partition itself as a valid subset\n",
    "            inactive_neighbors_valid_subsets_combinations.append(partition)\n",
    "\n",
    "            # drop repetitions\n",
    "            inactive_neighbors_valid_subsets_combinations = [sorted(list(subset)) for subset in set(tuple(subset) for subset in inactive_neighbors_valid_subsets_combinations)]\n",
    "            \n",
    "            # sort combinations by length\n",
    "            inactive_neighbors_valid_subsets_combinations = sorted(inactive_neighbors_valid_subsets_combinations, key=len)\n",
    "\n",
    "            # filter out empty subsets -- mainly for compatibility reasons due to last nodes (i.e., nodes w/o outgoing edges)\n",
    "            inactive_neighbors_valid_subsets_combinations = [subset for subset in inactive_neighbors_valid_subsets_combinations if subset != []]\n",
    "\n",
    "            return inactive_neighbors_valid_subsets_combinations\n",
    "            \n",
    "\n",
    "\n",
    "    # initialize dictionary for valid subsets origniating from each node\n",
    "    valid_subsets_dict = {}\n",
    "    \n",
    "    # run the algorithm on each node and remove that node from the adjacency matrix after each iteration\n",
    "    my_adjacency_matrix = non_target_adjacency_matrix.copy()\n",
    "    for node in range(n):\n",
    "        # create active dictionary\n",
    "        global_active_dict = {i: False for i in range(n)}\n",
    "\n",
    "        # set node as active\n",
    "        global_active_dict[0] = True\n",
    "\n",
    "        # initialize dict for valid subsets of nodes (and also partitions) to act as memory\n",
    "        global_memory_dict = {}\n",
    "\n",
    "        # get valid subsets of node 0 in current adjacency matrix\n",
    "        valid_subsets = valid_subsets_recursive(my_adjacency_matrix, global_active_dict, global_memory_dict, [0])\n",
    "\n",
    "        # adjust output above for actual node number in main adjacency matrix\n",
    "        valid_subsets_dict[node] = [[element + node for element in subset] for subset in valid_subsets]\n",
    "\n",
    "        # ensure empty subsets do not exist in valid subsets\n",
    "        valid_subsets_dict[node] = [subset for subset in valid_subsets_dict[node] if len(subset) > 0]\n",
    "\n",
    "        # remove current node from adjacency matrix for next iteration\n",
    "        my_adjacency_matrix = my_adjacency_matrix[1:,1:]\n",
    "    \n",
    "    return valid_subsets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ad774b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_combination_valid(combination, n):\n",
    "    # Flatten list of combination\n",
    "    covered_tasks_list = [element for sublist in combination for element in sublist]\n",
    "    \n",
    "    # Create a set of the flattened list\n",
    "    covered_tasks_set = set(covered_tasks_list)\n",
    "    \n",
    "    # Check if the flattened set has exactly n elements and contains all elements from 0 to n-1\n",
    "    if len(covered_tasks_list) == n and covered_tasks_set == set(range(n)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_combinations(valid_subsets_dict, current_key=0, current_combination=None, result=None):\n",
    "    if current_combination is None:\n",
    "        current_combination = []\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    # Base case: if convered all tasks add current combination to the result list\n",
    "    if is_combination_valid(current_combination, len(valid_subsets_dict)):\n",
    "        result.append(current_combination)\n",
    "        return result\n",
    "\n",
    "    # Recursive case: iterate through the list of lists at the current key\n",
    "    for subset in valid_subsets_dict[current_key]:\n",
    "        # Create a new combination including the current subset\n",
    "        new_combination = current_combination + [subset]\n",
    "        new_combination_flattened = [element for sublist in new_combination for element in sublist]\n",
    "        \n",
    "        # lower the load of computation by skipping invalid combinations\n",
    "        # combination is invalid if:\n",
    "        # 1. length of new combination is greater than the length of valid subsets dictionary\n",
    "        # 2. new combination contains repetitive elements\n",
    "\n",
    "        # skip cases in which the new combination is invalid\n",
    "        if len(new_combination_flattened) > len(valid_subsets_dict):\n",
    "            continue\n",
    "        if len(new_combination_flattened) != len(set(new_combination_flattened)):\n",
    "            continue\n",
    "\n",
    "        # print(f'new combination: {new_combination}')\n",
    "\n",
    "        # Check which nodes are NOT covered by the new combination\n",
    "        uncovered_nodes = list(set(range(len(valid_subsets_dict))) - set(new_combination_flattened))\n",
    "        if len(uncovered_nodes) == 0:\n",
    "            if is_combination_valid(new_combination, len(valid_subsets_dict)):\n",
    "                result.append(new_combination)\n",
    "                return result\n",
    "        else:\n",
    "            # Recursively call the function to process the next key\n",
    "            for next_key in range(current_key + 1, len(valid_subsets_dict)):\n",
    "                generate_combinations(valid_subsets_dict, next_key, new_combination, result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61692d1b",
   "metadata": {},
   "source": [
    "### Compute costs of all \"valid\" execution plans\n",
    "#### New check for validity: automated cost of tasks in non-singleton partition must be less than human costs doing partition tasks separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3b358c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_boundary(adjacency_matrix, partition):\n",
    "    # create a matrix whose columns are nodes not in the partition and whose rows are nodes in the partition\n",
    "    # (subset adjacency matrix to outgoing edges of partition nodes --i.e., rows-- and incoming edges of non-partition nodes --i.e., columns.)\n",
    "    reduced_matrix = np.delete(adjacency_matrix, partition, axis=1) \n",
    "    reduced_matrix = reduced_matrix[partition, :]\n",
    "\n",
    "    # find nodes in partition w/ an edge to non-partition nodes\n",
    "    partition_boundary_tasks = [i for i in partition if np.any(reduced_matrix[partition.index(i), :])]\n",
    "\n",
    "    return partition_boundary_tasks\n",
    "\n",
    "\n",
    "def compute_partition_cost(adjacency_matrix, M_dict, A_dict, D_dict, AI_quality, partition):\n",
    "    # initialize task_done_by_human as False\n",
    "    # (only if partition is singleton and human cost <= automated cost partition is done manually)\n",
    "    task_done_by_human = False\n",
    "\n",
    "    # initialize partition boundary tasks as empty set []\n",
    "    partition_boundary_tasks = []\n",
    "\n",
    "    # if partition is a singleton \n",
    "    # pick minimum of human and management cost\n",
    "    if len(partition) == 1:\n",
    "        partition_is_valid = True # single-node partition is always valid\n",
    "\n",
    "        # calculate human cost\n",
    "        human_cost = sum(M_dict[key] for key in partition)\n",
    "\n",
    "        # calculate management cost\n",
    "        AI_cost = sum(A_dict[key] for key in partition)\n",
    "        difficulty = sum(D_dict[key] for key in partition)\n",
    "        management_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "        \n",
    "        # pick the minimum of the two\n",
    "        if human_cost < management_cost:\n",
    "            partition_cost = human_cost\n",
    "            task_done_by_human = True\n",
    "        else:\n",
    "            partition_cost = management_cost\n",
    "            partition_boundary_tasks = partition\n",
    "    \n",
    "\n",
    "    # if partition not a singleton \n",
    "    # calculate management cost and return if partition passes a sanity check\n",
    "    if len(partition) > 1:\n",
    "        # calculate human cost\n",
    "        human_cost = sum(M_dict[key] for key in partition)\n",
    "\n",
    "        # calculate management cost\n",
    "        # first get boundary tasks in partition\n",
    "        partition_boundary_tasks = get_partition_boundary(adjacency_matrix, partition)\n",
    "\n",
    "        # sanity check: no partition should have inner boundary of empty set\n",
    "        if len(partition_boundary_tasks) == 0:\n",
    "            # raise ValueError(f'Inner boundary of partition {partition} is empty set.')\n",
    "            return 100000000, [], [], False\n",
    "        \n",
    "        # if partition has at least one boundary task calculate management cost\n",
    "        # use boundary tasks for calculating management costs and partition tasks for difficulty\n",
    "        AI_cost = sum(A_dict[key] for key in partition_boundary_tasks)\n",
    "        difficulty = sum(D_dict[key] for key in partition)\n",
    "        management_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "\n",
    "        # sanity check for partition validity: \n",
    "        # if human cost < management cost partition is invalid (should not have been formed)\n",
    "        if human_cost < management_cost:\n",
    "            partition_cost = 100000000 # (value doesn't matter)\n",
    "            partition_is_valid = False\n",
    "        else:\n",
    "            partition_cost = management_cost\n",
    "            partition_is_valid = True\n",
    "            partition_boundary_tasks\n",
    "    \n",
    "    return partition_cost, partition_boundary_tasks, task_done_by_human, partition_is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "694fca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Random Thought: maybe better to sort valid_partitions on descending partition order to avoid recalculating single node partitions everytime? \n",
    "# tho the downside is that we have to first do the heavy calculations first...\n",
    "\n",
    "\n",
    "def execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, alpha):\n",
    "    execution_plan = []\n",
    "    execution_plan_augmented_tasks = []\n",
    "    execution_plan_human_tasks = []\n",
    "    execution_plan_cost = []\n",
    "    counter = 0\n",
    "    for scheme in valid_partitions:\n",
    "        # initialize scheme cost\n",
    "        # and partitions that are done manually\n",
    "        scheme_cost = 0\n",
    "        augmented_tasks = []\n",
    "        human_tasks = []\n",
    "        \n",
    "        for partition in scheme:\n",
    "            # calculate partition cost \n",
    "            partition_cost, partition_boundary_tasks, task_done_by_human, partition_is_valid = compute_partition_cost(adjacency_matrix, M_dict, A_dict, D_dict, alpha, partition)\n",
    "        \n",
    "            # if (automated) partition is invalid ignore partition scheme\n",
    "            # and stop calculating costs of further partitions\n",
    "            if not partition_is_valid:\n",
    "                break\n",
    "\n",
    "            if task_done_by_human:\n",
    "                human_tasks.append(partition)\n",
    "            \n",
    "            if not task_done_by_human:\n",
    "                for boundary_task in partition_boundary_tasks:\n",
    "                    augmented_tasks.append([boundary_task])\n",
    "\n",
    "            # if (automated) partition passes sanity check\n",
    "            # add this partition's cost to partition scheme cost\n",
    "            scheme_cost += partition_cost\n",
    "        \n",
    "        # if stopped because an (automated) partition wasn't valid\n",
    "        # ignore current partition scheme and continue\n",
    "        if not partition_is_valid:\n",
    "            continue\n",
    "        \n",
    "        # if partition scheme makes sense append costs\n",
    "        execution_plan.append(scheme)\n",
    "        execution_plan_augmented_tasks.append(augmented_tasks)\n",
    "        execution_plan_human_tasks.append(human_tasks)\n",
    "        execution_plan_cost.append(scheme_cost)\n",
    "\n",
    "    return execution_plan, execution_plan_augmented_tasks, execution_plan_human_tasks, execution_plan_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7354c2d",
   "metadata": {},
   "source": [
    "### Combine steps into a function to run a for loop over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e122fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAG_costMin(input_path, output_path, n=1000):\n",
    "    # set alpha as AI quality metric\n",
    "    epsilon = 1e-8\n",
    "    alpha_list = np.linspace(epsilon, 1-epsilon, n).tolist()\n",
    "\n",
    "\n",
    "\n",
    "    # read DAG\n",
    "    dag_df = pd.read_csv(input_path)\n",
    "\n",
    "    # remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "    if 'comment' in dag_df.columns:\n",
    "        dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "    # get task stats\n",
    "    tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "\n",
    "    tasks_stats = tasks_stats[['task', 'human_cost', 'management_cost', 'management_difficulty']]\n",
    "    tasks_stats.rename(columns={'management_difficulty': 'difficulty'}, inplace=True)\n",
    "\n",
    "    # tasks_stats = tasks_stats[['task', 'human_cost', 'machine_cost', 'completion_difficulty']]\n",
    "    # tasks_stats.rename(columns={'machine_cost': 'management_cost', 'completion_difficulty': 'difficulty'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # extract list of tasks and create a dictionary for indexing tasks\n",
    "    tasks_list = tasks_stats['task'].unique()\n",
    "    tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "    # create numpy array of adjacency matrix\n",
    "    adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    for _, row in dag_df.iterrows():\n",
    "        source_index = aux_dict[row['source']]\n",
    "        target_index = aux_dict[row['target']]\n",
    "        adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "\n",
    "    # add task_dict key and reset index\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    tasks_stats['dict_index'] = tasks_stats.apply(lambda row: aux_dict[row.task], axis=1)\n",
    "    tasks_stats = tasks_stats.sort_values(by='dict_index')\n",
    "    tasks_stats = tasks_stats.set_index('dict_index', drop=False)\n",
    "    tasks_stats.index.name = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # create dictionaries for human cost, management cost, and difficulty\n",
    "    M_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['human_cost']))\n",
    "    A_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['management_cost']))\n",
    "    D_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['difficulty']))\n",
    "\n",
    "\n",
    "    # get all valid subsets in a dict\n",
    "    valid_subsets_creation_start = time.time()\n",
    "    valid_subsets_dict = get_valid_DAG_subsets(adjacency_matrix)\n",
    "    valid_subsets_creation_time = (time.time() - valid_subsets_creation_start)/60\n",
    "    print(f\"valid subsets dictionary creation: {valid_subsets_creation_time:.2f} minutes\")\n",
    "    for key, value in valid_subsets_dict.items():\n",
    "        # Print the length of the value (which is a list in this case)\n",
    "        print(f\"Length of node {key} subsets': {len(value)}\")\n",
    "    \n",
    "    # generate valid combinations\n",
    "    generate_valid_partitions_start = time.time()\n",
    "    valid_partitions = generate_combinations(valid_subsets_dict, 0)\n",
    "    generate_valid_partitions_time = (time.time() - generate_valid_partitions_start)/60\n",
    "    print(f\"valid execution plans generation: {generate_valid_partitions_time:.2f} minutes\")\n",
    "\n",
    "    # Print stats\n",
    "    print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "\n",
    "    \n",
    "    # run once to get stat\n",
    "    execution_plan, execution_plan_augmented_tasks, execution_plan_human_tasks, execution_plan_cost = execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, 0.5)\n",
    "    print(f'Number of valid execution plans for alpha = 0.5 as example: {len(execution_plan)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    random.seed(1)\n",
    "    minimum_cost_list = []\n",
    "    number_of_optimal_schemes_list = []\n",
    "    optimal_execution_plan_list = []\n",
    "    optimal_plan_augmentedTasks_list = []\n",
    "    optimal_plan_augmentedTasks_count_list = []\n",
    "    optimal_plan_humanTasks_list = []\n",
    "    optimal_plan_humanTasks_count_list = []\n",
    "    for counter, alpha in enumerate(alpha_list):\n",
    "        # if counter % 100 == 0:\n",
    "        #     print(f'-- Running {counter}th alpha --')\n",
    "\n",
    "        # get list of execution plans and costs for this alpha\n",
    "        execution_plan, execution_plan_augmented_tasks, execution_plan_human_tasks, execution_plan_cost = execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, alpha)\n",
    "\n",
    "        # choose minimum\n",
    "        minimum_cost = min(execution_plan_cost)\n",
    "        minimum_cost_index = [index for index, value in enumerate(execution_plan_cost) if value == minimum_cost]\n",
    "\n",
    "        # in rare cases there are more than one optimal plan\n",
    "        if len(minimum_cost_index) > 1:\n",
    "            optimal_execution_scheme = [execution_plan[index] for index in minimum_cost_index]\n",
    "            optimal_execution_human_tasks = [execution_plan_human_tasks[index] for index in minimum_cost_index]\n",
    "            optimal_execution_augmented_tasks = [execution_plan_augmented_tasks[index] for index in minimum_cost_index]\n",
    "            # print(alpha)\n",
    "            # print(optimal_execution_scheme)\n",
    "            # print(optimal_execution_human_tasks)\n",
    "            # print(f'Multiple Execution Plans for alpha={alpha}')\n",
    "        else:\n",
    "            optimal_execution_scheme = execution_plan[minimum_cost_index[0]]\n",
    "            optimal_execution_human_tasks = execution_plan_human_tasks[minimum_cost_index[0]]\n",
    "            optimal_execution_augmented_tasks = execution_plan_augmented_tasks[minimum_cost_index[0]]\n",
    "\n",
    "        # append lists\n",
    "        minimum_cost_list.append(minimum_cost)\n",
    "        number_of_optimal_schemes_list.append(len(minimum_cost_index))\n",
    "        optimal_execution_plan_list.append(optimal_execution_scheme)\n",
    "        optimal_plan_augmentedTasks_list.append(optimal_execution_augmented_tasks)\n",
    "        optimal_plan_augmentedTasks_count_list.append(len(optimal_execution_augmented_tasks))\n",
    "        optimal_plan_humanTasks_list.append(optimal_execution_human_tasks)\n",
    "        optimal_plan_humanTasks_count_list.append(len(optimal_execution_human_tasks))\n",
    "\n",
    "    # save outputs\n",
    "    output_df = pd.DataFrame({\n",
    "        'alpha': alpha_list,\n",
    "        'optimal_schemes_count': number_of_optimal_schemes_list,\n",
    "        'cost': minimum_cost_list,\n",
    "        'optimal_scheme': optimal_execution_plan_list,\n",
    "        'optimal_scheme_augmented_tasks': optimal_plan_augmentedTasks_list,\n",
    "        'augmented_tasks_count': optimal_plan_augmentedTasks_count_list,\n",
    "        'optimal_scheme_human_tasks': optimal_plan_humanTasks_list,\n",
    "        'human_tasks_count': optimal_plan_humanTasks_count_list\n",
    "    })\n",
    "    output_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ec907",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ee7f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of alphas to sweep over: 10\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# number of alphas to sweep over\n",
    "n = 10\n",
    "print(f'Number of alphas to sweep over: {n}')\n",
    "\n",
    "onet_data_path = f'{data_path}/data/onet_occupations_yearly.csv'\n",
    "\n",
    "# occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "#                    'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "#                    'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "#                    'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists', \n",
    "#                    'personalCareAides', 'proofreadersAndCopyMarkers', 'chiropractors', \n",
    "#                    'shippingReceivingAndInventoryClerks', 'cooksShortOrder', 'orthodontists',\n",
    "#                    'subwayAndStreetcarOperators', 'packersAndPackagersHand', 'hoistAndWinchOperators', \n",
    "#                    'forgingMachineSettersOperatorsAndTenders', 'avionicsTechnicians', 'dishwashers', \n",
    "#                    'dispatchersExceptPoliceFireAndAmbulance', 'familyMedicinePhysicians', 'MachineFeedersAndOffbearers'\n",
    "#                    ]\n",
    "\n",
    "# occupation_list = ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators'\n",
    "#                    ]\n",
    "\n",
    "# removed \"personalCareAides\"\n",
    "occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "                   'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "                   'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists', \n",
    "                   'proofreadersAndCopyMarkers', 'chiropractors', \n",
    "                   'shippingReceivingAndInventoryClerks', 'cooksShortOrder', 'orthodontists',\n",
    "                   'subwayAndStreetcarOperators', 'packersAndPackagersHand', 'hoistAndWinchOperators', \n",
    "                   'forgingMachineSettersOperatorsAndTenders', 'avionicsTechnicians', 'dishwashers', \n",
    "                   'dispatchersExceptPoliceFireAndAmbulance', 'familyMedicinePhysicians', 'MachineFeedersAndOffbearers'\n",
    "                   ]\n",
    "\n",
    "# occupation_list = ['travelAgents']\n",
    "\n",
    "occupation_list = ['audiovisualEquipmentInstallerAndRepairers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3a99b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Running: audiovisualEquipmentInstallerAndRepairers ----------------------\n",
      "Number of non-target tasks: 11\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "Length of node 0 subsets': 1019\n",
      "Length of node 1 subsets': 510\n",
      "Length of node 2 subsets': 256\n",
      "Length of node 3 subsets': 128\n",
      "Length of node 4 subsets': 56\n",
      "Length of node 5 subsets': 21\n",
      "Length of node 6 subsets': 12\n",
      "Length of node 7 subsets': 8\n",
      "Length of node 8 subsets': 2\n",
      "Length of node 9 subsets': 1\n",
      "Length of node 10 subsets': 1\n",
      "valid execution plans generation: 0.40 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 426772\n",
      "Number of valid execution plans for alpha = 0.5 as example: 4162\n",
      "\n",
      "audiovisualEquipmentInstallerAndRepairers Naive DAG runtime: 135.21 seconds\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "Length of node 0 subsets': 459\n",
      "Length of node 1 subsets': 208\n",
      "Length of node 2 subsets': 156\n",
      "Length of node 3 subsets': 72\n",
      "Length of node 4 subsets': 44\n",
      "Length of node 5 subsets': 13\n",
      "Length of node 6 subsets': 10\n",
      "Length of node 7 subsets': 3\n",
      "Length of node 8 subsets': 2\n",
      "Length of node 9 subsets': 1\n",
      "Length of node 10 subsets': 1\n",
      "valid execution plans generation: 0.02 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 19294\n",
      "Number of valid execution plans for alpha = 0.5 as example: 360\n",
      "\n",
      "audiovisualEquipmentInstallerAndRepairers Conditioned Naive DAG runtime: 6.03 seconds\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "Length of node 0 subsets': 948\n",
      "Length of node 1 subsets': 504\n",
      "Length of node 2 subsets': 200\n",
      "Length of node 3 subsets': 128\n",
      "Length of node 4 subsets': 32\n",
      "Length of node 5 subsets': 13\n",
      "Length of node 6 subsets': 7\n",
      "Length of node 7 subsets': 8\n",
      "Length of node 8 subsets': 2\n",
      "Length of node 9 subsets': 1\n",
      "Length of node 10 subsets': 1\n",
      "valid execution plans generation: 0.20 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 223341\n",
      "Number of valid execution plans for alpha = 0.5 as example: 2531\n",
      "\n",
      "audiovisualEquipmentInstallerAndRepairers First-Last Task DAG runtime: 224.90 seconds\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "Length of node 0 subsets': 249\n",
      "Length of node 1 subsets': 120\n",
      "Length of node 2 subsets': 36\n",
      "Length of node 3 subsets': 20\n",
      "Length of node 4 subsets': 8\n",
      "Length of node 5 subsets': 1\n",
      "Length of node 6 subsets': 3\n",
      "Length of node 7 subsets': 1\n",
      "Length of node 8 subsets': 2\n",
      "Length of node 9 subsets': 1\n",
      "Length of node 10 subsets': 1\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 4788\n",
      "Number of valid execution plans for alpha = 0.5 as example: 552\n",
      "\n",
      "audiovisualEquipmentInstallerAndRepairers Conditioned First-Last Task DAG runtime: 1.55 seconds\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "Length of node 0 subsets': 1023\n",
      "Length of node 1 subsets': 512\n",
      "Length of node 2 subsets': 256\n",
      "Length of node 3 subsets': 128\n",
      "Length of node 4 subsets': 64\n",
      "Length of node 5 subsets': 28\n",
      "Length of node 6 subsets': 15\n",
      "Length of node 7 subsets': 8\n",
      "Length of node 8 subsets': 4\n",
      "Length of node 9 subsets': 2\n",
      "Length of node 10 subsets': 1\n",
      "valid execution plans generation: 1.67 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 589564\n",
      "Number of valid execution plans for alpha = 0.5 as example: 7377\n",
      "\n",
      "audiovisualEquipmentInstallerAndRepairers Partitioned DAG runtime: 767.41 seconds\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "Length of node 0 subsets': 483\n",
      "Length of node 1 subsets': 223\n",
      "Length of node 2 subsets': 102\n",
      "Length of node 3 subsets': 44\n",
      "Length of node 4 subsets': 26\n",
      "Length of node 5 subsets': 11\n",
      "Length of node 6 subsets': 10\n",
      "Length of node 7 subsets': 5\n",
      "Length of node 8 subsets': 3\n",
      "Length of node 9 subsets': 2\n",
      "Length of node 10 subsets': 1\n",
      "valid execution plans generation: 0.02 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 16904\n",
      "Number of valid execution plans for alpha = 0.5 as example: 404\n",
      "\n",
      "audiovisualEquipmentInstallerAndRepairers Conditioned Partitioned DAG runtime: 5.62 seconds\n",
      "\n",
      "\n",
      "************* audiovisualEquipmentInstallerAndRepairers runtime: 19.02 minutes *************\n",
      "\n",
      "runtime since start: 19.02 minutes\n",
      "\n",
      "\n",
      "\n",
      "Total Runtime: 19.02 minutes\n"
     ]
    }
   ],
   "source": [
    "num_tasks_current = 0\n",
    "num_tasks_previous = 0\n",
    "for occupation in occupation_list:\n",
    "    print(f'\\n---------------------- Running: {occupation} ----------------------')\n",
    "    occupation_start_time = time.time()\n",
    "\n",
    "    # generate occupation-specific strings\n",
    "    GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)\n",
    "\n",
    "\n",
    "    # Get occupation tasks to create all possible partitions\n",
    "    tasks = get_tasks(onet_data_path, occupation_code)\n",
    "    num_tasks_current = len(tasks)\n",
    "    print(f'Number of non-target tasks: {num_tasks_current}')\n",
    "\n",
    "    # Manual DAG\n",
    "    M_input_path = f'{occupation_folder}/{occupation}_M_DAG_df.csv'\n",
    "    M_output_path = f'{occupation_folder}/{occupation}_costMin_M.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    N_input_path = f'{occupation_folder}/{occupation}_N_GPT_DAG_df.csv'\n",
    "    N_output_path = f'{occupation_folder}/{occupation}_costMin_N.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    CN_input_path = f'{occupation_folder}/{occupation}_CN_GPT_DAG_df.csv'\n",
    "    CN_output_path = f'{occupation_folder}/{occupation}_costMin_CN.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    FLT_input_path = f'{occupation_folder}/{occupation}_FLT_GPT_DAG_df.csv'\n",
    "    FLT_output_path = f'{occupation_folder}/{occupation}_costMin_FLT.csv'\n",
    "\n",
    "    # Conditioned First Last Task DAG\n",
    "    CFLT_input_path = f'{occupation_folder}/{occupation}_CFLT_GPT_DAG_df.csv'\n",
    "    CFLT_output_path = f'{occupation_folder}/{occupation}_costMin_CFLT.csv'\n",
    "\n",
    "    # Partitioned DAG\n",
    "    P_input_path = f'{occupation_folder}/{occupation}_P_GPT_DAG_df.csv'\n",
    "    P_output_path = f'{occupation_folder}/{occupation}_costMin_P.csv'\n",
    "\n",
    "    # Conditioned Partitioned DAG\n",
    "    CP_input_path = f'{occupation_folder}/{occupation}_CP_GPT_DAG_df.csv'\n",
    "    CP_output_path = f'{occupation_folder}/{occupation}_costMin_CP.csv'\n",
    "    \n",
    "\n",
    "\n",
    "    # create list of all DAGs\n",
    "    if occupation in ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators']:\n",
    "        DAG_indicator_list = ['Manual DAG', 'Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [M_input_path, N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_paths_list = [M_output_path, N_output_path, CN_output_path, FLT_output_path, CFLT_output_path, P_output_path, CP_output_path]\n",
    "    else:\n",
    "        DAG_indicator_list = ['Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_paths_list = [N_output_path, CN_output_path, FLT_output_path, CFLT_output_path, P_output_path, CP_output_path]\n",
    "\n",
    "\n",
    "    for DAG_indicator, input_path, output_path in zip(DAG_indicator_list, input_paths_list, output_paths_list):\n",
    "        print(f'\\n-------Running: {occupation} - {DAG_indicator}-------')\n",
    "        \n",
    "        DAG_start_time = time.time()\n",
    "        DAG_costMin(input_path, output_path, n)\n",
    "        DAG_end_time = time.time()\n",
    "\n",
    "        DAG_execution_time = DAG_end_time - DAG_start_time\n",
    "        print(f\"\\n{occupation} {DAG_indicator} runtime: {DAG_execution_time:.2f} seconds\")\n",
    "\n",
    "    occupation_end_time = time.time()\n",
    "    occupation_execution_time = (occupation_end_time - occupation_start_time)/60\n",
    "    print(f\"\\n\\n************* {occupation} runtime: {occupation_execution_time:.2f} minutes *************\")\n",
    "    runtime_since_start = (time.time() - start_time)/60\n",
    "    print(f\"\\nruntime since start: {runtime_since_start:.2f} minutes\\n\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"\\n\\nTotal Runtime: {execution_time:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
