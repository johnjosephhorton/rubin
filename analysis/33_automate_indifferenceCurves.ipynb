{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)\n",
    "\n",
    "with open('functions.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9b99db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['caffeinate']>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run caffeinate in the background to prevent sleep\n",
    "subprocess.Popen(['caffeinate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymanshahidi':\n",
    "    main_folder_path = '/Users/peymanshahidi/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a49d43",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb953c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def partitions(set_):\n",
    "    if not set_:\n",
    "        yield []\n",
    "        return\n",
    "    for i in range(1, len(set_) + 1):\n",
    "        for part in combinations(set_, i):\n",
    "            remaining = set(set_) - set(part)\n",
    "            if not remaining:\n",
    "                yield [list(part)]\n",
    "            else:\n",
    "                for b in partitions(list(remaining)):\n",
    "                    yield [list(part)] + b\n",
    "\n",
    "def generate_unique_partitions(numbers):\n",
    "    all_partitions = set()\n",
    "    for partition in partitions(numbers):\n",
    "        # Create a frozenset of frozensets to make each partition hashable and order-independent\n",
    "        partition_set = frozenset(frozenset(part) for part in partition)\n",
    "        all_partitions.add(partition_set)\n",
    "    \n",
    "    # Convert the frozensets back to lists for the final output\n",
    "    unique_partitions = [list(map(list, partition)) for partition in all_partitions]\n",
    "\n",
    "    # Sort elements\n",
    "    unique_partitions = sorted([sorted(x) for x in unique_partitions], key=len)\n",
    "    return unique_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c64d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_connected(matrix):\n",
    "    # Number of nodes in the matrix\n",
    "    num_nodes = matrix.shape[0]\n",
    "    \n",
    "    # Visited array to keep track of visited nodes\n",
    "    visited = np.zeros(num_nodes, dtype=bool)\n",
    "    \n",
    "    # Helper function to perform DFS\n",
    "    def dfs(node):\n",
    "        visited[node] = True\n",
    "        # Visit all the neighbors of the current node\n",
    "        for neighbor in range(num_nodes):\n",
    "            if matrix[node, neighbor] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "            elif matrix[neighbor, node] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "    \n",
    "    # Start DFS from the first node (node 0)\n",
    "    dfs(0)\n",
    "    \n",
    "    # If all nodes are visited, the matrix is connected\n",
    "    return np.all(visited)\n",
    "\n",
    "\n",
    "def validate_partition_using_connectedness(adjacency_matrix, tasks_list):\n",
    "    # Return valid if Singleton\n",
    "    if len(tasks_list) == 1:\n",
    "        return True\n",
    "    # Check if partition forms connected graph\n",
    "    else:\n",
    "        # Subset original adjacency matrix\n",
    "        subset_matrix = adjacency_matrix[np.ix_(tasks_list, tasks_list)]\n",
    "\n",
    "        # check if subset matrix is a connected graph\n",
    "        subset_matrix_connected = is_connected(subset_matrix)\n",
    "\n",
    "        # return true if connected and false otherwise\n",
    "        return subset_matrix_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb1d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_boundary(adjacency_matrix, partition):\n",
    "    # create a matrix whose columns are nodes not in the partition and whose rows are nodes in the partition\n",
    "    # (subset adjacency matrix to outgoing edges of partition nodes --i.e., rows-- and incoming edges of non-partition nodes --i.e., columns.)\n",
    "    reduced_matrix = np.delete(adjacency_matrix, partition, axis=1) \n",
    "    reduced_matrix = reduced_matrix[partition, :]\n",
    "\n",
    "    # find nodes in partition w/ an edge to non-partition nodes\n",
    "    partition_boundary_tasks = [i for i in partition if np.any(reduced_matrix[partition.index(i), :])]\n",
    "\n",
    "    return partition_boundary_tasks\n",
    "\n",
    "\n",
    "def compute_plan_cost(adjacency_matrix, \n",
    "                      human_labor_dict, \n",
    "                      machine_automation_dict, \n",
    "                      machine_management_dict, \n",
    "                      management_difficulty_dict, \n",
    "                      automation_difficulty_dict,\n",
    "                      AI_quality, execution_plan, human_tasks):\n",
    "    # initialize costs\n",
    "    human_labor_cost = 0\n",
    "    management_cost = 0\n",
    "    automation_cost = 0\n",
    "\n",
    "    managed_tasks_list = []\n",
    "    automated_tasks_list = []\n",
    "    for partition in execution_plan:\n",
    "        if len(partition) == 1:\n",
    "            if partition[0] in human_tasks:\n",
    "                labor_cost = sum(human_labor_dict[key] for key in partition)\n",
    "                human_labor_cost += labor_cost\n",
    "            else:\n",
    "                managed_tasks_list.append(partition[0])\n",
    "                machine_management_cost = sum(machine_management_dict[key] for key in partition)\n",
    "                management_difficulty = sum(management_difficulty_dict[key] for key in partition)\n",
    "                management_cost += machine_management_cost * (AI_quality ** (-1 * management_difficulty))\n",
    "        else:\n",
    "            # determine which tasks are automated and which tasks are managed\n",
    "            managed_tasks = get_partition_boundary(adjacency_matrix, partition)\n",
    "            automated_tasks = [task for task in partition if task not in managed_tasks]\n",
    "            managed_tasks_list.append(managed_tasks)\n",
    "            automated_tasks_list.append(automated_tasks)\n",
    "\n",
    "            # calculate management cost of partition\n",
    "            machine_management_cost = sum(machine_management_dict[key] for key in managed_tasks)\n",
    "            management_difficulty = sum(management_difficulty_dict[key] for key in managed_tasks)\n",
    "            management_cost += machine_management_cost * (AI_quality ** (-1 * management_difficulty))\n",
    "\n",
    "            # calculate labor cost of partition\n",
    "            machine_automation_cost = sum(machine_automation_dict[key] for key in automated_tasks)\n",
    "            automation_difficulty = sum(automation_difficulty_dict[key] for key in automated_tasks)\n",
    "            automation_cost += machine_automation_cost * (AI_quality ** (-1 * automation_difficulty))\n",
    "\n",
    "    # rounding\n",
    "    human_labor_cost = np.round(human_labor_cost, 4)\n",
    "    management_cost = np.round(management_cost, 4)\n",
    "    automation_cost = np.round(automation_cost, 4)\n",
    "\n",
    "    return human_labor_cost, management_cost, automation_cost, managed_tasks_list, automated_tasks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c91ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dominated_points(df):\n",
    "    # Sort the DataFrame by labor_cost and then by automation_cost\n",
    "    df_sorted = df.sort_values(by=['labor_cost', 'automation_cost']).reset_index(drop=True)\n",
    "\n",
    "    # Find index of lowest automation cost for each labor cost\n",
    "    idx = df_sorted.groupby('labor_cost')['automation_cost'].idxmin()\n",
    "    if len(idx) == 1:\n",
    "        idx = idx[0]\n",
    "    df_sorted = df_sorted.loc[idx]\n",
    "\n",
    "    # Find index of lowest labor cost for each automation cost\n",
    "    idx = df_sorted.groupby('automation_cost')['labor_cost'].idxmin()\n",
    "    if len(idx) == 1:\n",
    "        idx = idx[0]\n",
    "    df_sorted = df_sorted.loc[idx]\n",
    "\n",
    "    # Initialize an empty list to store the non-dominated points\n",
    "    non_dominated_points = []\n",
    "    \n",
    "    # Iterate through each point in the DataFrame\n",
    "    for i, point in df_sorted.iterrows():\n",
    "        # Check if this point is dominated by any other point\n",
    "        dominated = False\n",
    "        for j, other_point in df_sorted.iterrows():\n",
    "            if (other_point['labor_cost'] < point['labor_cost']) and (other_point['automation_cost'] < point['automation_cost']):\n",
    "                dominated = True\n",
    "                break\n",
    "        # If the point is not dominated, add it to the list\n",
    "        if not dominated:\n",
    "            non_dominated_points.append(point)\n",
    "    \n",
    "    # Return the non-dominated points as a DataFrame\n",
    "    return pd.DataFrame(non_dominated_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8d2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def calc_CES_params(df, labor_wage):\n",
    "    ###### hacky way of getting around zero input values ######\n",
    "    df['labor_cost'] = df['labor_cost'].apply(lambda x: x + epsilon if x == 0 else x)\n",
    "    df['automation_cost'] = df['automation_cost'].apply(lambda x: x + epsilon if x == 0 else x)\n",
    "\n",
    "    # Prepare data for regression\n",
    "    df['const'] = 1\n",
    "    df['log_automation_over_humanLabor'] = np.log(df['automation_cost'] / df['labor_cost'])\n",
    "    df['log_laborWage_over_AIrentalCost'] = np.log(labor_wage / df['AI_rental_cost'])\n",
    "\n",
    "    # Run regression\n",
    "    X = df[['const', 'log_automation_over_humanLabor']]\n",
    "    Y = df['log_laborWage_over_AIrentalCost']\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "\n",
    "    # Get elasticity of technical substitution\n",
    "    beta_0, beta_1 = model.params\n",
    "\n",
    "    sigma = 1 / beta_1\n",
    "    gamma = 1 / (1 + np.exp(beta_0))\n",
    "\n",
    "    return sigma, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74eb8f3",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7edaf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_indiff_curve(alpha_list, input_path, output_suffix):\n",
    "\n",
    "    # read DAG\n",
    "    dag_df = pd.read_csv(input_path)\n",
    "\n",
    "    # remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "    if 'comment' in dag_df.columns:\n",
    "        dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "\n",
    "    # get task stats and create a list\n",
    "    tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "    tasks_list = tasks_stats['task'].unique()\n",
    "\n",
    "    tasks_stats['machine_cost'] = 1\n",
    "\n",
    "\n",
    "    # create a dictionary for indexing tasks\n",
    "    tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "    # create numpy array of adjacency matrix\n",
    "    adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    for _, row in dag_df.iterrows():\n",
    "        source_index = aux_dict[row['source']]\n",
    "        target_index = aux_dict[row['target']]\n",
    "        adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # add task_dict key and reset index\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    tasks_stats['dict_index'] = tasks_stats.apply(lambda row: aux_dict[row.task], axis=1)\n",
    "    tasks_stats = tasks_stats.sort_values(by='dict_index')\n",
    "    tasks_stats = tasks_stats.set_index('dict_index', drop=False)\n",
    "    tasks_stats.index.name = None\n",
    "\n",
    "\n",
    "    # Generate list of numbers for non-\"Target\" tasks in occupation\n",
    "    tasks_list_numbers = list(range(len(tasks_list)-1)) # -1 for \"Target\" task: don't want to include it in task partitions as it's its own separate partition\n",
    "\n",
    "    # Generate all possible partitioning schemes\n",
    "    all_partitions = generate_unique_partitions(tasks_list_numbers)\n",
    "\n",
    "\n",
    "\n",
    "    # Get valid partitioning schemes from all possible partitions to cut computation load\n",
    "    valid_partitions = []\n",
    "    for scheme in all_partitions:\n",
    "        # Set valid partitions count to 0\n",
    "        valid_partition_count = 0\n",
    "        for partition in scheme:\n",
    "            valid_partition = validate_partition_using_connectedness(adjacency_matrix, partition)\n",
    "            if valid_partition:\n",
    "                valid_partition_count += 1\n",
    "        \n",
    "        # If number of valid partitions within a partition scheme is equal to \n",
    "        # number of partitions in partition scheme then partition scheme is valid\n",
    "        if valid_partition_count == len(scheme):\n",
    "            valid_partitions.append(scheme)\n",
    "\n",
    "    # Print stats\n",
    "    print(f'Number of all possible partitioning schemes: {len(all_partitions)}')\n",
    "    print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "\n",
    "    # get how many \"singleton\" partitions there are in valid partition\n",
    "    valid_execution_plans = pd.DataFrame()\n",
    "    for my_valid_partition in valid_partitions:\n",
    "        singleton_partitions = [lst[0] for lst in my_valid_partition if len(lst) == 1]\n",
    "\n",
    "        # get the power set of \"singleton\" partitions\n",
    "        # goal is to generate ways singleton tasks can be done by human or AI\n",
    "        all_combinations = [[]]\n",
    "        for r in range(1, len(singleton_partitions) + 1):\n",
    "            combinations_r = itertools.combinations(singleton_partitions, r)\n",
    "            all_combinations.extend(combinations_r)\n",
    "\n",
    "        # Convert the combinations to a list of lists (optional)\n",
    "        all_combinations = [list(comb) for comb in all_combinations]\n",
    "        all_combinations\n",
    "\n",
    "        # repeat my_valid_partition for each combination in all_combinations to create a dataframe later\n",
    "        my_valid_partition_repeated = [my_valid_partition for _ in range(len(all_combinations))]\n",
    "        aux_df = pd.DataFrame({'execution_plan': my_valid_partition_repeated, \n",
    "                            'human_tasks': all_combinations})\n",
    "        \n",
    "        # append to valid_execution_plans\n",
    "        valid_execution_plans = pd.concat([valid_execution_plans, aux_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    # create dictionaries for human cost, management cost, and difficulty\n",
    "    human_labor_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['human_cost']))\n",
    "    machine_automation_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['machine_cost']))\n",
    "    machine_management_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['management_cost']))\n",
    "\n",
    "    management_difficulty_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['management_difficulty']))\n",
    "    automation_difficulty_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['completion_difficulty']))\n",
    "\n",
    "\n",
    "\n",
    "    # calculate plan costs for each alpha\n",
    "    indiff_df = pd.DataFrame()\n",
    "    for counter, alpha in enumerate(alpha_list):\n",
    "        my_alpha_indiff_df = pd.DataFrame()\n",
    "        for execution_plan, human_tasks in zip(valid_execution_plans['execution_plan'], valid_execution_plans['human_tasks']):\n",
    "            # calculate plan costs\n",
    "            human_labor_cost, management_cost, automation_cost, managed_tasks, automated_tasks = compute_plan_cost(adjacency_matrix, \n",
    "                                                                                                                human_labor_dict, \n",
    "                                                                                                                machine_automation_dict, \n",
    "                                                                                                                machine_management_dict, \n",
    "                                                                                                                management_difficulty_dict, \n",
    "                                                                                                                automation_difficulty_dict,\n",
    "                                                                                                                alpha, execution_plan, human_tasks)\n",
    "\n",
    "            # create a dataframe to store execution plan costs\n",
    "            aux_df = pd.DataFrame({'alpha': [alpha], \n",
    "                                'execution_plan': [execution_plan],\n",
    "                                'human_tasks': [human_tasks],\n",
    "                                'managed_tasks': [managed_tasks],\n",
    "                                'automated_tasks': [automated_tasks],\n",
    "                                'human_labor_cost': [human_labor_cost],\n",
    "                                'management_cost': [management_cost],\n",
    "                                'automation_cost': [automation_cost]\n",
    "                                })\n",
    "            \n",
    "            # append to execution_plan_costs_df\n",
    "            my_alpha_indiff_df = pd.concat([my_alpha_indiff_df, aux_df], ignore_index=True)\n",
    "        \n",
    "        # append to master dataframe\n",
    "        indiff_df = pd.concat([indiff_df, my_alpha_indiff_df], ignore_index=True)\n",
    "\n",
    "    indiff_df_orig = indiff_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "    indiff_df = indiff_df_orig.copy()\n",
    "\n",
    "    # min wage = $15 / hour\n",
    "    labor_wage = 1\n",
    "\n",
    "    # management cost: rental cost of capital\n",
    "    AI_rental_cost = 1\n",
    "\n",
    "    # avg number of tokens per prompt\n",
    "    #avg_num_prompts_list = [1, 5, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "    #avg_num_prompts = 10000\n",
    "\n",
    "    # GPT-4 cost per 1 million tokens: $30\n",
    "    #API_cost = avg_num_prompts * 30 / 1e6 # $ per prompt\n",
    "\n",
    "    # convert machine labor cost to minutes and add to human labor cost in minutes\n",
    "    indiff_df['labor_cost'] = indiff_df.apply(lambda row: row['human_labor_cost'] + (row['management_cost']), axis=1)\n",
    "\n",
    "    # calculate total cost\n",
    "    indiff_df['total_cost'] = indiff_df.apply(lambda row: row['labor_cost'] * labor_wage + row['automation_cost'] * AI_rental_cost, axis=1)\n",
    "\n",
    "    # find optimal execution plan\n",
    "    indiff_df['min_total_cost_flag'] = indiff_df.groupby('alpha')['total_cost'].transform(lambda x: x == x.min())\n",
    "\n",
    "\n",
    "\n",
    "    # Get lower envelope of points\n",
    "    lower_envelope_df = pd.DataFrame()\n",
    "    for my_alpha in alpha_list:\n",
    "        # subset corresponding alpha's data from master dataset\n",
    "        my_alpha_indiff_df = indiff_df[indiff_df['alpha']==my_alpha]\n",
    "\n",
    "        # drop duplicates, if any exist\n",
    "        my_alpha_indiff_df = my_alpha_indiff_df.drop_duplicates(subset=['labor_cost', 'automation_cost'])\n",
    "\n",
    "        # get lower envelope for current alpha\n",
    "        my_alpha_indiff_lower_envelope_df = remove_dominated_points(my_alpha_indiff_df)\n",
    "\n",
    "        # append to lower envelope dataframe\n",
    "        lower_envelope_df = pd.concat([lower_envelope_df, my_alpha_indiff_lower_envelope_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Get multiple labor-management wage pairs and find optimal plan for each\n",
    "    n = 500 # number of different pairs / 2\n",
    "    cost_max = 10000\n",
    "\n",
    "    # for a fixed labor_wage generate n different AI_rental_cost values\n",
    "    list_1 = list(np.linspace(1, cost_max, n))\n",
    "    list_2 = [1 / value for value in list_1]\n",
    "    AI_rental_cost_list = list_1[1:] + list_2\n",
    "    AI_rental_cost_list = [value * labor_wage for value in AI_rental_cost_list]\n",
    "\n",
    "    # Fix labor_wage and vary AI_rental_cost \n",
    "    optimal_plans_df = pd.DataFrame()\n",
    "    for AI_rental_cost in AI_rental_cost_list:\n",
    "        # Calculate total cost\n",
    "        lower_envelope_df['total_cost'] = lower_envelope_df.apply(lambda row: row['labor_cost'] * labor_wage + row['automation_cost'] * AI_rental_cost, axis=1)\n",
    "\n",
    "        # Find optimal execution plan given labor_wage and AI_rental_cost\n",
    "        lower_envelope_df['min_total_cost_flag'] = lower_envelope_df.groupby('alpha')['total_cost'].transform(lambda x: x == x.min())\n",
    "\n",
    "        # Save optimal plan to master dataframe\n",
    "        aux_df = lower_envelope_df[lower_envelope_df.min_total_cost_flag]\n",
    "        aux_df['AI_rental_cost'] = AI_rental_cost\n",
    "        optimal_plans_df = pd.concat([optimal_plans_df, aux_df], ignore_index=True)\n",
    "\n",
    "    # Sort by alpha and AI_rental_cost\n",
    "    optimal_plans_df = optimal_plans_df.sort_values(by=['alpha', 'AI_rental_cost']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Run regression to find elasticity of technical substitution\n",
    "    ETS_df = pd.DataFrame()\n",
    "    for my_alpha in alpha_list:\n",
    "        # Subset data for current alpha\n",
    "        my_alpha_regression_df = optimal_plans_df[optimal_plans_df.alpha == my_alpha]\n",
    "\n",
    "        # Calculate elasticity of technical substitution\n",
    "        sigma, gamma = calc_CES_params(my_alpha_regression_df, labor_wage)\n",
    "\n",
    "        # Append to dataframe\n",
    "        aux_df = pd.DataFrame({'alpha': [my_alpha], 'sigma': [sigma], 'gamma': [gamma]})\n",
    "        ETS_df = pd.concat([ETS_df, aux_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot version 1a: no scaling with lower envelope points colored red - linear axes\n",
    "    # Create a 2x5 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(22, 18))\n",
    "\n",
    "    # Iterate over each subplot position and add a plot\n",
    "    for i in range(3): \n",
    "        for j in range(5): \n",
    "            alpha_index = (i % 5) * 5 + j\n",
    "            my_alpha = alpha_list[alpha_index] # ignore epsilon\n",
    "\n",
    "            # Plot full data for given alpha\n",
    "            my_alpha_indiff_df = indiff_df[indiff_df['alpha']==my_alpha]\n",
    "            axs[i, j].scatter(my_alpha_indiff_df.labor_cost, my_alpha_indiff_df.automation_cost, \n",
    "                            s=15, \n",
    "                            label=f'Dominated Plans (' + r'$n$' + f'={my_alpha_indiff_df.shape[0]})')\n",
    "            \n",
    "            # Plot lower envelope points in red\n",
    "            my_alpha_lower_envelope_df = lower_envelope_df[lower_envelope_df['alpha']==my_alpha]\n",
    "            axs[i, j].scatter(my_alpha_lower_envelope_df.labor_cost, my_alpha_lower_envelope_df.automation_cost, \n",
    "                            s=20, \n",
    "                            color='red', \n",
    "                            label='Lower Envelope (' + r'$n$' + f'={my_alpha_lower_envelope_df.shape[0]})')\n",
    "            \n",
    "            # Add elasticity and labor share for current alpha\n",
    "            my_alpha_sigma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'sigma'].values[0]\n",
    "            my_alpha_gamma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'gamma'].values[0]\n",
    "            \n",
    "            sigma_display = r'$\\sigma$' + f'={my_alpha_sigma:.2f}'\n",
    "            gamma_display = r'$\\gamma$' + f'={my_alpha_gamma:.4f}'\n",
    "            axs[i, j].text(0.95, 0.85, sigma_display, transform=axs[i, j].transAxes, fontsize=12, verticalalignment='top', horizontalalignment='right', color='red')\n",
    "            axs[i, j].text(0.95, 0.8, gamma_display, transform=axs[i, j].transAxes, fontsize=12, verticalalignment='top', horizontalalignment='right', color='darkred')\n",
    "            \n",
    "            # other plot aesthetics\n",
    "            axs[i, j].title.set_text(r'$\\alpha$' + f'={np.round(my_alpha,4)}')\n",
    "            if i == 2:\n",
    "                axs[i, j].set_xlabel('Human Labor + AI Management (minutes)')\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel('Machine Automation (API calls)')\n",
    "            axs[i, j].legend(loc = 'upper right')\n",
    "\n",
    "    fig.suptitle(f'Indifference Curves for {plot_title_occupation}: First-Last-Task DAG\\n' +\n",
    "                '(Scale-varying Axes)' +\n",
    "                '\\n' + \n",
    "                '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n' + \n",
    "                r'CES Production Function: $F(K,L) = [\\gamma K^{\\frac{\\sigma-1}{\\sigma}} + (1-\\gamma) L^{\\frac{\\sigma-1}{\\sigma}}]^{\\frac{\\sigma}{\\sigma-1}} $' + \n",
    "                '\\n' +\n",
    "                '\\n',\n",
    "                fontsize=16)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_linear_{output_suffix}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Plot version 2a: scaling with only lower envelope points - linear axes\n",
    "    # Create a 2x5 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(22, 18))\n",
    "\n",
    "    # Iterate over each subplot position and add a plot\n",
    "    for i in range(3): \n",
    "        for j in range(5): \n",
    "            alpha_index = (i % 5) * 5 + j\n",
    "            my_alpha = alpha_list[alpha_index] # ignore epsilon\n",
    "\n",
    "            # Plot lower envelope points in red\n",
    "            my_alpha_lower_envelope_df = lower_envelope_df[lower_envelope_df['alpha']==my_alpha]\n",
    "            axs[i, j].scatter(my_alpha_lower_envelope_df.labor_cost, my_alpha_lower_envelope_df.automation_cost, \n",
    "                            s=20, \n",
    "                            label='Lower Envelope (' + r'$n$' + f'={my_alpha_lower_envelope_df.shape[0]})')\n",
    "            \n",
    "            # Add elasticity and labor share for current alpha\n",
    "            my_alpha_sigma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'sigma'].values[0]\n",
    "            my_alpha_gamma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'gamma'].values[0]\n",
    "            \n",
    "            sigma_display = r'$\\sigma$' + f'={my_alpha_sigma:.2f}'\n",
    "            gamma_display = r'$\\gamma$' + f'={my_alpha_gamma:.4f}'\n",
    "            axs[i, j].text(0.95, 0.9, sigma_display, transform=axs[i, j].transAxes, fontsize=12, \n",
    "                        verticalalignment='top', horizontalalignment='right', color='red')\n",
    "            axs[i, j].text(0.95, 0.85, gamma_display, transform=axs[i, j].transAxes, fontsize=12, \n",
    "                        verticalalignment='top', horizontalalignment='right', color='darkred')\n",
    "            \n",
    "            # other plot aesthetics\n",
    "            axs[i, j].title.set_text(r'$\\alpha$' + f'={np.round(my_alpha,4)}')\n",
    "            if i == 2:\n",
    "                axs[i, j].set_xlabel('Human Labor + AI Management (minutes)')\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel('Machine Automation (API calls)')\n",
    "            axs[i, j].legend(loc = 'upper right')\n",
    "\n",
    "            # set x and y limits\n",
    "            xh = lower_envelope_df[lower_envelope_df.alpha==alpha_list[0]]['labor_cost'].max() * 1.05\n",
    "            xl = - xh / 50\n",
    "            yh = lower_envelope_df[lower_envelope_df.alpha==0.3]['automation_cost'].max() * 1.05\n",
    "            yl = - yh / 50\n",
    "            \n",
    "            axs[i, j].set_xlim(xl, xh)\n",
    "            axs[i, j].set_ylim(yl, yh)\n",
    "\n",
    "    fig.suptitle(f'(Lower Envelope of) Indifference Curves for {plot_title_occupation}: First-Last-Task DAG\\n' +\n",
    "                '(Fixed-scale Axes)' +\n",
    "                '\\n' + \n",
    "                '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n' + \n",
    "                r'CES Production Function: $F(K,L) = [\\gamma K^{\\frac{\\sigma-1}{\\sigma}} + (1-\\gamma) L^{\\frac{\\sigma-1}{\\sigma}}]^{\\frac{\\sigma}{\\sigma-1}} $' + \n",
    "                '\\n' +\n",
    "                '\\n',\n",
    "                fontsize=16)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_lowerEnv_linear_{output_suffix}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Plot version 1b: no scaling with lower envelope points colored red - log axes\n",
    "    # Create a 2x5 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(22, 18))\n",
    "\n",
    "    # Iterate over each subplot position and add a plot\n",
    "    for i in range(3): \n",
    "        for j in range(5): \n",
    "            alpha_index = (i % 5) * 5 + j\n",
    "            my_alpha = alpha_list[alpha_index] # ignore epsilon\n",
    "\n",
    "            # Plot full data for given alpha\n",
    "            my_alpha_indiff_df = indiff_df[indiff_df['alpha']==my_alpha]\n",
    "\n",
    "            # log scale\n",
    "            my_alpha_indiff_df['labor_cost'] = np.log(my_alpha_indiff_df['labor_cost'])\n",
    "            my_alpha_indiff_df['automation_cost'] = np.log(my_alpha_indiff_df['automation_cost'])\n",
    "\n",
    "            axs[i, j].scatter(my_alpha_indiff_df.labor_cost, my_alpha_indiff_df.automation_cost, \n",
    "                            s=15, \n",
    "                            label=f'Dominated Plans (' + r'$n$' + f'={my_alpha_indiff_df.shape[0]})')\n",
    "            \n",
    "            # Plot lower envelope points in red\n",
    "            my_alpha_lower_envelope_df = lower_envelope_df[lower_envelope_df['alpha']==my_alpha]\n",
    "\n",
    "            # log scale\n",
    "            my_alpha_lower_envelope_df['labor_cost'] = np.log(my_alpha_lower_envelope_df['labor_cost'])\n",
    "            my_alpha_lower_envelope_df['automation_cost'] = np.log(my_alpha_lower_envelope_df['automation_cost'])\n",
    "\n",
    "            axs[i, j].scatter(my_alpha_lower_envelope_df.labor_cost, my_alpha_lower_envelope_df.automation_cost, \n",
    "                            s=20, \n",
    "                            color='red', \n",
    "                            label='Lower Envelope (' + r'$n$' + f'={my_alpha_lower_envelope_df.shape[0]})')\n",
    "            \n",
    "            # Add elasticity and labor share for current alpha\n",
    "            my_alpha_sigma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'sigma'].values[0]\n",
    "            my_alpha_gamma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'gamma'].values[0]\n",
    "            \n",
    "            sigma_display = r'$\\sigma$' + f'={my_alpha_sigma:.2f}'\n",
    "            gamma_display = r'$\\gamma$' + f'={my_alpha_gamma:.4f}'\n",
    "            axs[i, j].text(0.95, 0.85, sigma_display, transform=axs[i, j].transAxes, fontsize=12, verticalalignment='top', horizontalalignment='right', color='red')\n",
    "            axs[i, j].text(0.95, 0.8, gamma_display, transform=axs[i, j].transAxes, fontsize=12, verticalalignment='top', horizontalalignment='right', color='darkred')\n",
    "            \n",
    "            # other plot aesthetics\n",
    "            axs[i, j].title.set_text(r'$\\alpha$' + f'={np.round(my_alpha,4)}')\n",
    "            if i == 2:\n",
    "                axs[i, j].set_xlabel('Human Labor + AI Management (log minutes)')\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel('Machine Automation (log API calls)')\n",
    "            axs[i, j].legend(loc = 'upper right')\n",
    "\n",
    "    fig.suptitle(f'Indifference Curves for {plot_title_occupation}: First-Last-Task DAG\\n' +\n",
    "                '(Scale-varying Axes in logs)' +\n",
    "                '\\n' + \n",
    "                '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n' + \n",
    "                r'CES Production Function: $F(K,L) = [\\gamma K^{\\frac{\\sigma-1}{\\sigma}} + (1-\\gamma) L^{\\frac{\\sigma-1}{\\sigma}}]^{\\frac{\\sigma}{\\sigma-1}} $' + \n",
    "                '\\n' +\n",
    "                '\\n',\n",
    "                fontsize=16)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_log_{output_suffix}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Plot version 2a: scaling with only lower envelope points - log axes\n",
    "    # Create a 2x5 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(22, 18))\n",
    "\n",
    "    # Iterate over each subplot position and add a plot\n",
    "    for i in range(3): \n",
    "        for j in range(5): \n",
    "            alpha_index = (i % 5) * 5 + j\n",
    "            my_alpha = alpha_list[alpha_index] # ignore epsilon\n",
    "\n",
    "            # Plot lower envelope points in red\n",
    "            my_alpha_lower_envelope_df = lower_envelope_df[lower_envelope_df['alpha']==my_alpha]\n",
    "\n",
    "            # log scale\n",
    "            my_alpha_lower_envelope_df['labor_cost'] = np.log(my_alpha_lower_envelope_df['labor_cost'])\n",
    "            my_alpha_lower_envelope_df['automation_cost'] = np.log(my_alpha_lower_envelope_df['automation_cost'])\n",
    "\n",
    "            axs[i, j].scatter(my_alpha_lower_envelope_df.labor_cost, my_alpha_lower_envelope_df.automation_cost, \n",
    "                            s=20, \n",
    "                            label='Lower Envelope (' + r'$n$' + f'={my_alpha_lower_envelope_df.shape[0]})')\n",
    "            \n",
    "            # Add elasticity and labor share for current alpha\n",
    "            my_alpha_sigma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'sigma'].values[0]\n",
    "            my_alpha_gamma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'gamma'].values[0]\n",
    "            \n",
    "            sigma_display = r'$\\sigma$' + f'={my_alpha_sigma:.2f}'\n",
    "            gamma_display = r'$\\gamma$' + f'={my_alpha_gamma:.4f}'\n",
    "            axs[i, j].text(0.95, 0.9, sigma_display, transform=axs[i, j].transAxes, fontsize=12, \n",
    "                        verticalalignment='top', horizontalalignment='right', color='red')\n",
    "            axs[i, j].text(0.95, 0.85, gamma_display, transform=axs[i, j].transAxes, fontsize=12, \n",
    "                        verticalalignment='top', horizontalalignment='right', color='darkred')\n",
    "            \n",
    "            # other plot aesthetics\n",
    "            axs[i, j].title.set_text(r'$\\alpha$' + f'={np.round(my_alpha,4)}')\n",
    "            if i == 2:\n",
    "                axs[i, j].set_xlabel('Human Labor + AI Management (log minutes)')\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel('Machine Automation (log API calls)')\n",
    "            axs[i, j].legend(loc = 'upper right')\n",
    "\n",
    "            # set x and y limits\n",
    "            axs[i, j].set_xlim(-0.5, 7)\n",
    "            axs[i, j].set_ylim(-2, 20)\n",
    "\n",
    "\n",
    "    fig.suptitle(f'(Lower Envelope of) Indifference Curves for {plot_title_occupation}: First-Last-Task DAG\\n' +\n",
    "                '(Fixed-scale Axes in logs)' +\n",
    "                '\\n' + \n",
    "                '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n' + \n",
    "                r'CES Production Function: $F(K,L) = [\\gamma K^{\\frac{\\sigma-1}{\\sigma}} + (1-\\gamma) L^{\\frac{\\sigma-1}{\\sigma}}]^{\\frac{\\sigma}{\\sigma-1}} $' + \n",
    "                '\\n' +\n",
    "                '\\n',\n",
    "                fontsize=16)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_lowerEnv_log_{output_suffix}.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b61e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha list: [0.005, 0.007500000000000001, 0.01, 0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99999999]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# set alpha as AI quality metric\n",
    "epsilon = 1e-8\n",
    "# alpha_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1-epsilon]\n",
    "# alpha_list = [epsilon*5e4, epsilon*1e5, epsilon*5e5, epsilon*1e6, epsilon*5e6, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "alpha_list = [epsilon*5e5, epsilon*7.5e5, epsilon*1e6, epsilon*2.5e6, epsilon*5e6, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1-epsilon]\n",
    "print(f'alpha list: {alpha_list}')\n",
    "\n",
    "occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "                   'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "                   'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists',\n",
    "                   ]\n",
    "\n",
    "# occupation_list = ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators'\n",
    "#                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789cc1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Running: pileDriverOperators ----------------------\n",
      "\n",
      "-------Running: pileDriverOperators - Manual DAG-------\n",
      "Number of all possible partitioning schemes: 52\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "\n",
      "pileDriverOperators Manual DAG runtime: 14.26 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 52\n",
      "Number of valid partitioning schemes given DAG structure: 30\n",
      "\n",
      "pileDriverOperators Naive DAG runtime: 13.92 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 52\n",
      "Number of valid partitioning schemes given DAG structure: 20\n",
      "\n",
      "pileDriverOperators Conditioned Naive DAG runtime: 13.85 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 52\n",
      "Number of valid partitioning schemes given DAG structure: 40\n",
      "\n",
      "pileDriverOperators First-Last Task DAG runtime: 14.16 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 52\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "\n",
      "pileDriverOperators Conditioned First-Last Task DAG runtime: 13.92 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 52\n",
      "Number of valid partitioning schemes given DAG structure: 52\n",
      "\n",
      "pileDriverOperators Partitioned DAG runtime: 13.86 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 52\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "\n",
      "pileDriverOperators Conditioned Partitioned DAG runtime: 14.28 seconds\n",
      "\n",
      "\n",
      "************* pileDriverOperators runtime: 1.64 minutes *************\n",
      "\n",
      "runtime since start: 1.64 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: dredgeOperators ----------------------\n",
      "\n",
      "-------Running: dredgeOperators - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 94\n",
      "\n",
      "dredgeOperators Naive DAG runtime: 15.76 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 32\n",
      "\n",
      "dredgeOperators Conditioned Naive DAG runtime: 13.82 seconds\n",
      "\n",
      "-------Running: dredgeOperators - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 110\n",
      "\n",
      "dredgeOperators First-Last Task DAG runtime: 15.61 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 32\n",
      "\n",
      "dredgeOperators Conditioned First-Last Task DAG runtime: 13.95 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 168\n",
      "\n",
      "dredgeOperators Partitioned DAG runtime: 17.05 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 54\n",
      "\n",
      "dredgeOperators Conditioned Partitioned DAG runtime: 14.55 seconds\n",
      "\n",
      "\n",
      "************* dredgeOperators runtime: 1.51 minutes *************\n",
      "\n",
      "runtime since start: 3.15 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: gradersAndSortersForAgriculturalProducts ----------------------\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 104\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Naive DAG runtime: 15.45 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 80\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Conditioned Naive DAG runtime: 15.83 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 126\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts First-Last Task DAG runtime: 16.03 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 68\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Conditioned First-Last Task DAG runtime: 15.16 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 158\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Partitioned DAG runtime: 17.47 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 203\n",
      "Number of valid partitioning schemes given DAG structure: 68\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Conditioned Partitioned DAG runtime: 15.22 seconds\n",
      "\n",
      "\n",
      "************* gradersAndSortersForAgriculturalProducts runtime: 1.59 minutes *************\n",
      "\n",
      "runtime since start: 4.74 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: insuranceUnderwriters ----------------------\n",
      "\n",
      "-------Running: insuranceUnderwriters - Manual DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 192\n",
      "\n",
      "insuranceUnderwriters Manual DAG runtime: 19.26 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 543\n",
      "\n",
      "insuranceUnderwriters Naive DAG runtime: 28.58 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 293\n",
      "\n",
      "insuranceUnderwriters Conditioned Naive DAG runtime: 22.22 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 718\n",
      "\n",
      "insuranceUnderwriters First-Last Task DAG runtime: 32.74 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 402\n",
      "\n",
      "insuranceUnderwriters Conditioned First-Last Task DAG runtime: 24.69 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 825\n",
      "\n",
      "insuranceUnderwriters Partitioned DAG runtime: 32.40 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 250\n",
      "\n",
      "insuranceUnderwriters Conditioned Partitioned DAG runtime: 23.72 seconds\n",
      "\n",
      "\n",
      "************* insuranceUnderwriters runtime: 3.06 minutes *************\n",
      "\n",
      "runtime since start: 7.80 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: insuranceAppraisersForAutoDamage ----------------------\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 671\n",
      "\n",
      "insuranceAppraisersForAutoDamage Naive DAG runtime: 29.23 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 512\n",
      "\n",
      "insuranceAppraisersForAutoDamage Conditioned Naive DAG runtime: 25.91 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 696\n",
      "\n",
      "insuranceAppraisersForAutoDamage First-Last Task DAG runtime: 30.60 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 616\n",
      "\n",
      "insuranceAppraisersForAutoDamage Conditioned First-Last Task DAG runtime: 30.76 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 877\n",
      "\n",
      "insuranceAppraisersForAutoDamage Partitioned DAG runtime: 33.63 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 576\n",
      "\n",
      "insuranceAppraisersForAutoDamage Conditioned Partitioned DAG runtime: 28.08 seconds\n",
      "\n",
      "\n",
      "************* insuranceAppraisersForAutoDamage runtime: 2.97 minutes *************\n",
      "\n",
      "runtime since start: 10.77 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: floorSandersAndFinishers ----------------------\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 464\n",
      "\n",
      "floorSandersAndFinishers Naive DAG runtime: 28.09 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 348\n",
      "\n",
      "floorSandersAndFinishers Conditioned Naive DAG runtime: 23.42 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 706\n",
      "\n",
      "floorSandersAndFinishers First-Last Task DAG runtime: 30.53 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 464\n",
      "\n",
      "floorSandersAndFinishers Conditioned First-Last Task DAG runtime: 26.23 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 718\n",
      "\n",
      "floorSandersAndFinishers Partitioned DAG runtime: 32.94 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 390\n",
      "\n",
      "floorSandersAndFinishers Conditioned Partitioned DAG runtime: 24.01 seconds\n",
      "\n",
      "\n",
      "************* floorSandersAndFinishers runtime: 2.75 minutes *************\n",
      "\n",
      "runtime since start: 13.52 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: reinforcingIronAndRebarWorkers ----------------------\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 758\n",
      "\n",
      "reinforcingIronAndRebarWorkers Naive DAG runtime: 30.45 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 452\n",
      "\n",
      "reinforcingIronAndRebarWorkers Conditioned Naive DAG runtime: 24.48 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 376\n",
      "\n",
      "reinforcingIronAndRebarWorkers First-Last Task DAG runtime: 23.63 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 306\n",
      "\n",
      "reinforcingIronAndRebarWorkers Conditioned First-Last Task DAG runtime: 24.49 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 825\n",
      "\n",
      "reinforcingIronAndRebarWorkers Partitioned DAG runtime: 34.09 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 489\n",
      "\n",
      "reinforcingIronAndRebarWorkers Conditioned Partitioned DAG runtime: 26.61 seconds\n",
      "\n",
      "\n",
      "************* reinforcingIronAndRebarWorkers runtime: 2.73 minutes *************\n",
      "\n",
      "runtime since start: 16.25 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: travelAgents ----------------------\n",
      "\n",
      "-------Running: travelAgents - Manual DAG-------\n",
      "Number of all possible partitioning schemes: 4140\n",
      "Number of valid partitioning schemes given DAG structure: 192\n",
      "\n",
      "travelAgents Manual DAG runtime: 31.83 seconds\n",
      "\n",
      "-------Running: travelAgents - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 4140\n",
      "Number of valid partitioning schemes given DAG structure: 2106\n",
      "\n",
      "travelAgents Naive DAG runtime: 94.87 seconds\n",
      "\n",
      "-------Running: travelAgents - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 4140\n",
      "Number of valid partitioning schemes given DAG structure: 476\n",
      "\n",
      "travelAgents Conditioned Naive DAG runtime: 37.70 seconds\n",
      "\n",
      "-------Running: travelAgents - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 4140\n",
      "Number of valid partitioning schemes given DAG structure: 2428\n",
      "\n",
      "travelAgents First-Last Task DAG runtime: 106.34 seconds\n",
      "\n",
      "-------Running: travelAgents - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 4140\n",
      "Number of valid partitioning schemes given DAG structure: 1432\n",
      "\n",
      "travelAgents Conditioned First-Last Task DAG runtime: 69.00 seconds\n",
      "\n",
      "-------Running: travelAgents - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 4140\n",
      "Number of valid partitioning schemes given DAG structure: 3749\n",
      "\n",
      "travelAgents Partitioned DAG runtime: 156.06 seconds\n",
      "\n",
      "-------Running: travelAgents - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 4140\n",
      "Number of valid partitioning schemes given DAG structure: 1054\n",
      "\n",
      "travelAgents Conditioned Partitioned DAG runtime: 55.71 seconds\n",
      "\n",
      "\n",
      "************* travelAgents runtime: 9.19 minutes *************\n",
      "\n",
      "runtime since start: 25.44 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: dataEntryKeyer ----------------------\n",
      "\n",
      "-------Running: dataEntryKeyer - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 3355\n",
      "\n",
      "dataEntryKeyer Naive DAG runtime: 322.02 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 2204\n",
      "\n",
      "dataEntryKeyer Conditioned Naive DAG runtime: 202.12 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 2048\n",
      "\n",
      "dataEntryKeyer First-Last Task DAG runtime: 190.94 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 1400\n",
      "\n",
      "dataEntryKeyer Conditioned First-Last Task DAG runtime: 144.50 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 2189\n",
      "\n",
      "dataEntryKeyer Partitioned DAG runtime: 252.49 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 914\n",
      "\n",
      "dataEntryKeyer Conditioned Partitioned DAG runtime: 134.43 seconds\n",
      "\n",
      "\n",
      "************* dataEntryKeyer runtime: 20.78 minutes *************\n",
      "\n",
      "runtime since start: 46.22 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: athletesAndSportsCompetitors ----------------------\n",
      "\n",
      "-------Running: athletesAndSportsCompetitors - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 7407\n",
      "\n",
      "athletesAndSportsCompetitors Naive DAG runtime: 3729.51 seconds\n",
      "\n",
      "-------Running: athletesAndSportsCompetitors - Conditioned Naive DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 320\n",
      "\n",
      "athletesAndSportsCompetitors Conditioned Naive DAG runtime: 996.42 seconds\n",
      "\n",
      "-------Running: athletesAndSportsCompetitors - First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 824\n",
      "\n",
      "athletesAndSportsCompetitors First-Last Task DAG runtime: 2906.65 seconds\n",
      "\n",
      "-------Running: athletesAndSportsCompetitors - Conditioned First-Last Task DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 160\n",
      "\n",
      "athletesAndSportsCompetitors Conditioned First-Last Task DAG runtime: 473.62 seconds\n",
      "\n",
      "-------Running: athletesAndSportsCompetitors - Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 4929\n",
      "\n",
      "athletesAndSportsCompetitors Partitioned DAG runtime: 17745.65 seconds\n",
      "\n",
      "-------Running: athletesAndSportsCompetitors - Conditioned Partitioned DAG-------\n",
      "Number of all possible partitioning schemes: 21147\n",
      "Number of valid partitioning schemes given DAG structure: 724\n",
      "\n",
      "athletesAndSportsCompetitors Conditioned Partitioned DAG runtime: 1935.67 seconds\n",
      "\n",
      "\n",
      "************* athletesAndSportsCompetitors runtime: 463.13 minutes *************\n",
      "\n",
      "runtime since start: 509.34 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: audiovisualEquipmentInstallerAndRepairers ----------------------\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Naive DAG-------\n",
      "Number of all possible partitioning schemes: 678570\n",
      "Number of valid partitioning schemes given DAG structure: 426772\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moccupation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDAG_indicator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m DAG_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 53\u001b[0m plot_indiff_curve(alpha_list, input_path, output_suffix)\n\u001b[1;32m     54\u001b[0m DAG_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     56\u001b[0m DAG_execution_time \u001b[38;5;241m=\u001b[39m DAG_end_time \u001b[38;5;241m-\u001b[39m DAG_start_time\n",
      "Cell \u001b[0;32mIn[9], line 128\u001b[0m, in \u001b[0;36mplot_indiff_curve\u001b[0;34m(alpha_list, input_path, output_suffix)\u001b[0m\n\u001b[1;32m    117\u001b[0m     aux_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: [alpha], \n\u001b[1;32m    118\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution_plan\u001b[39m\u001b[38;5;124m'\u001b[39m: [execution_plan],\n\u001b[1;32m    119\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_tasks\u001b[39m\u001b[38;5;124m'\u001b[39m: [human_tasks],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautomation_cost\u001b[39m\u001b[38;5;124m'\u001b[39m: [automation_cost]\n\u001b[1;32m    125\u001b[0m                         })\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# append to execution_plan_costs_df\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     my_alpha_indiff_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([my_alpha_indiff_df, aux_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# append to master dataframe\u001b[39;00m\n\u001b[1;32m    131\u001b[0m indiff_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([indiff_df, my_alpha_indiff_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for occupation in occupation_list:\n",
    "    print(f'\\n---------------------- Running: {occupation} ----------------------')\n",
    "    occupation_start_time = time.time()\n",
    "\n",
    "    # generate occupation-specific strings\n",
    "    GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)\n",
    "\n",
    "    # Manual DAG\n",
    "    M_input_path = f'{occupation_folder}/{occupation}_M_DAG_df.csv'\n",
    "    M_output_suffix = f'M'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    N_input_path = f'{occupation_folder}/{occupation}_N_GPT_DAG_df.csv'\n",
    "    N_output_suffix = f'N'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    CN_input_path = f'{occupation_folder}/{occupation}_CN_GPT_DAG_df.csv'\n",
    "    CN_output_suffix = f'CN'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    FLT_input_path = f'{occupation_folder}/{occupation}_FLT_GPT_DAG_df.csv'\n",
    "    FLT_output_suffix = f'FLT'\n",
    "\n",
    "    # Conditioned First Last Task DAG\n",
    "    CFLT_input_path = f'{occupation_folder}/{occupation}_CFLT_GPT_DAG_df.csv'\n",
    "    CFLT_output_suffix = f'CFLT'\n",
    "\n",
    "    # Partitioned DAG\n",
    "    P_input_path = f'{occupation_folder}/{occupation}_P_GPT_DAG_df.csv'\n",
    "    P_output_suffix = f'P'\n",
    "\n",
    "    # Conditioned Partitioned DAG\n",
    "    CP_input_path = f'{occupation_folder}/{occupation}_CP_GPT_DAG_df.csv'\n",
    "    CP_output_suffix = f'CP'\n",
    "    \n",
    "\n",
    "\n",
    "    # create list of all DAGs\n",
    "    if occupation in ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators']:\n",
    "        DAG_indicator_list = ['Manual DAG', 'Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [M_input_path, N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_suffixs_list = [M_output_suffix, N_output_suffix, CN_output_suffix, FLT_output_suffix, CFLT_output_suffix, P_output_suffix, CP_output_suffix]\n",
    "    else:\n",
    "        DAG_indicator_list = ['Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_suffixs_list = [N_output_suffix, CN_output_suffix, FLT_output_suffix, CFLT_output_suffix, P_output_suffix, CP_output_suffix]\n",
    "\n",
    "\n",
    "    for DAG_indicator, input_path, output_suffix in zip(DAG_indicator_list, input_paths_list, output_suffixs_list):\n",
    "        print(f'\\n-------Running: {occupation} - {DAG_indicator}-------')\n",
    "        \n",
    "        DAG_start_time = time.time()\n",
    "        plot_indiff_curve(alpha_list, input_path, output_suffix)\n",
    "        DAG_end_time = time.time()\n",
    "\n",
    "        DAG_execution_time = DAG_end_time - DAG_start_time\n",
    "        print(f\"\\n{occupation} {DAG_indicator} runtime: {DAG_execution_time:.2f} seconds\")\n",
    "\n",
    "    occupation_end_time = time.time()\n",
    "    occupation_execution_time = (occupation_end_time - occupation_start_time)/60\n",
    "    print(f\"\\n\\n************* {occupation} runtime: {occupation_execution_time:.2f} minutes *************\")\n",
    "    runtime_since_start = (time.time() - start_time)/60\n",
    "    print(f\"\\nruntime since start: {runtime_since_start:.2f} minutes\\n\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"\\n\\nTotal Runtime: {execution_time:.2f} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
