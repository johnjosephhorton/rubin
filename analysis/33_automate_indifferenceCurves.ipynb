{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)\n",
    "\n",
    "with open('functions.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run caffeinate in the background to prevent sleep\n",
    "subprocess.Popen(['caffeinate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymanshahidi':\n",
    "    main_folder_path = '/Users/peymanshahidi/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a49d43",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb953c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def partitions(set_):\n",
    "    if not set_:\n",
    "        yield []\n",
    "        return\n",
    "    for i in range(1, len(set_) + 1):\n",
    "        for part in combinations(set_, i):\n",
    "            remaining = set(set_) - set(part)\n",
    "            if not remaining:\n",
    "                yield [list(part)]\n",
    "            else:\n",
    "                for b in partitions(list(remaining)):\n",
    "                    yield [list(part)] + b\n",
    "\n",
    "def generate_unique_partitions(numbers):\n",
    "    all_partitions = set()\n",
    "    for partition in partitions(numbers):\n",
    "        # Create a frozenset of frozensets to make each partition hashable and order-independent\n",
    "        partition_set = frozenset(frozenset(part) for part in partition)\n",
    "        all_partitions.add(partition_set)\n",
    "    \n",
    "    # Convert the frozensets back to lists for the final output\n",
    "    unique_partitions = [list(map(list, partition)) for partition in all_partitions]\n",
    "\n",
    "    # Sort elements\n",
    "    unique_partitions = sorted([sorted(x) for x in unique_partitions], key=len)\n",
    "    return unique_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c64d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_connected(matrix):\n",
    "    # Number of nodes in the matrix\n",
    "    num_nodes = matrix.shape[0]\n",
    "    \n",
    "    # Visited array to keep track of visited nodes\n",
    "    visited = np.zeros(num_nodes, dtype=bool)\n",
    "    \n",
    "    # Helper function to perform DFS\n",
    "    def dfs(node):\n",
    "        visited[node] = True\n",
    "        # Visit all the neighbors of the current node\n",
    "        for neighbor in range(num_nodes):\n",
    "            if matrix[node, neighbor] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "            elif matrix[neighbor, node] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "    \n",
    "    # Start DFS from the first node (node 0)\n",
    "    dfs(0)\n",
    "    \n",
    "    # If all nodes are visited, the matrix is connected\n",
    "    return np.all(visited)\n",
    "\n",
    "\n",
    "def validate_partition_using_connectedness(adjacency_matrix, tasks_list):\n",
    "    # Return valid if Singleton\n",
    "    if len(tasks_list) == 1:\n",
    "        return True\n",
    "    # Check if partition forms connected graph\n",
    "    else:\n",
    "        # Subset original adjacency matrix\n",
    "        subset_matrix = adjacency_matrix[np.ix_(tasks_list, tasks_list)]\n",
    "\n",
    "        # check if subset matrix is a connected graph\n",
    "        subset_matrix_connected = is_connected(subset_matrix)\n",
    "\n",
    "        # return true if connected and false otherwise\n",
    "        return subset_matrix_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acb1d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_boundary(adjacency_matrix, partition):\n",
    "    # create a matrix whose columns are nodes not in the partition and whose rows are nodes in the partition\n",
    "    # (subset adjacency matrix to outgoing edges of partition nodes --i.e., rows-- and incoming edges of non-partition nodes --i.e., columns.)\n",
    "    reduced_matrix = np.delete(adjacency_matrix, partition, axis=1) \n",
    "    reduced_matrix = reduced_matrix[partition, :]\n",
    "\n",
    "    # find nodes in partition w/ an edge to non-partition nodes\n",
    "    partition_boundary_tasks = [i for i in partition if np.any(reduced_matrix[partition.index(i), :])]\n",
    "\n",
    "    return partition_boundary_tasks\n",
    "\n",
    "\n",
    "def compute_plan_cost(adjacency_matrix, \n",
    "                      human_labor_dict, \n",
    "                      machine_automation_dict, \n",
    "                      machine_management_dict, \n",
    "                      management_difficulty_dict, \n",
    "                      automation_difficulty_dict,\n",
    "                      AI_quality, execution_plan, human_tasks):\n",
    "    # initialize costs\n",
    "    human_labor_cost = 0\n",
    "    management_cost = 0\n",
    "    automation_cost = 0\n",
    "\n",
    "    managed_tasks_list = []\n",
    "    automated_tasks_list = []\n",
    "    for partition in execution_plan:\n",
    "        if len(partition) == 1:\n",
    "            if partition[0] in human_tasks:\n",
    "                labor_cost = sum(human_labor_dict[key] for key in partition)\n",
    "                human_labor_cost += labor_cost\n",
    "            else:\n",
    "                managed_tasks_list.append(partition[0])\n",
    "                machine_management_cost = sum(machine_management_dict[key] for key in partition)\n",
    "                management_difficulty = sum(management_difficulty_dict[key] for key in partition)\n",
    "                management_cost += machine_management_cost * (AI_quality ** (-1 * management_difficulty))\n",
    "        else:\n",
    "            # determine which tasks are automated and which tasks are managed\n",
    "            managed_tasks = get_partition_boundary(adjacency_matrix, partition)\n",
    "            automated_tasks = [task for task in partition if task not in managed_tasks]\n",
    "            managed_tasks_list.append(managed_tasks)\n",
    "            automated_tasks_list.append(automated_tasks)\n",
    "\n",
    "            # calculate management cost of partition\n",
    "            machine_management_cost = sum(machine_management_dict[key] for key in managed_tasks)\n",
    "            management_difficulty = sum(management_difficulty_dict[key] for key in managed_tasks)\n",
    "            management_cost += machine_management_cost * (AI_quality ** (-1 * management_difficulty))\n",
    "\n",
    "            # calculate labor cost of partition\n",
    "            machine_automation_cost = sum(machine_automation_dict[key] for key in automated_tasks)\n",
    "            automation_difficulty = sum(automation_difficulty_dict[key] for key in automated_tasks)\n",
    "            automation_cost += machine_automation_cost * (AI_quality ** (-1 * automation_difficulty))\n",
    "\n",
    "    # rounding\n",
    "    human_labor_cost = np.round(human_labor_cost, 4)\n",
    "    management_cost = np.round(management_cost, 4)\n",
    "    automation_cost = np.round(automation_cost, 4)\n",
    "\n",
    "    return human_labor_cost, management_cost, automation_cost, managed_tasks_list, automated_tasks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c91ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dominated_points(df):\n",
    "    # Sort the DataFrame by labor_cost and then by automation_cost\n",
    "    df_sorted = df.sort_values(by=['labor_cost', 'automation_cost']).reset_index(drop=True)\n",
    "\n",
    "    # Find index of lowest automation cost for each labor cost\n",
    "    idx = df_sorted.groupby('labor_cost')['automation_cost'].idxmin()\n",
    "    if len(idx) == 1:\n",
    "        idx = idx[0]\n",
    "    df_sorted = df_sorted.loc[idx]\n",
    "\n",
    "    # Find index of lowest labor cost for each automation cost\n",
    "    idx = df_sorted.groupby('automation_cost')['labor_cost'].idxmin()\n",
    "    if len(idx) == 1:\n",
    "        idx = idx[0]\n",
    "    df_sorted = df_sorted.loc[idx]\n",
    "\n",
    "    # Initialize an empty list to store the non-dominated points\n",
    "    non_dominated_points = []\n",
    "    \n",
    "    # Iterate through each point in the DataFrame\n",
    "    for i, point in df_sorted.iterrows():\n",
    "        # Check if this point is dominated by any other point\n",
    "        dominated = False\n",
    "        for j, other_point in df_sorted.iterrows():\n",
    "            if (other_point['labor_cost'] < point['labor_cost']) and (other_point['automation_cost'] < point['automation_cost']):\n",
    "                dominated = True\n",
    "                break\n",
    "        # If the point is not dominated, add it to the list\n",
    "        if not dominated:\n",
    "            non_dominated_points.append(point)\n",
    "    \n",
    "    # Return the non-dominated points as a DataFrame\n",
    "    return pd.DataFrame(non_dominated_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b8d2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def calc_CES_params(df, labor_wage):\n",
    "    ###### hacky way of getting around zero input values ######\n",
    "    df['labor_cost'] = df['labor_cost'].apply(lambda x: x + epsilon if x == 0 else x)\n",
    "    df['automation_cost'] = df['automation_cost'].apply(lambda x: x + epsilon if x == 0 else x)\n",
    "\n",
    "    # Prepare data for regression\n",
    "    df['const'] = 1\n",
    "    df['log_automation_over_humanLabor'] = np.log(df['automation_cost'] / df['labor_cost'])\n",
    "    df['log_laborWage_over_AIrentalCost'] = np.log(labor_wage / df['AI_rental_cost'])\n",
    "\n",
    "    # Run regression\n",
    "    X = df[['const', 'log_automation_over_humanLabor']]\n",
    "    Y = df['log_laborWage_over_AIrentalCost']\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "\n",
    "    # Get elasticity of technical substitution\n",
    "    beta_0, beta_1 = model.params\n",
    "\n",
    "    sigma = 1 / beta_1\n",
    "    gamma = 1 / (1 + np.exp(beta_0))\n",
    "\n",
    "    return sigma, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74eb8f3",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7edaf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_indiff_curve(input_path, output_suffix):\n",
    "\n",
    "    # read DAG\n",
    "    dag_df = pd.read_csv(input_path)\n",
    "\n",
    "    # remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "    if 'comment' in dag_df.columns:\n",
    "        dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "\n",
    "    # get task stats and create a list\n",
    "    tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "    tasks_list = tasks_stats['task'].unique()\n",
    "\n",
    "    tasks_stats['machine_cost'] = 1e5 # tasks_stats['machine_cost'] / 300\n",
    "\n",
    "\n",
    "    # create a dictionary for indexing tasks\n",
    "    tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "    # create numpy array of adjacency matrix\n",
    "    adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    for _, row in dag_df.iterrows():\n",
    "        source_index = aux_dict[row['source']]\n",
    "        target_index = aux_dict[row['target']]\n",
    "        adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # add task_dict key and reset index\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    tasks_stats['dict_index'] = tasks_stats.apply(lambda row: aux_dict[row.task], axis=1)\n",
    "    tasks_stats = tasks_stats.sort_values(by='dict_index')\n",
    "    tasks_stats = tasks_stats.set_index('dict_index', drop=False)\n",
    "    tasks_stats.index.name = None\n",
    "\n",
    "\n",
    "    # Generate list of numbers for non-\"Target\" tasks in occupation\n",
    "    tasks_list_numbers = list(range(len(tasks_list)-1)) # -1 for \"Target\" task: don't want to include it in task partitions as it's its own separate partition\n",
    "\n",
    "    # Generate all possible partitioning schemes\n",
    "    all_partitions = generate_unique_partitions(tasks_list_numbers)\n",
    "\n",
    "\n",
    "\n",
    "    # Get valid partitioning schemes from all possible partitions to cut computation load\n",
    "    valid_partitions = []\n",
    "    for scheme in all_partitions:\n",
    "        # Set valid partitions count to 0\n",
    "        valid_partition_count = 0\n",
    "        for partition in scheme:\n",
    "            valid_partition = validate_partition_using_connectedness(adjacency_matrix, partition)\n",
    "            if valid_partition:\n",
    "                valid_partition_count += 1\n",
    "        \n",
    "        # If number of valid partitions within a partition scheme is equal to \n",
    "        # number of partitions in partition scheme then partition scheme is valid\n",
    "        if valid_partition_count == len(scheme):\n",
    "            valid_partitions.append(scheme)\n",
    "\n",
    "    # Print stats\n",
    "    print(f'Number of all possible partitioning schemes: {len(all_partitions)}')\n",
    "    print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "\n",
    "    # get how many \"singleton\" partitions there are in valid partition\n",
    "    valid_execution_plans = pd.DataFrame()\n",
    "    for my_valid_partition in valid_partitions:\n",
    "        singleton_partitions = [lst[0] for lst in my_valid_partition if len(lst) == 1]\n",
    "\n",
    "        # get the power set of \"singleton\" partitions\n",
    "        # goal is to generate ways singleton tasks can be done by human or AI\n",
    "        all_combinations = [[]]\n",
    "        for r in range(1, len(singleton_partitions) + 1):\n",
    "            combinations_r = itertools.combinations(singleton_partitions, r)\n",
    "            all_combinations.extend(combinations_r)\n",
    "\n",
    "        # Convert the combinations to a list of lists (optional)\n",
    "        all_combinations = [list(comb) for comb in all_combinations]\n",
    "        all_combinations\n",
    "\n",
    "        # repeat my_valid_partition for each combination in all_combinations to create a dataframe later\n",
    "        my_valid_partition_repeated = [my_valid_partition for _ in range(len(all_combinations))]\n",
    "        aux_df = pd.DataFrame({'execution_plan': my_valid_partition_repeated, \n",
    "                            'human_tasks': all_combinations})\n",
    "        \n",
    "        # append to valid_execution_plans\n",
    "        valid_execution_plans = pd.concat([valid_execution_plans, aux_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    # create dictionaries for human cost, management cost, and difficulty\n",
    "    human_labor_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['human_cost']))\n",
    "    machine_automation_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['machine_cost']))\n",
    "    machine_management_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['management_cost']))\n",
    "\n",
    "    management_difficulty_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['management_difficulty']))\n",
    "    automation_difficulty_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['completion_difficulty']))\n",
    "\n",
    "\n",
    "\n",
    "    # calculate plan costs for each alpha\n",
    "    indiff_df = pd.DataFrame()\n",
    "    for counter, alpha in enumerate(alpha_list):\n",
    "        my_alpha_indiff_df = pd.DataFrame()\n",
    "        for execution_plan, human_tasks in zip(valid_execution_plans['execution_plan'], valid_execution_plans['human_tasks']):\n",
    "            # calculate plan costs\n",
    "            human_labor_cost, management_cost, automation_cost, managed_tasks, automated_tasks = compute_plan_cost(adjacency_matrix, \n",
    "                                                                                                                human_labor_dict, \n",
    "                                                                                                                machine_automation_dict, \n",
    "                                                                                                                machine_management_dict, \n",
    "                                                                                                                management_difficulty_dict, \n",
    "                                                                                                                automation_difficulty_dict,\n",
    "                                                                                                                alpha, execution_plan, human_tasks)\n",
    "\n",
    "            # create a dataframe to store execution plan costs\n",
    "            aux_df = pd.DataFrame({'alpha': [alpha], \n",
    "                                'execution_plan': [execution_plan],\n",
    "                                'human_tasks': [human_tasks],\n",
    "                                'managed_tasks': [managed_tasks],\n",
    "                                'automated_tasks': [automated_tasks],\n",
    "                                'human_labor_cost': [human_labor_cost],\n",
    "                                'management_cost': [management_cost],\n",
    "                                'automation_cost': [automation_cost]\n",
    "                                })\n",
    "            \n",
    "            # append to execution_plan_costs_df\n",
    "            my_alpha_indiff_df = pd.concat([my_alpha_indiff_df, aux_df], ignore_index=True)\n",
    "        \n",
    "        # append to master dataframe\n",
    "        indiff_df = pd.concat([indiff_df, my_alpha_indiff_df], ignore_index=True)\n",
    "\n",
    "    indiff_df_orig = indiff_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "    indiff_df = indiff_df_orig.copy()\n",
    "\n",
    "    # min wage = $15 / hour\n",
    "    labor_wage = 1\n",
    "\n",
    "    # management cost: rental cost of capital\n",
    "    AI_rental_cost = 1\n",
    "\n",
    "    # avg number of tokens per prompt\n",
    "    #avg_num_prompts_list = [1, 5, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "    #avg_num_prompts = 10000\n",
    "\n",
    "    # GPT-4 cost per 1 million tokens: $30\n",
    "    #API_cost = avg_num_prompts * 30 / 1e6 # $ per prompt\n",
    "\n",
    "    # convert machine labor cost to minutes and add to human labor cost in minutes\n",
    "    indiff_df['labor_cost'] = indiff_df.apply(lambda row: row['human_labor_cost'] + (row['management_cost']), axis=1)\n",
    "\n",
    "    # calculate total cost\n",
    "    indiff_df['total_cost'] = indiff_df.apply(lambda row: row['labor_cost'] * labor_wage + row['automation_cost'] * AI_rental_cost, axis=1)\n",
    "\n",
    "    # find optimal execution plan\n",
    "    indiff_df['min_total_cost_flag'] = indiff_df.groupby('alpha')['total_cost'].transform(lambda x: x == x.min())\n",
    "\n",
    "\n",
    "\n",
    "    # Get lower envelope of points\n",
    "    lower_envelope_df = pd.DataFrame()\n",
    "    for my_alpha in alpha_list:\n",
    "        # subset corresponding alpha's data from master dataset\n",
    "        my_alpha_indiff_df = indiff_df[indiff_df['alpha']==my_alpha]\n",
    "\n",
    "        # drop duplicates, if any exist\n",
    "        my_alpha_indiff_df = my_alpha_indiff_df.drop_duplicates(subset=['labor_cost', 'automation_cost'])\n",
    "\n",
    "        # get lower envelope for current alpha\n",
    "        my_alpha_indiff_lower_envelope_df = remove_dominated_points(my_alpha_indiff_df)\n",
    "\n",
    "        # append to lower envelope dataframe\n",
    "        lower_envelope_df = pd.concat([lower_envelope_df, my_alpha_indiff_lower_envelope_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Get multiple labor-management wage pairs and find optimal plan for each\n",
    "    n = 500 # number of different pairs / 2\n",
    "    cost_max = 10000\n",
    "\n",
    "    # for a fixed labor_wage generate n different AI_rental_cost values\n",
    "    list_1 = list(np.linspace(1, cost_max, n))\n",
    "    list_2 = [1 / value for value in list_1]\n",
    "    AI_rental_cost_list = list_1[1:] + list_2\n",
    "    AI_rental_cost_list = [value * labor_wage for value in AI_rental_cost_list]\n",
    "\n",
    "    # Fix labor_wage and vary AI_rental_cost \n",
    "    optimal_plans_df = pd.DataFrame()\n",
    "    for AI_rental_cost in AI_rental_cost_list:\n",
    "        # Calculate total cost\n",
    "        lower_envelope_df['total_cost'] = lower_envelope_df.apply(lambda row: row['labor_cost'] * labor_wage + row['automation_cost'] * AI_rental_cost, axis=1)\n",
    "\n",
    "        # Find optimal execution plan given labor_wage and AI_rental_cost\n",
    "        lower_envelope_df['min_total_cost_flag'] = lower_envelope_df.groupby('alpha')['total_cost'].transform(lambda x: x == x.min())\n",
    "\n",
    "        # Save optimal plan to master dataframe\n",
    "        aux_df = lower_envelope_df[lower_envelope_df.min_total_cost_flag]\n",
    "        aux_df['AI_rental_cost'] = AI_rental_cost\n",
    "        optimal_plans_df = pd.concat([optimal_plans_df, aux_df], ignore_index=True)\n",
    "\n",
    "    # Sort by alpha and AI_rental_cost\n",
    "    optimal_plans_df = optimal_plans_df.sort_values(by=['alpha', 'AI_rental_cost']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Run regression to find elasticity of technical substitution\n",
    "    ETS_df = pd.DataFrame()\n",
    "    for my_alpha in alpha_list:\n",
    "        # Subset data for current alpha\n",
    "        my_alpha_regression_df = optimal_plans_df[optimal_plans_df.alpha == my_alpha]\n",
    "\n",
    "        # Calculate elasticity of technical substitution\n",
    "        sigma, gamma = calc_CES_params(my_alpha_regression_df, labor_wage)\n",
    "\n",
    "        # Append to dataframe\n",
    "        aux_df = pd.DataFrame({'alpha': [my_alpha], 'sigma': [sigma], 'gamma': [gamma]})\n",
    "        ETS_df = pd.concat([ETS_df, aux_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot version 1: no scaling with lower envelope points colored red\n",
    "\n",
    "    # Create a 2x5 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(22, 18))\n",
    "\n",
    "    # Iterate over each subplot position and add a plot\n",
    "    for i in range(3): \n",
    "        for j in range(5): \n",
    "            alpha_index = (i % 5) * 5 + j\n",
    "            my_alpha = alpha_list[alpha_index] # ignore epsilon\n",
    "\n",
    "            # Plot full data for given alpha\n",
    "            my_alpha_indiff_df = indiff_df[indiff_df['alpha']==my_alpha]\n",
    "            axs[i, j].scatter(my_alpha_indiff_df.labor_cost, my_alpha_indiff_df.automation_cost, \n",
    "                            s=15, \n",
    "                            label=f'Dominated Plans (' + r'$n$' + f'={my_alpha_indiff_df.shape[0]})')\n",
    "            \n",
    "            # Plot lower envelope points in red\n",
    "            my_alpha_lower_envelope_df = lower_envelope_df[lower_envelope_df['alpha']==my_alpha]\n",
    "            axs[i, j].scatter(my_alpha_lower_envelope_df.labor_cost, my_alpha_lower_envelope_df.automation_cost, \n",
    "                            s=20, \n",
    "                            color='red', \n",
    "                            label='Lower Envelope (' + r'$n$' + f'={my_alpha_lower_envelope_df.shape[0]})')\n",
    "            \n",
    "            # Add elasticity and labor share for current alpha\n",
    "            my_alpha_sigma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'sigma'].values[0]\n",
    "            my_alpha_gamma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'gamma'].values[0]\n",
    "            \n",
    "            sigma_display = r'$\\sigma$' + f'={my_alpha_sigma:.2f}'\n",
    "            gamma_display = r'$\\gamma$' + f'={my_alpha_gamma:.4f}'\n",
    "            axs[i, j].text(0.95, 0.85, sigma_display, transform=axs[i, j].transAxes, fontsize=12, verticalalignment='top', horizontalalignment='right', color='red')\n",
    "            axs[i, j].text(0.95, 0.8, gamma_display, transform=axs[i, j].transAxes, fontsize=12, verticalalignment='top', horizontalalignment='right', color='darkred')\n",
    "            \n",
    "            # other plot aesthetics\n",
    "            axs[i, j].title.set_text(r'$\\alpha$' + f'={np.round(my_alpha,4)}')\n",
    "            if i == 2:\n",
    "                axs[i, j].set_xlabel('Human Labor + AI Management (minutes)')\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel('Machine Automation (API calls)')\n",
    "            axs[i, j].legend(loc = 'upper right')\n",
    "\n",
    "\n",
    "    fig.suptitle(f'Indifference Curves for {plot_title_occupation}: First-Last-Task DAG\\n' +\n",
    "                '(Scale-varying Axes)' +\n",
    "                '\\n' + \n",
    "                '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n' + \n",
    "                r'CES Production Function: $F(K,L) = [\\gamma K^{\\frac{\\sigma-1}{\\sigma}} + (1-\\gamma) L^{\\frac{\\sigma-1}{\\sigma}}]^{\\frac{\\sigma}{\\sigma-1}} $' + \n",
    "                '\\n' +\n",
    "                '\\n',\n",
    "                fontsize=16)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_varyingScale_{output_suffix}.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # Plot version 2: scaling with only lower envelope points \n",
    "\n",
    "    # Create a 2x5 grid of subplots\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(22, 18))\n",
    "\n",
    "    # Iterate over each subplot position and add a plot\n",
    "    for i in range(3): \n",
    "        for j in range(5): \n",
    "            alpha_index = (i % 5) * 5 + j\n",
    "            my_alpha = alpha_list[alpha_index] # ignore epsilon\n",
    "\n",
    "            # # Plot full data for given alpha\n",
    "            # my_alpha_indiff_df = indiff_df[indiff_df['alpha']==my_alpha]\n",
    "            # axs[i, j].scatter(my_alpha_indiff_df.labor_cost, my_alpha_indiff_df.automation_cost, \n",
    "            #                 s=15, \n",
    "            #                 label=f'Dominated Plans (' + r'$n$' + f'={my_alpha_indiff_df.shape[0]})')\n",
    "            \n",
    "            # Plot lower envelope points in red\n",
    "            my_alpha_lower_envelope_df = lower_envelope_df[lower_envelope_df['alpha']==my_alpha]\n",
    "            axs[i, j].scatter(my_alpha_lower_envelope_df.labor_cost, my_alpha_lower_envelope_df.automation_cost, \n",
    "                            s=20, \n",
    "                            #color='red',\n",
    "                            label='Lower Envelope (' + r'$n$' + f'={my_alpha_lower_envelope_df.shape[0]})')\n",
    "            \n",
    "            # Add elasticity and labor share for current alpha\n",
    "            my_alpha_sigma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'sigma'].values[0]\n",
    "            my_alpha_gamma = ETS_df.loc[ETS_df['alpha'] == my_alpha, 'gamma'].values[0]\n",
    "            \n",
    "            sigma_display = r'$\\sigma$' + f'={my_alpha_sigma:.2f}'\n",
    "            gamma_display = r'$\\gamma$' + f'={my_alpha_gamma:.4f}'\n",
    "            axs[i, j].text(0.95, 0.9, sigma_display, transform=axs[i, j].transAxes, fontsize=12, \n",
    "                        verticalalignment='top', horizontalalignment='right', color='red')\n",
    "            axs[i, j].text(0.95, 0.85, gamma_display, transform=axs[i, j].transAxes, fontsize=12, \n",
    "                        verticalalignment='top', horizontalalignment='right', color='darkred')\n",
    "            \n",
    "            # other plot aesthetics\n",
    "            axs[i, j].title.set_text(r'$\\alpha$' + f'={np.round(my_alpha,4)}')\n",
    "            if i == 2:\n",
    "                axs[i, j].set_xlabel('Human Labor + AI Management (minutes)')\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel('Machine Automation (API calls)')\n",
    "            axs[i, j].legend(loc = 'upper right')\n",
    "\n",
    "            # set x and y limits\n",
    "            xh = lower_envelope_df[lower_envelope_df.alpha==alpha_list[0]]['labor_cost'].max() * 1.1\n",
    "            xl = - xh / 50\n",
    "            yh = lower_envelope_df[lower_envelope_df.alpha==0.5]['automation_cost'].max() * 1.1\n",
    "            yl = - yh / 50\n",
    "            \n",
    "            axs[i, j].set_xlim(xl, xh)\n",
    "            axs[i, j].set_ylim(yl, yh)\n",
    "\n",
    "\n",
    "    fig.suptitle(f'(Lower Envelope of) Indifference Curves for {plot_title_occupation}: First-Last-Task DAG\\n' +\n",
    "                '(Fixed-scale Axes)' +\n",
    "                '\\n' + \n",
    "                '------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n' + \n",
    "                r'CES Production Function: $F(K,L) = [\\gamma K^{\\frac{\\sigma-1}{\\sigma}} + (1-\\gamma) L^{\\frac{\\sigma-1}{\\sigma}}]^{\\frac{\\sigma}{\\sigma-1}} $' + \n",
    "                '\\n' +\n",
    "                '\\n',\n",
    "                fontsize=16)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_fixedScale_{output_suffix}.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b61e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# set alpha as AI quality metric\n",
    "epsilon = 1e-8\n",
    "# alpha_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1-epsilon]\n",
    "# alpha_list = [epsilon*5e4, epsilon*1e5, epsilon*5e5, epsilon*1e6, epsilon*5e6, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "alpha_list = [epsilon*5e5, epsilon*7.5e5, epsilon*1e6, epsilon*2.5e6, epsilon*5e6, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1-epsilon]\n",
    "print(f'alpha list: {alpha_list}')\n",
    "\n",
    "occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "                   'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "                   'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists',\n",
    "                   ]\n",
    "\n",
    "# occupation_list = ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators'\n",
    "#                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for occupation in occupation_list:\n",
    "    print(f'\\n---------------------- Running: {occupation} ----------------------')\n",
    "    occupation_start_time = time.time()\n",
    "\n",
    "    # generate occupation-specific strings\n",
    "    GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)\n",
    "\n",
    "    # Manual DAG\n",
    "    M_input_path = f'{occupation_folder}/{occupation}_M_DAG_df.csv'\n",
    "    M_output_suffix = f'M'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    N_input_path = f'{occupation_folder}/{occupation}_N_GPT_DAG_df.csv'\n",
    "    N_output_suffix = f'N'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    CN_input_path = f'{occupation_folder}/{occupation}_CN_GPT_DAG_df.csv'\n",
    "    CN_output_suffix = f'CN'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    FLT_input_path = f'{occupation_folder}/{occupation}_FLT_GPT_DAG_df.csv'\n",
    "    FLT_output_suffix = f'FLT'\n",
    "\n",
    "    # Conditioned First Last Task DAG\n",
    "    CFLT_input_path = f'{occupation_folder}/{occupation}_CFLT_GPT_DAG_df.csv'\n",
    "    CFLT_output_suffix = f'CFLT'\n",
    "\n",
    "    # Partitioned DAG\n",
    "    P_input_path = f'{occupation_folder}/{occupation}_P_GPT_DAG_df.csv'\n",
    "    P_output_suffix = f'P'\n",
    "\n",
    "    # Conditioned Partitioned DAG\n",
    "    CP_input_path = f'{occupation_folder}/{occupation}_CP_GPT_DAG_df.csv'\n",
    "    CP_output_suffix = f'CP'\n",
    "    \n",
    "\n",
    "\n",
    "    # create list of all DAGs\n",
    "    if occupation in ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators']:\n",
    "        DAG_indicator_list = ['Manual DAG', 'Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [M_input_path, N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_suffixs_list = [M_output_suffix, N_output_suffix, CN_output_suffix, FLT_output_suffix, CFLT_output_suffix, P_output_suffix, CP_output_suffix]\n",
    "    else:\n",
    "        DAG_indicator_list = ['Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_suffixs_list = [N_output_suffix, CN_output_suffix, FLT_output_suffix, CFLT_output_suffix, P_output_suffix, CP_output_suffix]\n",
    "\n",
    "\n",
    "    for DAG_indicator, input_path, output_suffix in zip(DAG_indicator_list, input_paths_list, output_suffixs_list):\n",
    "        print(f'\\n-------Running: {occupation} - {DAG_indicator}-------')\n",
    "        \n",
    "        DAG_start_time = time.time()\n",
    "        plot_indiff_curve(input_path, output_suffix)\n",
    "        DAG_end_time = time.time()\n",
    "\n",
    "        DAG_execution_time = DAG_end_time - DAG_start_time\n",
    "        print(f\"\\n{occupation} {DAG_indicator} runtime: {DAG_execution_time:.2f} seconds\")\n",
    "\n",
    "    occupation_end_time = time.time()\n",
    "    occupation_execution_time = (occupation_end_time - occupation_start_time)/60\n",
    "    print(f\"\\n\\n************* {occupation} runtime: {occupation_execution_time:.2f} minutes *************\")\n",
    "    runtime_since_start = (time.time() - start_time)/60\n",
    "    print(f\"\\nruntime since start: {runtime_since_start:.2f} minutes\\n\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"\\n\\nTotal Runtime: {execution_time:.2f} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
