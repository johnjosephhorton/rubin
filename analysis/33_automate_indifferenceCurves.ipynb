{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)\n",
    "\n",
    "with open('functions.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymansh':\n",
    "    main_folder_path = '/Users/peymansh/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54980713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tasks(onet_data_path,\n",
    "              occupation_code):\n",
    "\n",
    "    # Load the data\n",
    "    onet = pd.read_csv(onet_data_path)\n",
    "    onet = onet.sort_values(by=['year', 'occ_code', 'occ_title', 'task_id'])\n",
    "    onet = onet[onet['year'] == 2023].reset_index(drop=True)\n",
    "\n",
    "    # Get list of tasks\n",
    "    my_df = onet[(onet.occ_code == f'{occupation_code}') & (onet.year == 2023)]\n",
    "    tasks = my_df['task'].unique().tolist()\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7085e",
   "metadata": {},
   "source": [
    "### Generate all possible partition schemes for the set of tasks (ignoring structre of the DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4a503b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def partitions(set_):\n",
    "    if not set_:\n",
    "        yield []\n",
    "        return\n",
    "    for i in range(1, len(set_) + 1):\n",
    "        for part in combinations(set_, i):\n",
    "            remaining = set(set_) - set(part)\n",
    "            if not remaining:\n",
    "                yield [list(part)]\n",
    "            else:\n",
    "                for b in partitions(list(remaining)):\n",
    "                    yield [list(part)] + b\n",
    "\n",
    "def generate_unique_partitions(numbers):\n",
    "    all_partitions = set()\n",
    "    for partition in partitions(numbers):\n",
    "        # Create a frozenset of frozensets to make each partition hashable and order-independent\n",
    "        partition_set = frozenset(frozenset(part) for part in partition)\n",
    "        all_partitions.add(partition_set)\n",
    "    \n",
    "    # Convert the frozensets back to lists for the final output\n",
    "    unique_partitions = [list(map(list, partition)) for partition in all_partitions]\n",
    "\n",
    "    # Sort elements\n",
    "    unique_partitions = sorted([sorted(x) for x in unique_partitions], key=len)\n",
    "    return unique_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081384b",
   "metadata": {},
   "source": [
    "### Check if partition scheme is \"valid\" (i.e., if its non-singleton partitions are a connected graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14339429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_connected(matrix):\n",
    "    # Number of nodes in the matrix\n",
    "    num_nodes = matrix.shape[0]\n",
    "    \n",
    "    # Visited array to keep track of visited nodes\n",
    "    visited = np.zeros(num_nodes, dtype=bool)\n",
    "    \n",
    "    # Helper function to perform DFS\n",
    "    def dfs(node):\n",
    "        visited[node] = True\n",
    "        # Visit all the neighbors of the current node\n",
    "        for neighbor in range(num_nodes):\n",
    "            if matrix[node, neighbor] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "            elif matrix[neighbor, node] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "    \n",
    "    # Start DFS from the first node (node 0)\n",
    "    dfs(0)\n",
    "    \n",
    "    # If all nodes are visited, the matrix is connected\n",
    "    return np.all(visited)\n",
    "\n",
    "\n",
    "def validate_partition_using_connectedness(adjacency_matrix, tasks_list):\n",
    "    # Return valid if Singleton\n",
    "    if len(tasks_list) == 1:\n",
    "        return True\n",
    "    # Check if partition forms connected graph\n",
    "    else:\n",
    "        # Subset original adjacency matrix\n",
    "        subset_matrix = adjacency_matrix[np.ix_(tasks_list, tasks_list)]\n",
    "\n",
    "        # check if subset matrix is a connected graph\n",
    "        subset_matrix_connected = is_connected(subset_matrix)\n",
    "\n",
    "        # return true if connected and false otherwise\n",
    "        return subset_matrix_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bac775ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_boundary(adjacency_matrix, partition):\n",
    "    # create a matrix whose columns are nodes not in the partition and whose rows are nodes in the partition\n",
    "    # (subset adjacency matrix to outgoing edges of partition nodes --i.e., rows-- and incoming edges of non-partition nodes --i.e., columns.)\n",
    "    reduced_matrix = np.delete(adjacency_matrix, partition, axis=1) \n",
    "    reduced_matrix = reduced_matrix[partition, :]\n",
    "\n",
    "    # find nodes in partition w/ an edge to non-partition nodes\n",
    "    partition_boundary_tasks = [i for i in partition if np.any(reduced_matrix[partition.index(i), :])]\n",
    "\n",
    "    return partition_boundary_tasks\n",
    "\n",
    "\n",
    "def compute_plan_cost(adjacency_matrix, M_dict, A_dict, D_dict, AI_quality, execution_plan, human_tasks):\n",
    "    # initialize costs\n",
    "    total_cost = 0\n",
    "    labor_cost = 0\n",
    "    management_cost = 0\n",
    "\n",
    "    for partition in execution_plan:\n",
    "        if len(partition) == 1:\n",
    "            if partition[0] in human_tasks:\n",
    "                partition_cost = sum(M_dict[key] for key in partition)\n",
    "                labor_cost += partition_cost\n",
    "            else:\n",
    "                AI_cost = sum(A_dict[key] for key in partition)\n",
    "                difficulty = sum(D_dict[key] for key in partition)\n",
    "                partition_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "                management_cost += partition_cost\n",
    "        else:\n",
    "            # calculate automated-chain management cost\n",
    "            partition_boundary_tasks = get_partition_boundary(adjacency_matrix, partition)\n",
    "            AI_cost = sum(A_dict[key] for key in partition_boundary_tasks)\n",
    "            difficulty = sum(D_dict[key] for key in partition)\n",
    "            partition_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "            management_cost += partition_cost\n",
    "        \n",
    "        total_cost += partition_cost\n",
    "\n",
    "    return total_cost, labor_cost, management_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7354c2d",
   "metadata": {},
   "source": [
    "### Combine steps into a function to run a for loop over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e122fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAG_indiffCurve(input_path, output_path, unique_partitions, alpha_list):\n",
    "    # read DAG\n",
    "    dag_df = pd.read_csv(input_path)\n",
    "\n",
    "    # remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "    if 'comment' in dag_df.columns:\n",
    "        dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "    # get task stats\n",
    "    tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # extract list of tasks and create a dictionary for indexing tasks\n",
    "    tasks_list = tasks_stats['task'].unique()\n",
    "    tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "    # create numpy array of adjacency matrix\n",
    "    adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    for _, row in dag_df.iterrows():\n",
    "        source_index = aux_dict[row['source']]\n",
    "        target_index = aux_dict[row['target']]\n",
    "        adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Define a break-even difficulty for base AI quality (alpha)\n",
    "    # Above break-even difficulty threshold task is done manually\n",
    "    # As AI quality (alpha) goes up break-even difficulty goes up\n",
    "    # for index, alpha in enumerate(alpha_list):\n",
    "    #     if index % np.floor(n/4) == np.floor(n/4) - 1:\n",
    "    #         pretty_label = str(np.round(alpha,2)*100).split('.')[0]\n",
    "    #         #tasks_stats[f'be_difficulty_{pretty_label}'] = np.log(tasks_stats['management_cost'] / tasks_stats['human_cost']) / np.log(alpha)\n",
    "\n",
    "\n",
    "    # add task_dict key and reset index\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    tasks_stats['dict_index'] = tasks_stats.apply(lambda row: aux_dict[row.task], axis=1)\n",
    "    tasks_stats = tasks_stats.sort_values(by='dict_index')\n",
    "    tasks_stats = tasks_stats.set_index('dict_index', drop=False)\n",
    "    tasks_stats.index.name = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # create dictionaries for human cost, management cost, and difficulty\n",
    "    M_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['human_cost']))\n",
    "    A_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['management_cost']))\n",
    "    D_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['difficulty']))\n",
    "\n",
    "\n",
    "\n",
    "    # Get valid partitioning schemes\n",
    "    valid_partitions = []\n",
    "    for scheme in unique_partitions:\n",
    "        # Set valid partitions count to 0\n",
    "        valid_partition_count = 0\n",
    "        for partition in scheme:\n",
    "            valid_partition = validate_partition_using_connectedness(adjacency_matrix, partition)\n",
    "            if valid_partition:\n",
    "                valid_partition_count += 1\n",
    "        \n",
    "        # If number of valid partitions within a partition scheme is equal to \n",
    "        # number of partitions in partition scheme then partition scheme is valid\n",
    "        if valid_partition_count == len(scheme):\n",
    "            valid_partitions.append(scheme)\n",
    "\n",
    "    # Print stats\n",
    "    print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # get how many \"singleton\" partitions there are in valid partition\n",
    "    valid_execution_plans = pd.DataFrame()\n",
    "    for my_valid_partition in valid_partitions:\n",
    "        singleton_partitions = [lst[0] for lst in my_valid_partition if len(lst) == 1]\n",
    "        #singleton_partitions = [lst for lst in my_valid_partition if len(lst) == 1]\n",
    "\n",
    "        # get the power set of \"singleton\" partitions\n",
    "        # goal is to generate ways singleton tasks can be done by human or AI\n",
    "        all_combinations = [[]]\n",
    "        for r in range(1, len(singleton_partitions) + 1):\n",
    "            combinations_r = itertools.combinations(singleton_partitions, r)\n",
    "            all_combinations.extend(combinations_r)\n",
    "\n",
    "        # Convert the combinations to a list of lists (optional)\n",
    "        all_combinations = [list(comb) for comb in all_combinations]\n",
    "        all_combinations\n",
    "\n",
    "        # repeat my_valid_partition for each combination in all_combinations to create a dataframe later\n",
    "        my_valid_partition_repeated = [my_valid_partition for _ in range(len(all_combinations))]\n",
    "        aux_df = pd.DataFrame({'execution_plan': my_valid_partition_repeated, \n",
    "                            'human_tasks': all_combinations})\n",
    "        \n",
    "        # append to valid_execution_plans\n",
    "        valid_execution_plans = pd.concat([valid_execution_plans, aux_df], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # calculate plan costs for each alpha\n",
    "    execution_plan_costs_df = pd.DataFrame()\n",
    "    for counter, alpha in enumerate(alpha_list):\n",
    "        my_alpha_execution_plan_costs_df = pd.DataFrame()\n",
    "        for execution_plan, human_tasks in zip(valid_execution_plans['execution_plan'], valid_execution_plans['human_tasks']):\n",
    "            # calculate plan costs\n",
    "            total_cost, labor_cost, management_cost = compute_plan_cost(adjacency_matrix, M_dict, A_dict, D_dict, alpha, execution_plan, human_tasks)\n",
    "\n",
    "            # create a dataframe to store execution plan costs\n",
    "            aux_df = pd.DataFrame({'alpha': [alpha], \n",
    "                                   'execution_plan': [execution_plan],\n",
    "                                   'human_tasks': [human_tasks],\n",
    "                                   'total_cost': [total_cost],\n",
    "                                   'labor_cost': [labor_cost],\n",
    "                                   'management_cost': [management_cost]})\n",
    "            \n",
    "            # append to execution_plan_costs_df\n",
    "            my_alpha_execution_plan_costs_df = pd.concat([my_alpha_execution_plan_costs_df, aux_df], ignore_index=True)\n",
    "        \n",
    "\n",
    "        # find optimal execution plan\n",
    "        my_alpha_execution_plan_costs_df['min_total_cost_flag'] = (my_alpha_execution_plan_costs_df['total_cost'] == my_alpha_execution_plan_costs_df['total_cost'].min())\n",
    "\n",
    "        # append to master dataframe\n",
    "        execution_plan_costs_df = pd.concat([execution_plan_costs_df, my_alpha_execution_plan_costs_df], ignore_index=True)\n",
    "    execution_plan_costs_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ec907",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ee7f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# set alpha as AI quality metric\n",
    "epsilon = 1e-8\n",
    "alpha_list = [epsilon, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1-epsilon]\n",
    "\n",
    "onet_data_path = f'{data_path}/data/onet_occupations_yearly.csv'\n",
    "\n",
    "occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "                   'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "                   'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists', \n",
    "                   'personalCareAides', 'proofreadersAndCopyMarkers', 'chiropractors', \n",
    "                   'shippingReceivingAndInventoryClerks', 'cooksShortOrder', 'orthodontists',\n",
    "                   'subwayAndStreetcarOperators', 'packersAndPackagersHand', 'hoistAndWinchOperators', \n",
    "                   'forgingMachineSettersOperatorsAndTenders', 'avionicsTechnicians', 'dishwashers', \n",
    "                   'dispatchersExceptPoliceFireAndAmbulance', 'familyMedicinePhysicians', 'MachineFeedersAndOffbearers'\n",
    "                   ]\n",
    "\n",
    "occupation_list = ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators'\n",
    "                   ]\n",
    "\n",
    "\n",
    "\n",
    "occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "                   'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "                   'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors'\n",
    "                   ]\n",
    "\n",
    "# occupation_list = ['travelAgents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3a99b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Running: pileDriverOperators ----------------------\n",
      "Number of non-target tasks: 5\n",
      "Time to generate all possible partition schemes: 0.00 seconds\n",
      "Number of all possible partitioning schemes: 52\n",
      "\n",
      "-------Running: pileDriverOperators - Manual DAG-------\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "\n",
      "pileDriverOperators Manual DAG runtime: 0.37 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Naive DAG-------\n",
      "Number of valid partitioning schemes given DAG structure: 30\n",
      "\n",
      "pileDriverOperators Naive DAG runtime: 0.35 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned Naive DAG-------\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "\n",
      "pileDriverOperators Conditioned Naive DAG runtime: 0.31 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - First-Last Task DAG-------\n",
      "Number of valid partitioning schemes given DAG structure: 30\n",
      "\n",
      "pileDriverOperators First-Last Task DAG runtime: 0.34 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned First-Last Task DAG-------\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "\n",
      "pileDriverOperators Conditioned First-Last Task DAG runtime: 0.32 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Partitioned DAG-------\n",
      "Number of valid partitioning schemes given DAG structure: 52\n",
      "\n",
      "pileDriverOperators Partitioned DAG runtime: 0.48 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned Partitioned DAG-------\n",
      "Number of valid partitioning schemes given DAG structure: 34\n"
     ]
    }
   ],
   "source": [
    "num_tasks_current = 0\n",
    "num_tasks_previous = 0\n",
    "for occupation in occupation_list:\n",
    "    print(f'\\n---------------------- Running: {occupation} ----------------------')\n",
    "    occupation_start_time = time.time()\n",
    "\n",
    "    # generate occupation-specific strings\n",
    "    GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)\n",
    "\n",
    "\n",
    "    # Get occupation tasks to create all possible partitions\n",
    "    tasks = get_tasks(onet_data_path, occupation_code)\n",
    "    num_tasks_current = len(tasks)\n",
    "    print(f'Number of non-target tasks: {num_tasks_current}')\n",
    "\n",
    "    if num_tasks_current < 10:\n",
    "        n = 1000\n",
    "    else: \n",
    "        n = 100\n",
    "\n",
    "    # if number of tasks in new occupation has increased generate new set of possible partitions\n",
    "    if num_tasks_current != num_tasks_previous:\n",
    "        unique_partitions_start_time = time.time()\n",
    "\n",
    "        # Generate list of numbers for non-\"Target\" tasks in occupation\n",
    "        tasks_list_numbers = list(range(num_tasks_current))\n",
    "\n",
    "        # Generate all possible partitioning schemes\n",
    "        unique_partitions = generate_unique_partitions(tasks_list_numbers)\n",
    "        unique_partitions_end_time = time.time()\n",
    "\n",
    "        unique_partitions_execution_time = unique_partitions_end_time - unique_partitions_start_time\n",
    "        print(f'Time to generate all possible partition schemes: {unique_partitions_execution_time:.2f} seconds')\n",
    "    \n",
    "    # update num_tasks_previous for next iteration and print stats\n",
    "    num_tasks_previous = num_tasks_current\n",
    "    print(f'Number of all possible partitioning schemes: {len(unique_partitions)}')\n",
    "\n",
    "\n",
    "    # Manual DAG\n",
    "    M_input_path = f'{occupation_folder}/{occupation}_M_DAG_df.csv'\n",
    "    M_output_path = f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_M.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    N_input_path = f'{occupation_folder}/{occupation}_N_GPT_DAG_df.csv'\n",
    "    N_output_path = f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_N.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    CN_input_path = f'{occupation_folder}/{occupation}_CN_GPT_DAG_df.csv'\n",
    "    CN_output_path = f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_CN.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    FLT_input_path = f'{occupation_folder}/{occupation}_FLT_GPT_DAG_df.csv'\n",
    "    FLT_output_path = f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_FLT.csv'\n",
    "\n",
    "    # Conditioned First Last Task DAG\n",
    "    CFLT_input_path = f'{occupation_folder}/{occupation}_CFLT_GPT_DAG_df.csv'\n",
    "    CFLT_output_path = f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_CFLT.csv'\n",
    "\n",
    "    # Partitioned DAG\n",
    "    P_input_path = f'{occupation_folder}/{occupation}_P_GPT_DAG_df.csv'\n",
    "    P_output_path = f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_P.csv'\n",
    "\n",
    "    # Conditioned Partitioned DAG\n",
    "    CP_input_path = f'{occupation_folder}/{occupation}_CP_GPT_DAG_df.csv'\n",
    "    CP_output_path = f'{occupation_folder}/indiffCurves/{occupation}_indiffCurves_CP.csv'\n",
    "    \n",
    "\n",
    "\n",
    "    # create list of all DAGs\n",
    "    if occupation in ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators']:\n",
    "        DAG_indicator_list = ['Manual DAG', 'Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [M_input_path, N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_paths_list = [M_output_path, N_output_path, CN_output_path, FLT_output_path, CFLT_output_path, P_output_path, CP_output_path]\n",
    "    else:\n",
    "        DAG_indicator_list = ['Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_paths_list = [N_output_path, CN_output_path, FLT_output_path, CFLT_output_path, P_output_path, CP_output_path]\n",
    "\n",
    "\n",
    "    for DAG_indicator, input_path, output_path in zip(DAG_indicator_list, input_paths_list, output_paths_list):\n",
    "        print(f'\\n-------Running: {occupation} - {DAG_indicator}-------')\n",
    "        \n",
    "        DAG_start_time = time.time()\n",
    "        DAG_indiffCurve(input_path, output_path, unique_partitions, alpha_list)\n",
    "        DAG_end_time = time.time()\n",
    "\n",
    "        DAG_execution_time = DAG_end_time - DAG_start_time\n",
    "        print(f\"\\n{occupation} {DAG_indicator} runtime: {DAG_execution_time:.2f} seconds\")\n",
    "\n",
    "    occupation_end_time = time.time()\n",
    "    occupation_execution_time = (occupation_end_time - occupation_start_time)/60\n",
    "    print(f\"\\n\\n************* {occupation} runtime: {occupation_execution_time:.2f} minutes *************\")\n",
    "    runtime_since_start = (time.time() - start_time)/60\n",
    "    print(f\"\\nruntime since start: {runtime_since_start:.2f} minutes\\n\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"\\n\\nTotal Runtime: {execution_time:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
