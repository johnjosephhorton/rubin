{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e0c01952",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('all')\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymansh':\n",
    "    main_folder_path = '/Users/peymansh/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2a30342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick occupation\n",
    "def pick_occupation(occupation):\n",
    "    if occupation == 'travelAgents':\n",
    "        GPT_input_occupation = 'travel agents'\n",
    "        plot_title_occupation = 'Travel Agents'\n",
    "        occupation_code = '41-3041'\n",
    "    elif occupation == 'insuranceUnderwriters':\n",
    "        GPT_input_occupation = 'insurance underwriters'\n",
    "        plot_title_occupation = 'Insurance Underwriters'\n",
    "        occupation_code = '13-2053'\n",
    "    \n",
    "    occupation_folder = f'{data_path}/daily_tasks_occupations_analysis/{occupation}'\n",
    "    return GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2b76c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha as AI quality metric\n",
    "n = 1000\n",
    "epsilon = 1e-8\n",
    "alpha_list = np.linspace(epsilon, 1-epsilon, n).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9747ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick occupation and initialize variables\n",
    "occupation = 'travelAgents'\n",
    "# occupation = 'insuranceUnderwriters'\n",
    "\n",
    "GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40506e",
   "metadata": {},
   "source": [
    "### Initialize input-output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3219e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffix = 'MAX_' # for when partition cost used MAX of machine costs in partition (somewhat like a least common multiple)\n",
    "suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "200fa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_manual_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}manual.csv'\n",
    "\n",
    "# # First Last Task DAG\n",
    "# input_path = f'{occupation_folder}/v1/{occupation}_firstLastTaskGPT_DAG_df.csv'\n",
    "# output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}firstLastTask.csv'\n",
    "\n",
    "# # Conditioned First Last Task DAG\n",
    "# input_path = f'{occupation_folder}/v1/{occupation}_conditionedGPT_fromFirstLastTask_DAG_df.csv'\n",
    "# output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}firstLastTask_conditioned.csv'\n",
    "\n",
    "# # Partitioned DAG\n",
    "# input_path = f'{occupation_folder}/v1/{occupation}_partitionedGPT_DAG_df.csv'\n",
    "# output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}partitioned.csv'\n",
    "\n",
    "# # Conditioned Partitioned DAG\n",
    "# input_path = f'{occupation_folder}/v1/{occupation}_conditionedGPT_fromPartitioned_DAG_df.csv'\n",
    "# output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}partitioned_conditioned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bd525be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Authorize reinsurance of policy when risk is high.',\n",
       " 1: 'Decline excessive risks.',\n",
       " 2: 'Examine documents to determine degree of risk from factors such as applicant health, financial standing and value, and condition of property.',\n",
       " 3: 'Evaluate possibility of losses due to catastrophe or excessive insurance.',\n",
       " 4: 'Write to field representatives, medical personnel, or others to obtain further information, quote rates, or explain company underwriting policies.',\n",
       " 5: 'Review company records to determine amount of insurance in force on single risk or group of closely related risks.',\n",
       " 6: 'Decrease value of policy when risk is substandard and specify applicable endorsements or apply rating to ensure safe, profitable distribution of risks, using reference materials.'}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read DAG\n",
    "dag_df = pd.read_csv(input_path)\n",
    "\n",
    "# remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "if 'comment' in dag_df.columns:\n",
    "    dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "# extract list of tasks and create a dictionary for indexing tasks\n",
    "tasks_list = list(set(dag_df['source']).union(set(dag_df['target'])))\n",
    "tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "# create numpy array of adjacency matrix\n",
    "adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "\n",
    "# Populate the adjacency matrix\n",
    "aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "for _, row in dag_df.iterrows():\n",
    "    source_index = aux_dict[row['source']]\n",
    "    target_index = aux_dict[row['target']]\n",
    "    adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "tasks_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "878246a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>human_cost</th>\n",
       "      <th>machine_cost</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Authorize reinsurance of policy when risk is h...</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decline excessive risks.</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Examine documents to determine degree of risk ...</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Evaluate possibility of losses due to catastro...</td>\n",
       "      <td>240</td>\n",
       "      <td>30</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write to field representatives, medical person...</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review company records to determine amount of ...</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decrease value of policy when risk is substand...</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                task  human_cost  \\\n",
       "0  Authorize reinsurance of policy when risk is h...          60   \n",
       "1                           Decline excessive risks.          60   \n",
       "2  Examine documents to determine degree of risk ...          60   \n",
       "3  Evaluate possibility of losses due to catastro...         240   \n",
       "4  Write to field representatives, medical person...          30   \n",
       "5  Review company records to determine amount of ...         120   \n",
       "6  Decrease value of policy when risk is substand...          60   \n",
       "\n",
       "   machine_cost  difficulty  \n",
       "0            10        0.85  \n",
       "1             5        0.75  \n",
       "2            30        0.85  \n",
       "3            30        0.85  \n",
       "4            30        0.75  \n",
       "5            30        0.75  \n",
       "6            10        0.85  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get task stats\n",
    "tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "\n",
    "# Define a break-even difficulty for base AI quality (alpha)\n",
    "# Above break-even difficulty threshold task is done manually\n",
    "# As AI quality (alpha) goes up break-even difficulty goes up\n",
    "for index, alpha in enumerate(alpha_list):\n",
    "    if index % np.floor(n/4) == np.floor(n/4) - 1:\n",
    "        pretty_label = str(np.round(alpha,2)*100).split('.')[0]\n",
    "        #tasks_stats[f'be_difficulty_{pretty_label}'] = np.log(tasks_stats['machine_cost'] / tasks_stats['human_cost']) / np.log(alpha)\n",
    "\n",
    "# add task_dict key and reset index\n",
    "aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "tasks_stats['dict_index'] = tasks_stats.apply(lambda row: aux_dict[row.task], axis=1)\n",
    "tasks_stats = tasks_stats.sort_values(by='dict_index')\n",
    "tasks_stats = tasks_stats.set_index('dict_index', drop=False)\n",
    "tasks_stats.index.name = None\n",
    "\n",
    "tasks_stats.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "08847641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_cost      630.00\n",
       "machine_cost    145.00\n",
       "difficulty        5.65\n",
       "dtype: float64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionaries for human cost, machine cost, and difficulty\n",
    "M_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['human_cost']))\n",
    "A_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['machine_cost']))\n",
    "D_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['difficulty']))\n",
    "\n",
    "# print stats\n",
    "tasks_stats.iloc[:,1:-1].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2663a1",
   "metadata": {},
   "source": [
    "### Generate all possible partition schemes for the set of tasks (ignoring structre of the DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a3a99b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def partitions(set_):\n",
    "    if not set_:\n",
    "        yield []\n",
    "        return\n",
    "    for i in range(1, len(set_) + 1):\n",
    "        for part in combinations(set_, i):\n",
    "            remaining = set(set_) - set(part)\n",
    "            if not remaining:\n",
    "                yield [list(part)]\n",
    "            else:\n",
    "                for b in partitions(list(remaining)):\n",
    "                    yield [list(part)] + b\n",
    "\n",
    "def generate_unique_partitions(numbers):\n",
    "    all_partitions = set()\n",
    "    for partition in partitions(numbers):\n",
    "        # Create a frozenset of frozensets to make each partition hashable and order-independent\n",
    "        partition_set = frozenset(frozenset(part) for part in partition)\n",
    "        all_partitions.add(partition_set)\n",
    "    \n",
    "    # Convert the frozensets back to lists for the final output\n",
    "    unique_partitions = [list(map(list, partition)) for partition in all_partitions]\n",
    "\n",
    "    # Sort elements\n",
    "    unique_partitions = sorted([sorted(x) for x in unique_partitions], key=len)\n",
    "    return unique_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1b1c4",
   "metadata": {},
   "source": [
    "### Check if partition scheme is \"valid\" (i.e., if its non-singleton partitions are a connected graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "298ba0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_connected(matrix):\n",
    "    # Number of nodes in the matrix\n",
    "    num_nodes = matrix.shape[0]\n",
    "    \n",
    "    # Visited array to keep track of visited nodes\n",
    "    visited = np.zeros(num_nodes, dtype=bool)\n",
    "    \n",
    "    # Helper function to perform DFS\n",
    "    def dfs(node):\n",
    "        visited[node] = True\n",
    "        # Visit all the neighbors of the current node\n",
    "        for neighbor in range(num_nodes):\n",
    "            if matrix[node, neighbor] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "            elif matrix[neighbor, node] == 1 and not visited[neighbor]:\n",
    "                dfs(neighbor)\n",
    "    \n",
    "    # Start DFS from the first node (node 0)\n",
    "    dfs(0)\n",
    "    \n",
    "    # If all nodes are visited, the matrix is connected\n",
    "    return np.all(visited)\n",
    "\n",
    "\n",
    "def validate_partition_using_connectedness(adjacency_matrix, tasks_list):\n",
    "    # Return valid if Singleton\n",
    "    if len(tasks_list) == 1:\n",
    "        return True\n",
    "    # Check if partition forms connected graph\n",
    "    else:\n",
    "        # Subset original adjacency matrix\n",
    "        subset_matrix = adjacency_matrix[np.ix_(tasks_list, tasks_list)]\n",
    "\n",
    "        # check if subset matrix is a connected graph\n",
    "        subset_matrix_connected = is_connected(subset_matrix)\n",
    "\n",
    "        # return true if connected and false otherwise\n",
    "        return subset_matrix_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "478d9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 170\n",
      "\n",
      "Example partitions:\n",
      "[[0], [1, 2, 3, 4, 5, 6]]\n",
      "[[0, 1, 2, 5, 6], [3, 4]]\n",
      "[[0, 2, 3, 4, 6], [1, 5]]\n",
      "[[0, 1, 2, 3, 5, 6], [4]]\n",
      "[[0, 6], [1, 5], [2, 3, 4]]\n",
      "[[0], [1, 2, 3, 4, 6], [5]]\n",
      "[[0, 3, 4, 6], [1], [2, 5]]\n",
      "[[0, 3, 4], [1, 2, 6], [5]]\n",
      "[[0], [1, 2, 4, 5, 6], [3]]\n",
      "[[0, 1, 2, 4, 6], [3], [5]]\n"
     ]
    }
   ],
   "source": [
    "# Generate list of numbers for tasks in occupation\n",
    "tasks_list_numbers = list(range(len(tasks_list)))\n",
    "\n",
    "# Generate all possible partitioning schemes\n",
    "all_partitions = generate_unique_partitions(tasks_list_numbers)\n",
    "\n",
    "# Get valid partitioning schemes\n",
    "valid_partitions = []\n",
    "for partition_scheme in all_partitions:\n",
    "\n",
    "    # Set valid partitions count to 0\n",
    "    valid_partition_count = 0\n",
    "    for partition in partition_scheme:\n",
    "        valid_partition = validate_partition_using_connectedness(adjacency_matrix, partition)\n",
    "        if valid_partition:\n",
    "            valid_partition_count += 1\n",
    "    \n",
    "    # If number of valid partitions within a partition scheme is equal to \n",
    "    # number of partitions in partition scheme then partition scheme is valid\n",
    "    if valid_partition_count == len(partition_scheme):\n",
    "        valid_partitions.append(partition_scheme)\n",
    "\n",
    "# Print stats\n",
    "print(f'Number of all possible partitioning schemes: {len(all_partitions)}')\n",
    "print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "# print some partitions\n",
    "print('\\nExample partitions:')\n",
    "for partition in valid_partitions[10:20]:\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45c08d",
   "metadata": {},
   "source": [
    "### Compute minimum cost for a given partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ab88fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_boundary(adjacency_matrix, partition):\n",
    "    # create a matrix whose columns are nodes not in the partition and whose rows are nodes in the partition\n",
    "    # (subset adjacency matrix to outgoing edges of partition nodes --i.e., rows-- and incoming edges of non-partition nodes --i.e., columns.)\n",
    "    reduced_matrix = np.delete(adjacency_matrix, partition, axis=1) \n",
    "    reduced_matrix = reduced_matrix[partition, :]\n",
    "\n",
    "    # find nodes in partition w/ an edge to non-partition nodes\n",
    "    partition_boundary_tasks = [i for i in partition if np.any(reduced_matrix[partition.index(i), :])]\n",
    "\n",
    "    return partition_boundary_tasks\n",
    "\n",
    "\n",
    "def compute_partition_cost(adjacency_matrix, M_dict, A_dict, D_dict, AI_quality, partition):\n",
    "    # initialize partition done manually as False \n",
    "    # (only if partition is singleton and manual cost <= automated cost partition is done manually)\n",
    "    partition_done_manually = False\n",
    "    \n",
    "    # calculate automation cost of doing partition\n",
    "    # first, get partition boundary tasks if partition contains more than one task\n",
    "    if len(partition) > 1:\n",
    "        partition_boundary_tasks = get_partition_boundary(adjacency_matrix, partition)\n",
    "        \n",
    "        \n",
    "    # if partition boundary has zero length partition is invalid\n",
    "        if len(partition_boundary_tasks) == 0:\n",
    "            partition_cost = 100000000 # (value doesn't matter)\n",
    "            partition_done_manually = False\n",
    "            partition_is_valid = False\n",
    "            return partition_cost, partition_done_manually, partition_is_valid\n",
    "\n",
    "\n",
    "    # if partition is a singleton pick minimum of manual and machine cost\n",
    "    if len(partition) == 1:\n",
    "        partition_is_valid = True\n",
    "\n",
    "        # calculate manual cost\n",
    "        manual_cost = sum(M_dict[key] for key in partition)\n",
    "\n",
    "        # calculate machine cost\n",
    "        AI_cost = sum(A_dict[key] for key in partition)\n",
    "        difficulty = sum(D_dict[key] for key in partition)\n",
    "        automation_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "        \n",
    "        # pick the minimum of the two\n",
    "        if manual_cost < automation_cost:\n",
    "            partition_cost = manual_cost\n",
    "            partition_done_manually = True \n",
    "        else:\n",
    "            partition_cost = automation_cost\n",
    "    \n",
    "\n",
    "    # if partition not a singleton calculate automation cost and return if partition passes a sanity check\n",
    "    if len(partition) > 1:\n",
    "\n",
    "        # calculate manual cost\n",
    "        manual_cost = sum(M_dict[key] for key in partition)\n",
    "\n",
    "\n",
    "        # calculate machine cost\n",
    "        # first get boundary tasks in partition\n",
    "        partition_boundary_tasks = get_partition_boundary(adjacency_matrix, partition)\n",
    "\n",
    "        # if partition has no boundary tasks partition is invalid\n",
    "        if len(partition_boundary_tasks) == 0:\n",
    "            partition_cost = 100000000 # (value doesn't matter)\n",
    "            partition_is_valid = False\n",
    "            return partition_cost, partition_done_manually, partition_is_valid \n",
    "        \n",
    "        # if partition has at least one boundary task calculate automation cost using boundary tasks for calculating machine costs and partition tasks for difficulty\n",
    "        if len(partition_boundary_tasks) > 0:\n",
    "            AI_cost = sum(A_dict[key] for key in partition_boundary_tasks)\n",
    "            difficulty = sum(D_dict[key] for key in partition)\n",
    "            automation_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "\n",
    "        # sanity check partition validity: if manual cost < automation cost partition is invalid (should not have been formed)\n",
    "        if manual_cost < automation_cost:\n",
    "            partition_cost = 100000000 # (value doesn't matter)\n",
    "            partition_is_valid = False\n",
    "        else:\n",
    "            partition_cost = automation_cost\n",
    "            partition_is_valid = True\n",
    "    \n",
    "    return partition_cost, partition_done_manually, partition_is_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d2185",
   "metadata": {},
   "source": [
    "### Compute costs of all \"valid\" execution plans\n",
    "#### New check for validity: automated cost of tasks in non-singleton partition must be less than human costs doing partition tasks separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "11e0cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid execution plans: 144\n",
      "\n",
      "Example Execution Plans:\n",
      "[[0, 2, 3, 4, 6], [1, 5]]\n",
      "[[0, 1, 2, 3, 5, 6], [4]]\n",
      "[[0], [1, 2, 3, 4, 6], [5]]\n",
      "[[0, 3, 4, 6], [1], [2, 5]]\n",
      "[[0, 3, 4], [1, 2, 6], [5]]\n"
     ]
    }
   ],
   "source": [
    "########## Random Thought: maybe better to sort valid_partitions on descending partition order to avoid recalculating single node partitions everytime? \n",
    "# tho the downside is that we have to first do the heavy calculations first...\n",
    "\n",
    "\n",
    "def execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, alpha):\n",
    "    execution_plan = []\n",
    "    execution_plan_manual_tasks = []\n",
    "    execution_cost = []\n",
    "    counter = 0\n",
    "    for partition_scheme in valid_partitions:\n",
    "        # initialize partition scheme cost\n",
    "        # and partitions that are done manually\n",
    "        partition_scheme_cost = 0\n",
    "        manual_partitions = []\n",
    "        \n",
    "        for partition in partition_scheme:\n",
    "            # calculate partition cost \n",
    "            partition_cost, partition_done_manually, partition_is_valid = compute_partition_cost(adjacency_matrix, M_dict, A_dict, D_dict, alpha, partition)\n",
    "        \n",
    "            # if (automated) partition is invalid ignore partition scheme\n",
    "            # and stop calculating costs of further partitions\n",
    "            if not partition_is_valid:\n",
    "                break\n",
    "\n",
    "            if partition_done_manually:\n",
    "                manual_partitions.append(partition)\n",
    "\n",
    "            # if (automated) partition passes sanity check\n",
    "            # add this partition's cost to partition scheme cost\n",
    "            partition_scheme_cost += partition_cost\n",
    "        \n",
    "        # if stopped because an (automated) partition wasn't valid\n",
    "        # ignore current partition scheme and continue\n",
    "        if not partition_is_valid:\n",
    "            continue\n",
    "        \n",
    "        # if partition scheme makes sense append costs\n",
    "        execution_plan.append(partition_scheme)\n",
    "        execution_plan_manual_tasks.append(manual_partitions)\n",
    "        execution_cost.append(partition_scheme_cost)\n",
    "\n",
    "        # if counter % (np.floor(len(valid_partitions)/3)) == 0:\n",
    "        #     print(partition_scheme)\n",
    "        #     print(partition_scheme_cost)\n",
    "        #     print('\\n')\n",
    "        # counter += 1\n",
    "\n",
    "    return execution_plan, execution_plan_manual_tasks, execution_cost\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "execution_plan, execution_plan_manual_tasks, execution_cost = execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, alpha)\n",
    "print(f'Number of valid execution plans: {len(execution_plan)}')\n",
    "\n",
    "# print some valid execution plans\n",
    "print('\\nExample Execution Plans:')\n",
    "for plan in execution_plan[10:15]:\n",
    "    print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4df3d1",
   "metadata": {},
   "source": [
    "### Calculate minimum cost for each alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5848de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "minimum_cost_list = []\n",
    "number_of_optimal_schemes_list = []\n",
    "optimal_execution_plan_list = []\n",
    "optimal_plan_manualTasks_list = []\n",
    "optimal_plan_manualTasks_count_list = []\n",
    "for alpha in alpha_list:\n",
    "    # get list of execution plans and costs for this alpha\n",
    "    execution_plan, execution_plan_manual_tasks, execution_cost = execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, alpha)\n",
    "\n",
    "    # choose minimum\n",
    "    minimum_cost = min(execution_cost)\n",
    "    minimum_cost_index = [index for index, value in enumerate(execution_cost) if value == minimum_cost]\n",
    "\n",
    "    # in rare cases there are more than one optimal plan\n",
    "    if len(minimum_cost_index) > 2:\n",
    "        print(alpha)\n",
    "        for index in minimum_cost_index:\n",
    "            optimal_execution_scheme = execution_plan[index]\n",
    "            optimal_execution_manual_tasks = execution_plan_manual_tasks[index]\n",
    "            print(optimal_execution_scheme)\n",
    "            print(optimal_execution_manual_tasks)\n",
    "    else:\n",
    "        optimal_execution_scheme = execution_plan[minimum_cost_index[0]]\n",
    "        optimal_execution_manual_tasks = execution_plan_manual_tasks[minimum_cost_index[0]]\n",
    "    \n",
    "    # append lists\n",
    "    minimum_cost_list.append(minimum_cost)\n",
    "    number_of_optimal_schemes_list.append(len(minimum_cost_index))\n",
    "    optimal_execution_plan_list.append(optimal_execution_scheme)\n",
    "    optimal_plan_manualTasks_list.append(optimal_execution_manual_tasks)\n",
    "    optimal_plan_manualTasks_count_list.append(len(optimal_execution_manual_tasks))\n",
    "\n",
    "# save outputs\n",
    "output_df = pd.DataFrame({\n",
    "    'alpha': alpha_list,\n",
    "    'optimal_schemes_count': number_of_optimal_schemes_list,\n",
    "    'cost': minimum_cost_list,\n",
    "    'optimal_scheme': optimal_execution_plan_list,\n",
    "    'optimal_scheme_manual_tasks': optimal_plan_manualTasks_list,\n",
    "    'manual_tasks_count': optimal_plan_manualTasks_count_list\n",
    "})\n",
    "output_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
