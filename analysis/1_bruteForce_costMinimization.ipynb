{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e0c01952",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('all')\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymansh':\n",
    "    main_folder_path = '/Users/peymansh/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "2a30342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick occupation\n",
    "def pick_occupation(occupation):\n",
    "    if occupation == 'travelAgents':\n",
    "        GPT_input_occupation = 'travel agents'\n",
    "        plot_title_occupation = 'Travel Agents'\n",
    "        occupation_code = '41-3041'\n",
    "    elif occupation == 'insuranceUnderwriters':\n",
    "        GPT_input_occupation = 'insurance underwriters'\n",
    "        plot_title_occupation = 'Insurance Underwriters'\n",
    "        occupation_code = '13-2053'\n",
    "    \n",
    "    occupation_folder = f'{data_path}/daily_tasks_occupations_analysis/{occupation}'\n",
    "    return GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "2b76c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha as AI quality metric\n",
    "alpha_list = np.linspace(0.3, 1-1e-4, 100).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "9747ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick occupation and initialize variables\n",
    "occupation = 'travelAgents'\n",
    "occupation = 'insuranceUnderwriters'\n",
    "\n",
    "GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40506e",
   "metadata": {},
   "source": [
    "### Initialize input-output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3219e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'MAX_' # for when partition cost used MAX of machine costs in partition (somewhat like a least common multiple)\n",
    "suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "200fa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual DAG\n",
    "input_path = f'{occupation_folder}/{occupation}_manual_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}manual.csv'\n",
    "\n",
    "# # First Last Task DAG\n",
    "# input_path = f'{occupation_folder}/v1/{occupation}_firstLastTaskGPT_DAG_df.csv'\n",
    "# output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}firstLastTask.csv'\n",
    "\n",
    "# Conditioned First Last Task DAG\n",
    "input_path = f'{occupation_folder}/v1/{occupation}_conditionedGPT_fromFirstLastTask_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}firstLastTask_conditioned.csv'\n",
    "\n",
    "# # Partitioned DAG\n",
    "# input_path = f'{occupation_folder}/v1/{occupation}_partitionedGPT_DAG_df.csv'\n",
    "# output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}partitioned.csv'\n",
    "\n",
    "# Conditioned Partitioned DAG\n",
    "input_path = f'{occupation_folder}/v1/{occupation}_conditionedGPT_fromPartitioned_DAG_df.csv'\n",
    "output_path = f'{occupation_folder}/{occupation}_costMin_{suffix}partitioned_conditioned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "bd525be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DAG\n",
    "dag_df = pd.read_csv(input_path)\n",
    "\n",
    "# extract list of tasks and create a dictionary for indexing tasks\n",
    "tasks_list = list(set(dag_df['source']).union(set(dag_df['target'])))\n",
    "tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "# create numpy array of adjacency matrix\n",
    "adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "\n",
    "# Populate the adjacency matrix\n",
    "aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "for _, row in dag_df.iterrows():\n",
    "    source_index = aux_dict[row['source']]\n",
    "    target_index = aux_dict[row['target']]\n",
    "    adjacency_matrix[source_index, target_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "878246a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>human_cost</th>\n",
       "      <th>machine_cost</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>be_difficulty_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decline excessive risks.</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>0.575717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write to field representatives, medical person...</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.575717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Evaluate possibility of losses due to catastro...</td>\n",
       "      <td>240</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>1.151433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decrease value of policy when risk is substand...</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>0.575717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review company records to determine amount of ...</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>1.151433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Authorize reinsurance of policy when risk is h...</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.575717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Examine documents to determine degree of risk ...</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>0.575717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                task  human_cost  \\\n",
       "0                           Decline excessive risks.          60   \n",
       "1  Write to field representatives, medical person...          30   \n",
       "2  Evaluate possibility of losses due to catastro...         240   \n",
       "3  Decrease value of policy when risk is substand...          60   \n",
       "4  Review company records to determine amount of ...         120   \n",
       "5  Authorize reinsurance of policy when risk is h...          60   \n",
       "6  Examine documents to determine degree of risk ...          60   \n",
       "\n",
       "   machine_cost  difficulty  be_difficulty_3  \n",
       "0            30           8         0.575717  \n",
       "1            15           7         0.575717  \n",
       "2            60          10         1.151433  \n",
       "3            30          10         0.575717  \n",
       "4            30           8         1.151433  \n",
       "5            30          12         0.575717  \n",
       "6            30          10         0.575717  "
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get task stats\n",
    "tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "\n",
    "# define a break-even difficulty for base AI quality (alpha)\n",
    "# above break-even difficulty threshold task is done manually\n",
    "# as AI quality (alpha) goes up break-even difficulty goes up\n",
    "for alpha in alpha_list:\n",
    "    if alpha*10 % 1 != 0:\n",
    "        continue\n",
    "    tasks_stats[f'be_difficulty_{str(alpha)[-1]}'] = np.log(tasks_stats['machine_cost'] / tasks_stats['human_cost']) / np.log(alpha)\n",
    "tasks_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "08847641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_cost         630.00000\n",
       "machine_cost       225.00000\n",
       "difficulty          65.00000\n",
       "be_difficulty_3      5.18145\n",
       "dtype: float64"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add task_dict key for indexing purposes\n",
    "aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "tasks_stats['dict_index'] = tasks_stats.apply(lambda row: aux_dict[row.task], axis=1)\n",
    "tasks_stats = tasks_stats.sort_values(by='dict_index')\n",
    "\n",
    "# create dictionaries for human cost, machine cost, and difficulty\n",
    "M_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['human_cost']))\n",
    "A_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['machine_cost']))\n",
    "D_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['difficulty']))\n",
    "\n",
    "# print stats\n",
    "tasks_stats.iloc[:,1:-1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2663a1",
   "metadata": {},
   "source": [
    "### Generate all possible partitions for the set of tasks (ignoring structre of the DAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "a3a99b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def partitions(set_):\n",
    "    if not set_:\n",
    "        yield []\n",
    "        return\n",
    "    for i in range(1, len(set_) + 1):\n",
    "        for part in combinations(set_, i):\n",
    "            remaining = set(set_) - set(part)\n",
    "            if not remaining:\n",
    "                yield [list(part)]\n",
    "            else:\n",
    "                for b in partitions(list(remaining)):\n",
    "                    yield [list(part)] + b\n",
    "\n",
    "def generate_unique_partitions(numbers):\n",
    "    all_partitions = set()\n",
    "    for partition in partitions(numbers):\n",
    "        # Create a frozenset of frozensets to make each partition hashable and order-independent\n",
    "        partition_set = frozenset(frozenset(part) for part in partition)\n",
    "        all_partitions.add(partition_set)\n",
    "    \n",
    "    # Convert the frozensets back to lists for the final output\n",
    "    unique_partitions = [list(map(list, partition)) for partition in all_partitions]\n",
    "\n",
    "    # Sort elements\n",
    "    unique_partitions = sorted([sorted(x) for x in unique_partitions], key=len)\n",
    "    return unique_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1b1c4",
   "metadata": {},
   "source": [
    "### Check if partition is \"valid\"\n",
    "#### Partition is called valid if the partition subset of the DAG contains no singleton node and a path exists between a first node to all last nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "8be45e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a path exists using BFS (Breadth-First Search) in the subset matrix\n",
    "def bfs_path_exists(matrix, start, goal):\n",
    "    from collections import deque\n",
    "\n",
    "    visited = [False] * len(matrix)\n",
    "    queue = deque([start])\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        if current == goal:\n",
    "            return True\n",
    "        \n",
    "        for neighbor, connected in enumerate(matrix[current]):\n",
    "            if connected and not visited[neighbor]:\n",
    "                visited[neighbor] = True\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def validate_partition(adjacency_matrix, tasks_list):\n",
    "    # Return valid if Singleton\n",
    "    if len(tasks_list) == 1:\n",
    "        return True\n",
    "\n",
    "    # Subset original adjacency matrix\n",
    "    subset_matrix = adjacency_matrix[np.ix_(tasks_list, tasks_list)]\n",
    "\n",
    "    \n",
    "    first_tasks = []\n",
    "    last_tasks = []\n",
    "    for task in tasks_list:\n",
    "        # subset of original task in subsetted matrix\n",
    "        subset_index = tasks_list.index(task)\n",
    "\n",
    "        row_check = np.all(subset_matrix[subset_index, :] == 0)\n",
    "        column_check = np.all(subset_matrix[:, subset_index] == 0)\n",
    "\n",
    "        # Step 1: declare invalid if singleton task (task w/o incoming or outgoing edges in partition) present\n",
    "        if row_check and column_check: # task is a singleton\n",
    "            return False\n",
    "        \n",
    "        # Step 2: find first/last tasks (defined as tasks with no incoming/outgoing edges withing partition)\n",
    "        if row_check: # no outgoing edge within partition means last task\n",
    "            last_tasks.append(task)\n",
    "        if column_check: # no incoming edge within partition means first task\n",
    "            first_tasks.append(task)\n",
    "\n",
    "    #print(f'First Tasks: {first_tasks}')\n",
    "    #print(f'Last Tasks: {last_tasks}')\n",
    "\n",
    "    # Step 3: ensure a path between last tasks and a first task exists\n",
    "    counter = 0\n",
    "    for last in last_tasks:\n",
    "        subset_index_last = tasks_list.index(last)\n",
    "        for first in first_tasks:\n",
    "            subset_index_first = tasks_list.index(first)\n",
    "            path_exists = bfs_path_exists(subset_matrix, subset_index_first, subset_index_last)\n",
    "            if path_exists:\n",
    "                counter += 1\n",
    "                break # break inner loop (first_tasks loop)\n",
    "    \n",
    "    if counter == len(last_tasks): # if all last tasks have a path\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4f44fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all possible partitioning schemes: 877\n",
      "Number of valid partitioning schemes given DAG structure: 766\n",
      "\n",
      "Example partitions:\n",
      "[[0, 2, 3, 4, 6], [1, 5]]\n",
      "[[0, 1, 3, 4], [2], [5, 6]]\n",
      "[[0, 1, 2], [3, 5], [4, 6]]\n",
      "[[0, 1, 2, 6], [3], [4, 5]]\n",
      "[[0, 1, 3, 4], [2, 5], [6]]\n"
     ]
    }
   ],
   "source": [
    "# Generate list of numbers for tasks in occupation\n",
    "tasks_list_numbers = list(range(len(tasks_list)))\n",
    "\n",
    "# Generate all possible partitioning schemes\n",
    "all_partitions = generate_unique_partitions(tasks_list_numbers)\n",
    "\n",
    "# Get valid partitioning schemes\n",
    "valid_partitions = []\n",
    "for partition_scheme in all_partitions:\n",
    "\n",
    "    # Set valid partitions count to 0\n",
    "    valid_partition_count = 0\n",
    "    for partition in partition_scheme:\n",
    "        valid_partition = validate_partition(adjacency_matrix, partition)\n",
    "        if valid_partition:\n",
    "            valid_partition_count += 1\n",
    "    \n",
    "    # If number of valid partitions within a partition scheme is equal to \n",
    "    # number of partitions in partition scheme then partition scheme is valid\n",
    "    if valid_partition_count == len(partition_scheme):\n",
    "        valid_partitions.append(partition_scheme)\n",
    "\n",
    "# Print stats\n",
    "print(f'Number of all possible partitioning schemes: {len(all_partitions)}')\n",
    "print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "# print some partitions\n",
    "print('\\nExample partitions:')\n",
    "for partition in valid_partitions[40:45]:\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45c08d",
   "metadata": {},
   "source": [
    "### Compute minimum cost for a given partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ab88fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_partition_cost(M_dict, A_dict, D_dict, AI_quality, partition):\n",
    "    # calculate manual cost of doing partition\n",
    "    manual_cost = sum(M_dict[key] for key in partition)\n",
    "    #print(f'Manual cost: {manual_cost}')\n",
    "\n",
    "    # calculate automation cost of doing partition\n",
    "    AI_cost = sum(A_dict[key] for key in partition)\n",
    "    #AI_cost = max(A_dict[key] for key in partition)\n",
    "    difficulty = sum(D_dict[key] for key in partition)\n",
    "    automation_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "    #print(f'Automation cost: {automation_cost}')\n",
    "\n",
    "    # initialize partition done manually as False \n",
    "    # (only if partition is singleton and manual cost <= automated cost partition is done manually)\n",
    "    partition_done_manually = False\n",
    "\n",
    "    # if partition is a singleton do nothing\n",
    "    if len(partition) == 1:\n",
    "        partition_is_valid = True\n",
    "        if manual_cost < automation_cost:\n",
    "            partition_cost = manual_cost\n",
    "            partition_done_manually = True \n",
    "        else:\n",
    "            partition_cost = automation_cost\n",
    "    \n",
    "    # if partition not a singleton check if manual cost of doing multiple tasks lower than automating them\n",
    "    else:\n",
    "        # sanity check: if manual cost < automation cost partition is invalid (should not have been formed)\n",
    "        if manual_cost < automation_cost:\n",
    "            partition_cost = 100000000 # (value doesn't matter)\n",
    "            partition_is_valid = False\n",
    "        else:\n",
    "            partition_cost = automation_cost\n",
    "            partition_is_valid = True\n",
    "\n",
    "    return partition_cost, partition_done_manually, partition_is_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d2185",
   "metadata": {},
   "source": [
    "### Compute costs of all \"valid\" plans\n",
    "#### Check for new validity condition: automated cost of tasks in partition must be less than the human costs of not deploying any machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "11e0cd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid execution plans: 766\n",
      "\n",
      "Example Execution Plans:\n",
      "[[0, 1, 2, 3, 5], [4, 6]]\n",
      "[[0, 1], [2, 3, 4, 5, 6]]\n",
      "[[0, 5, 6], [1, 2, 3, 4]]\n",
      "[[0, 1, 4], [2, 3, 5, 6]]\n",
      "[[0, 1, 2], [3, 4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "def execute_plans(valid_partitions, M_dict, A_dict, D_dict, alpha):\n",
    "    execution_plan = []\n",
    "    execution_plan_manual_tasks = []\n",
    "    execution_cost = []\n",
    "    counter = 0\n",
    "    for partition_scheme in valid_partitions:\n",
    "        # initialize partition scheme cost\n",
    "        # and partitions that are done manually\n",
    "        partition_scheme_cost = 0\n",
    "        manual_partitions = []\n",
    "        \n",
    "        for partition in partition_scheme:\n",
    "            # calculate partition cost \n",
    "            partition_cost, partition_done_manually, partition_is_valid = compute_partition_cost(M_dict, A_dict, D_dict, alpha, partition)\n",
    "        \n",
    "            # if (automated) partition is invalid ignore partition scheme\n",
    "            # and stop calculating costs of further partitions\n",
    "            if not partition_is_valid:\n",
    "                break\n",
    "\n",
    "            if partition_done_manually:\n",
    "                manual_partitions.append(partition)\n",
    "\n",
    "            # if (automated) partition passes sanity check\n",
    "            # add this partition's cost to partition scheme cost\n",
    "            partition_scheme_cost += partition_cost\n",
    "        \n",
    "        # if stopped because an (automated) partition wasn't valid\n",
    "        # ignore current partition scheme and continue\n",
    "        if not partition_is_valid:\n",
    "            continue\n",
    "        \n",
    "        # if partition scheme makes sense append costs\n",
    "        execution_plan.append(partition_scheme)\n",
    "        execution_plan_manual_tasks.append(manual_partitions)\n",
    "        execution_cost.append(partition_scheme_cost)\n",
    "\n",
    "        # if counter % (np.floor(len(valid_partitions)/3)) == 0:\n",
    "        #     print(partition_scheme)\n",
    "        #     print(partition_scheme_cost)\n",
    "        #     print('\\n')\n",
    "        # counter += 1\n",
    "\n",
    "    return execution_plan, execution_plan_manual_tasks, execution_cost\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "execution_plan, execution_plan_manual_tasks, execution_cost = execute_plans(valid_partitions, M_dict, A_dict, D_dict, alpha)\n",
    "print(f'Number of valid execution plans: {len(execution_plan)}')\n",
    "\n",
    "# print some valid execution plans\n",
    "print('\\nExample Execution Plans:')\n",
    "for plan in execution_plan[10:15]:\n",
    "    print(plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4df3d1",
   "metadata": {},
   "source": [
    "### Calculate minimum cost for each alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5848de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "minimum_cost_list = []\n",
    "number_of_optimal_schemes_list = []\n",
    "optimal_execution_plan_list = []\n",
    "optimal_plan_manualTasks_list = []\n",
    "optimal_plan_manualTasks_count_list = []\n",
    "for alpha in alpha_list:\n",
    "    # get list of execution plans and costs for this alpha\n",
    "    execution_plan, execution_plan_manual_tasks, execution_cost = execute_plans(valid_partitions, M_dict, A_dict, D_dict, alpha)\n",
    "\n",
    "    # choose minimum\n",
    "    minimum_cost = min(execution_cost)\n",
    "    minimum_cost_index = [index for index, value in enumerate(execution_cost) if value == minimum_cost]\n",
    "\n",
    "    # in rare cases there are more than one optimal plan\n",
    "    if len(minimum_cost_index) > 2:\n",
    "        print(alpha)\n",
    "        for index in minimum_cost_index:\n",
    "            optimal_execution_scheme = execution_plan[index]\n",
    "            optimal_execution_manual_tasks = execution_plan_manual_tasks[index]\n",
    "            print(optimal_execution_scheme)\n",
    "            print(optimal_execution_manual_tasks)\n",
    "    else:\n",
    "        optimal_execution_scheme = execution_plan[minimum_cost_index[0]]\n",
    "        optimal_execution_manual_tasks = execution_plan_manual_tasks[minimum_cost_index[0]]\n",
    "    \n",
    "    # append lists\n",
    "    minimum_cost_list.append(minimum_cost)\n",
    "    number_of_optimal_schemes_list.append(len(minimum_cost_index))\n",
    "    optimal_execution_plan_list.append(optimal_execution_scheme)\n",
    "    optimal_plan_manualTasks_list.append(optimal_execution_manual_tasks)\n",
    "    optimal_plan_manualTasks_count_list.append(len(optimal_execution_manual_tasks))\n",
    "\n",
    "# save outputs\n",
    "output_df = pd.DataFrame({\n",
    "    'alpha': alpha_list,\n",
    "    'optimal_schemes_count': number_of_optimal_schemes_list,\n",
    "    'minimum_cost': minimum_cost_list,\n",
    "    'optimal_scheme': optimal_execution_plan_list,\n",
    "    'optimal_scheme_manual_tasks': optimal_plan_manualTasks_list,\n",
    "    'manual_tasks_count': optimal_plan_manualTasks_count_list\n",
    "})\n",
    "output_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
