{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b861f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('libraries.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)\n",
    "\n",
    "with open('functions.py') as f:\n",
    "    code = f.read()\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdd9c966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['caffeinate']>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run caffeinate in the background to prevent sleep\n",
    "subprocess.Popen(['caffeinate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "280bdd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine user\n",
    "user = getpass.getuser()\n",
    "if user == 'peymansh':\n",
    "    main_folder_path = '/Users/peymansh/Dropbox (MIT)/Research/AI and Occupations/ai-exposure'\n",
    "    data_path = f'{main_folder_path}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54980713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tasks(onet_data_path,\n",
    "              occupation_code):\n",
    "\n",
    "    # Load the data\n",
    "    onet = pd.read_csv(onet_data_path)\n",
    "    onet = onet.sort_values(by=['year', 'occ_code', 'occ_title', 'task_id'])\n",
    "    onet = onet[onet['year'] == 2023].reset_index(drop=True)\n",
    "\n",
    "    # Get list of tasks\n",
    "    my_df = onet[(onet.occ_code == f'{occupation_code}') & (onet.year == 2023)]\n",
    "    tasks = my_df['task'].unique().tolist()\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8161f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(adjacency_matrix):\n",
    "    # Get the number of nodes (n) from the shape of the adjacency matrix\n",
    "    n = adjacency_matrix.shape[0]\n",
    "    \n",
    "    # Initialize an empty dictionary to store the neighbors for each node\n",
    "    neighbors = {i: [] for i in range(n)}\n",
    "    \n",
    "    # Loop through each entry in the adjacency matrix\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # If there's an edge from i to j or from j to i, add j to the neighbors of i\n",
    "            if adjacency_matrix[i, j] == 1 or adjacency_matrix[j, i] == 1:\n",
    "                if j not in neighbors[i]:  # Avoid duplicate neighbors\n",
    "                    neighbors[i].append(j)\n",
    "                if i not in neighbors[j]:  # Ensure symmetry in the undirected version\n",
    "                    neighbors[j].append(i)\n",
    "    \n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc8778eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inactive_node_neighbor_subset_combinations(inactive_neighbors_valid_subsets_dict):\n",
    "    # Step 1: Extract lists from dictionary and remove duplicates within lists\n",
    "    all_lists = [list(set(item)) for sublist in inactive_neighbors_valid_subsets_dict.values() for item in sublist]\n",
    "\n",
    "    # Step 2: Create all combinations of lists across keys\n",
    "    output_set = set()\n",
    "    for r in range(1, len(all_lists) + 1):\n",
    "        combinations = itertools.combinations(all_lists, r)\n",
    "        for combo in combinations:\n",
    "            # Flatten the combination of lists\n",
    "            flattened_combo = list(itertools.chain(*combo))\n",
    "            # Remove duplicates within the flattened list and sort for consistency\n",
    "            unique_combo = tuple(sorted(set(flattened_combo)))\n",
    "            # Add the unique combination to the output set\n",
    "            output_set.add(unique_combo)\n",
    "\n",
    "    # Convert the set back to a list of lists\n",
    "    output_list = [list(combo) for combo in output_set]\n",
    "    return sorted(output_list, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f85a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_DAG_subsets(adjacency_matrix):\n",
    "    # subset adjacency matrix to exclude Target node\n",
    "    non_target_adjacency_matrix = adjacency_matrix[:-1,:-1].copy()\n",
    "\n",
    "    # get number of non-Target nodes\n",
    "    n = non_target_adjacency_matrix.shape[0]\n",
    "\n",
    "\n",
    "    def valid_subsets_recursive(adjacency_matrix, active_dict, memory_dict, node):\n",
    "\n",
    "        # if partition already in memory return its value\n",
    "        try:\n",
    "            if len(memory_dict[node]) > 0:\n",
    "                return memory_dict[node]\n",
    "            \n",
    "        # if partition not in memory, get valid subsets of partition\n",
    "        except KeyError:\n",
    "            # get inactive neighbors of node\n",
    "            neighbors_list = find_neighbors(adjacency_matrix)[node]\n",
    "            inactive_neighbors_list = [neighbor for neighbor in neighbors_list if active_dict[neighbor] == False]\n",
    "\n",
    "            # if node is a last node (i.e. has no outgoing edges) return node and empty list\n",
    "            if len(inactive_neighbors_list) == 0:\n",
    "                memory_dict[node] = [[node], []]\n",
    "                return [[node], []]\n",
    "\n",
    "            # for each inactive neighbor get valid subsets\n",
    "            inactive_neighbors_valid_subsets = {}\n",
    "\n",
    "            ############\n",
    "            # set all neighbors to active and get valid subsets of neighbors\n",
    "            for neighbor in inactive_neighbors_list:\n",
    "                active_dict[neighbor] = True\n",
    "            # print(f'\\n\\n>>>>>>>>>active dict: {active_dict}<<<<<<<<<')\n",
    "            ############\n",
    "\n",
    "            for neighbor in inactive_neighbors_list:\n",
    "                # make a copy of active_dict and memory dict for the neighbor contingencies\n",
    "                neighbor_active_dict = active_dict.copy()\n",
    "                neighbor_memory_dict = memory_dict.copy()\n",
    "\n",
    "                valid_subsets = valid_subsets_recursive(adjacency_matrix, neighbor_active_dict, neighbor_memory_dict, neighbor)\n",
    "\n",
    "                # add node itself to valid subsets of neighbor\n",
    "                valid_subsets = [subset + [node] for subset in valid_subsets]\n",
    "                valid_subsets = sorted(valid_subsets, key=len)\n",
    "\n",
    "                # add valid subsets of neighbor to memory\n",
    "                memory_dict[neighbor] = valid_subsets\n",
    "\n",
    "                # append valid subsets of neighbor to valid subsets of node\n",
    "                inactive_neighbors_valid_subsets[neighbor] = valid_subsets\n",
    "\n",
    "            # create all combinations of valid subsets of inactive neighbors\n",
    "            inactive_neighbors_valid_subsets_combinations = create_inactive_node_neighbor_subset_combinations(inactive_neighbors_valid_subsets)\n",
    "            \n",
    "            # add node itself as a single node subset\n",
    "            inactive_neighbors_valid_subsets_combinations.append([node])\n",
    "\n",
    "            # keep only unique combinations\n",
    "            inactive_neighbors_valid_subsets_combinations = [list(t) for t in set(tuple(inner_list) for inner_list in inactive_neighbors_valid_subsets_combinations)]\n",
    "\n",
    "            # sort combinations by length\n",
    "            inactive_neighbors_valid_subsets_combinations = sorted(inactive_neighbors_valid_subsets_combinations, key=len)\n",
    "\n",
    "            # filter out empty subsets -- mainly for compatibility reasons due to last nodes (i.e., nodes w/o outgoing edges)\n",
    "            inactive_neighbors_valid_subsets_combinations = [subset for subset in inactive_neighbors_valid_subsets_combinations if subset != []]\n",
    "\n",
    "            return inactive_neighbors_valid_subsets_combinations\n",
    "\n",
    "\n",
    "    # initialize dictionary for valid subsets origniating from each node\n",
    "    valid_subsets_dict = {}\n",
    "    \n",
    "    # run the algorithm on each node and remove that node from the adjacency matrix after each iteration\n",
    "    my_adjacency_matrix = non_target_adjacency_matrix.copy()\n",
    "    for node in range(n):\n",
    "\n",
    "        # create active dictionary\n",
    "        global_active_dict = {i: False for i in range(n)}\n",
    "\n",
    "        # set node as active\n",
    "        global_active_dict[0] = True\n",
    "\n",
    "        # initialize dict for valid subsets of nodes (and also partitions) to act as memory\n",
    "        global_memory_dict = {}\n",
    "\n",
    "        # get valid subsets of node 0 in current adjacency matrix\n",
    "        valid_subsets = valid_subsets_recursive(my_adjacency_matrix, global_active_dict, global_memory_dict, 0)\n",
    "\n",
    "        # adjust output above for actual node number in main adjacency matrix\n",
    "        valid_subsets_dict[node] = [[element + node for element in subset] for subset in valid_subsets]\n",
    "\n",
    "        # ensure empty subsets do not exist in valid subsets\n",
    "        valid_subsets_dict[node] = [subset for subset in valid_subsets_dict[node] if len(subset) > 0]\n",
    "\n",
    "        # remove current node from adjacency matrix for next iteration\n",
    "        my_adjacency_matrix = my_adjacency_matrix[1:,1:]\n",
    "    \n",
    "    return valid_subsets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ad774b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_combination_valid(combination, n):\n",
    "    # Flatten list of combination\n",
    "    covered_tasks_list = [element for sublist in combination for element in sublist]\n",
    "    \n",
    "    # Create a set of the flattened list\n",
    "    covered_tasks_set = set(covered_tasks_list)\n",
    "    \n",
    "    # Check if the flattened set has exactly n elements and contains all elements from 0 to n-1\n",
    "    if len(covered_tasks_list) == n and covered_tasks_set == set(range(n)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_combinations(valid_subsets_dict, current_key=0, current_combination=None, result=None):\n",
    "    if current_combination is None:\n",
    "        current_combination = []\n",
    "    if result is None:\n",
    "        result = []\n",
    "\n",
    "    # Base case: if convered all tasks add current combination to the result list\n",
    "    if is_combination_valid(current_combination, len(valid_subsets_dict)):\n",
    "        result.append(current_combination)\n",
    "        return result\n",
    "\n",
    "    # Recursive case: iterate through the list of lists at the current key\n",
    "    for subset in valid_subsets_dict[current_key]:\n",
    "        # Create a new combination including the current subset\n",
    "        new_combination = current_combination + [subset]\n",
    "        new_combination_flattened = [element for sublist in new_combination for element in sublist]\n",
    "        \n",
    "        # lower the load of computation by skipping invalid combinations\n",
    "        # combination is invalid if:\n",
    "        # 1. length of new combination is greater than the length of valid subsets dictionary\n",
    "        # 2. new combination contains repetitive elements\n",
    "\n",
    "        # skip cases in which the new combination is invalid\n",
    "        if len(new_combination_flattened) > len(valid_subsets_dict):\n",
    "            continue\n",
    "        if len(new_combination_flattened) != len(set(new_combination_flattened)):\n",
    "            continue\n",
    "\n",
    "        # print(f'new combination: {new_combination}')\n",
    "\n",
    "        # Check which nodes are NOT covered by the new combination\n",
    "        uncovered_nodes = list(set(range(len(valid_subsets_dict))) - set(new_combination_flattened))\n",
    "        if len(uncovered_nodes) == 0:\n",
    "            if is_combination_valid(new_combination, len(valid_subsets_dict)):\n",
    "                result.append(new_combination)\n",
    "                return result\n",
    "        else:\n",
    "            # Recursively call the function to process the next key\n",
    "            for next_key in range(current_key + 1, len(valid_subsets_dict)):\n",
    "                generate_combinations(valid_subsets_dict, next_key, new_combination, result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61692d1b",
   "metadata": {},
   "source": [
    "### Compute costs of all \"valid\" execution plans\n",
    "#### New check for validity: automated cost of tasks in non-singleton partition must be less than human costs doing partition tasks separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3b358c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_boundary(adjacency_matrix, partition):\n",
    "    # create a matrix whose columns are nodes not in the partition and whose rows are nodes in the partition\n",
    "    # (subset adjacency matrix to outgoing edges of partition nodes --i.e., rows-- and incoming edges of non-partition nodes --i.e., columns.)\n",
    "    reduced_matrix = np.delete(adjacency_matrix, partition, axis=1) \n",
    "    reduced_matrix = reduced_matrix[partition, :]\n",
    "\n",
    "    # find nodes in partition w/ an edge to non-partition nodes\n",
    "    partition_boundary_tasks = [i for i in partition if np.any(reduced_matrix[partition.index(i), :])]\n",
    "\n",
    "    return partition_boundary_tasks\n",
    "\n",
    "\n",
    "def compute_partition_cost(adjacency_matrix, M_dict, A_dict, D_dict, AI_quality, partition):\n",
    "    # initialize task_done_by_human as False\n",
    "    # (only if partition is singleton and human cost <= automated cost partition is done manually)\n",
    "    task_done_by_human = False\n",
    "\n",
    "    # initialize partition boundary tasks as empty set []\n",
    "    partition_boundary_tasks = []\n",
    "\n",
    "    # if partition is a singleton \n",
    "    # pick minimum of human and management cost\n",
    "    if len(partition) == 1:\n",
    "        partition_is_valid = True # single-node partition is always valid\n",
    "\n",
    "        # calculate human cost\n",
    "        human_cost = sum(M_dict[key] for key in partition)\n",
    "\n",
    "        # calculate management cost\n",
    "        AI_cost = sum(A_dict[key] for key in partition)\n",
    "        difficulty = sum(D_dict[key] for key in partition)\n",
    "        management_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "        \n",
    "        # pick the minimum of the two\n",
    "        if human_cost < management_cost:\n",
    "            partition_cost = human_cost\n",
    "            task_done_by_human = True\n",
    "        else:\n",
    "            partition_cost = management_cost\n",
    "            partition_boundary_tasks = partition\n",
    "    \n",
    "\n",
    "    # if partition not a singleton \n",
    "    # calculate management cost and return if partition passes a sanity check\n",
    "    if len(partition) > 1:\n",
    "        # calculate human cost\n",
    "        human_cost = sum(M_dict[key] for key in partition)\n",
    "\n",
    "        # calculate management cost\n",
    "        # first get boundary tasks in partition\n",
    "        partition_boundary_tasks = get_partition_boundary(adjacency_matrix, partition)\n",
    "\n",
    "        # sanity check: no partition should have inner boundary of empty set\n",
    "        if len(partition_boundary_tasks) == 0:\n",
    "            # raise ValueError(f'Inner boundary of partition {partition} is empty set.')\n",
    "            return 100000000, [], [], False\n",
    "        \n",
    "        # if partition has at least one boundary task calculate management cost\n",
    "        # use boundary tasks for calculating management costs and partition tasks for difficulty\n",
    "        AI_cost = sum(A_dict[key] for key in partition_boundary_tasks)\n",
    "        difficulty = sum(D_dict[key] for key in partition)\n",
    "        management_cost = AI_cost * (AI_quality ** (-1 * difficulty))\n",
    "\n",
    "        # sanity check for partition validity: \n",
    "        # if human cost < management cost partition is invalid (should not have been formed)\n",
    "        if human_cost < management_cost:\n",
    "            partition_cost = 100000000 # (value doesn't matter)\n",
    "            partition_is_valid = False\n",
    "        else:\n",
    "            partition_cost = management_cost\n",
    "            partition_is_valid = True\n",
    "            partition_boundary_tasks\n",
    "    \n",
    "    return partition_cost, partition_boundary_tasks, task_done_by_human, partition_is_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "694fca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Random Thought: maybe better to sort valid_partitions on descending partition order to avoid recalculating single node partitions everytime? \n",
    "# tho the downside is that we have to first do the heavy calculations first...\n",
    "\n",
    "\n",
    "def execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, alpha):\n",
    "    execution_plan = []\n",
    "    execution_plan_augmented_tasks = []\n",
    "    execution_plan_human_tasks = []\n",
    "    execution_plan_cost = []\n",
    "    counter = 0\n",
    "    for scheme in valid_partitions:\n",
    "        # initialize scheme cost\n",
    "        # and partitions that are done manually\n",
    "        scheme_cost = 0\n",
    "        augmented_tasks = []\n",
    "        human_tasks = []\n",
    "        \n",
    "        for partition in scheme:\n",
    "            # calculate partition cost \n",
    "            partition_cost, partition_boundary_tasks, task_done_by_human, partition_is_valid = compute_partition_cost(adjacency_matrix, M_dict, A_dict, D_dict, alpha, partition)\n",
    "        \n",
    "            # if (automated) partition is invalid ignore partition scheme\n",
    "            # and stop calculating costs of further partitions\n",
    "            if not partition_is_valid:\n",
    "                break\n",
    "\n",
    "            if task_done_by_human:\n",
    "                human_tasks.append(partition)\n",
    "            \n",
    "            if not task_done_by_human:\n",
    "                for boundary_task in partition_boundary_tasks:\n",
    "                    augmented_tasks.append([boundary_task])\n",
    "\n",
    "            # if (automated) partition passes sanity check\n",
    "            # add this partition's cost to partition scheme cost\n",
    "            scheme_cost += partition_cost\n",
    "        \n",
    "        # if stopped because an (automated) partition wasn't valid\n",
    "        # ignore current partition scheme and continue\n",
    "        if not partition_is_valid:\n",
    "            continue\n",
    "        \n",
    "        # if partition scheme makes sense append costs\n",
    "        execution_plan.append(scheme)\n",
    "        execution_plan_augmented_tasks.append(augmented_tasks)\n",
    "        execution_plan_human_tasks.append(human_tasks)\n",
    "        execution_plan_cost.append(scheme_cost)\n",
    "\n",
    "    return execution_plan, execution_plan_augmented_tasks, execution_plan_human_tasks, execution_plan_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7354c2d",
   "metadata": {},
   "source": [
    "### Combine steps into a function to run a for loop over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e122fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAG_costMin(input_path, output_path, n=1000):\n",
    "    # set alpha as AI quality metric\n",
    "    epsilon = 1e-8\n",
    "    alpha_list = np.linspace(epsilon, 1-epsilon, n).tolist()\n",
    "\n",
    "\n",
    "\n",
    "    # read DAG\n",
    "    dag_df = pd.read_csv(input_path)\n",
    "\n",
    "    # remove edges if comment column labeled with \"TriangleRemovedFlag\" (edge is there for plotting purposes and is not part of the actual DAG)\n",
    "    if 'comment' in dag_df.columns:\n",
    "        dag_df = dag_df[~dag_df['comment'].str.endswith('TriangleRemovedFlag')]\n",
    "\n",
    "    # get task stats\n",
    "    tasks_stats = pd.read_csv(f'{occupation_folder}/{occupation}_taskStats.csv')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # extract list of tasks and create a dictionary for indexing tasks\n",
    "    tasks_list = tasks_stats['task'].unique()\n",
    "    tasks_dict = {i: node for i, node in enumerate(tasks_list, start=0)}\n",
    "\n",
    "    # create numpy array of adjacency matrix\n",
    "    adjacency_matrix = np.zeros((len(tasks_list), len(tasks_list)), dtype=int)\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    for _, row in dag_df.iterrows():\n",
    "        source_index = aux_dict[row['source']]\n",
    "        target_index = aux_dict[row['target']]\n",
    "        adjacency_matrix[source_index, target_index] = 1\n",
    "\n",
    "\n",
    "    # add task_dict key and reset index\n",
    "    aux_dict = {value: key for key, value in tasks_dict.items()}\n",
    "    tasks_stats['dict_index'] = tasks_stats.apply(lambda row: aux_dict[row.task], axis=1)\n",
    "    tasks_stats = tasks_stats.sort_values(by='dict_index')\n",
    "    tasks_stats = tasks_stats.set_index('dict_index', drop=False)\n",
    "    tasks_stats.index.name = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # create dictionaries for human cost, management cost, and difficulty\n",
    "    M_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['human_cost']))\n",
    "    A_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['management_cost']))\n",
    "    D_dict = dict(zip(tasks_stats['dict_index'], tasks_stats['difficulty']))\n",
    "\n",
    "\n",
    "    # get all valid subsets in a dict\n",
    "    valid_subsets_creation_start = time.time()\n",
    "    valid_subsets_dict = get_valid_DAG_subsets(adjacency_matrix)\n",
    "    valid_subsets_creation_time = (time.time() - valid_subsets_creation_start)/60\n",
    "    print(f\"valid subsets dictionary creation: {valid_subsets_creation_time:.2f} minutes\")\n",
    "    \n",
    "    # generate valid combinations\n",
    "    generate_valid_partitions_start = time.time()\n",
    "    valid_partitions = generate_combinations(valid_subsets_dict, 0)\n",
    "    generate_valid_partitions_time = (time.time() - generate_valid_partitions_start)/60\n",
    "    print(f\"valid execution plans generation: {generate_valid_partitions_time:.2f} minutes\")\n",
    "\n",
    "    # Print stats\n",
    "    print(f'Number of valid partitioning schemes given DAG structure: {len(valid_partitions)}')\n",
    "\n",
    "\n",
    "    \n",
    "    # run once to get stat\n",
    "    execution_plan, execution_plan_augmented_tasks, execution_plan_human_tasks, execution_plan_cost = execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, 0.5)\n",
    "    print(f'Number of valid execution plans: {len(execution_plan)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    random.seed(1)\n",
    "    minimum_cost_list = []\n",
    "    number_of_optimal_schemes_list = []\n",
    "    optimal_execution_plan_list = []\n",
    "    optimal_plan_augmentedTasks_list = []\n",
    "    optimal_plan_augmentedTasks_count_list = []\n",
    "    optimal_plan_humanTasks_list = []\n",
    "    optimal_plan_humanTasks_count_list = []\n",
    "    for counter, alpha in enumerate(alpha_list):\n",
    "        # if counter % 100 == 0:\n",
    "        #     print(f'-- Running {counter}th alpha --')\n",
    "\n",
    "        # get list of execution plans and costs for this alpha\n",
    "        execution_plan, execution_plan_augmented_tasks, execution_plan_human_tasks, execution_plan_cost = execute_plans(adjacency_matrix, valid_partitions, M_dict, A_dict, D_dict, alpha)\n",
    "\n",
    "        # choose minimum\n",
    "        minimum_cost = min(execution_plan_cost)\n",
    "        minimum_cost_index = [index for index, value in enumerate(execution_plan_cost) if value == minimum_cost]\n",
    "\n",
    "        # in rare cases there are more than one optimal plan\n",
    "        if len(minimum_cost_index) > 1:\n",
    "            optimal_execution_scheme = [execution_plan[index] for index in minimum_cost_index]\n",
    "            optimal_execution_human_tasks = [execution_plan_human_tasks[index] for index in minimum_cost_index]\n",
    "            optimal_execution_augmented_tasks = [execution_plan_augmented_tasks[index] for index in minimum_cost_index]\n",
    "            # print(alpha)\n",
    "            # print(optimal_execution_scheme)\n",
    "            # print(optimal_execution_human_tasks)\n",
    "            # print(f'Multiple Execution Plans for alpha={alpha}')\n",
    "        else:\n",
    "            optimal_execution_scheme = execution_plan[minimum_cost_index[0]]\n",
    "            optimal_execution_human_tasks = execution_plan_human_tasks[minimum_cost_index[0]]\n",
    "            optimal_execution_augmented_tasks = execution_plan_augmented_tasks[minimum_cost_index[0]]\n",
    "\n",
    "        # append lists\n",
    "        minimum_cost_list.append(minimum_cost)\n",
    "        number_of_optimal_schemes_list.append(len(minimum_cost_index))\n",
    "        optimal_execution_plan_list.append(optimal_execution_scheme)\n",
    "        optimal_plan_augmentedTasks_list.append(optimal_execution_augmented_tasks)\n",
    "        optimal_plan_augmentedTasks_count_list.append(len(optimal_execution_augmented_tasks))\n",
    "        optimal_plan_humanTasks_list.append(optimal_execution_human_tasks)\n",
    "        optimal_plan_humanTasks_count_list.append(len(optimal_execution_human_tasks))\n",
    "\n",
    "    # save outputs\n",
    "    output_df = pd.DataFrame({\n",
    "        'alpha': alpha_list,\n",
    "        'optimal_schemes_count': number_of_optimal_schemes_list,\n",
    "        'cost': minimum_cost_list,\n",
    "        'optimal_scheme': optimal_execution_plan_list,\n",
    "        'optimal_scheme_augmented_tasks': optimal_plan_augmentedTasks_list,\n",
    "        'augmented_tasks_count': optimal_plan_augmentedTasks_count_list,\n",
    "        'optimal_scheme_human_tasks': optimal_plan_humanTasks_list,\n",
    "        'human_tasks_count': optimal_plan_humanTasks_count_list\n",
    "    })\n",
    "    output_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ec907",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ee7f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of alphas to sweept over: 100\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# number of alphas to sweep over\n",
    "n = 100\n",
    "print(f'Number of alphas to sweept over: {n}')\n",
    "\n",
    "onet_data_path = f'{data_path}/data/onet_occupations_yearly.csv'\n",
    "\n",
    "occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "                   'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "                   'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "                   'athletesAndSportsCompetitors', 'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists', \n",
    "                   'personalCareAides', 'proofreadersAndCopyMarkers', 'chiropractors', \n",
    "                   'shippingReceivingAndInventoryClerks', 'cooksShortOrder', 'orthodontists',\n",
    "                   'subwayAndStreetcarOperators', 'packersAndPackagersHand', 'hoistAndWinchOperators', \n",
    "                   'forgingMachineSettersOperatorsAndTenders', 'avionicsTechnicians', 'dishwashers', \n",
    "                   'dispatchersExceptPoliceFireAndAmbulance', 'familyMedicinePhysicians', 'MachineFeedersAndOffbearers'\n",
    "                   ]\n",
    "\n",
    "# occupation_list = ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators'\n",
    "#                    ]\n",
    "\n",
    "\n",
    "\n",
    "# occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "#                    'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "#                    'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "#                    'athletesAndSportsCompetitors'\n",
    "#                    ]\n",
    "\n",
    "occupation_list = ['pileDriverOperators', 'dredgeOperators', 'gradersAndSortersForAgriculturalProducts',\n",
    "                   'insuranceUnderwriters', 'insuranceAppraisersForAutoDamage', 'floorSandersAndFinishers', \n",
    "                   'reinforcingIronAndRebarWorkers', 'travelAgents', 'dataEntryKeyer', \n",
    "                   'audiovisualEquipmentInstallerAndRepairers', 'hearingAidSpecialists', \n",
    "                   'personalCareAides',\n",
    "                   ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3a99b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Running: pileDriverOperators ----------------------\n",
      "Number of non-target tasks: 5\n",
      "\n",
      "-------Running: pileDriverOperators - Manual DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "Number of valid execution plans: 24\n",
      "\n",
      "pileDriverOperators Manual DAG runtime: 0.04 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 30\n",
      "Number of valid execution plans: 15\n",
      "\n",
      "pileDriverOperators Naive DAG runtime: 0.05 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "Number of valid execution plans: 15\n",
      "\n",
      "pileDriverOperators Conditioned Naive DAG runtime: 0.04 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 30\n",
      "Number of valid execution plans: 15\n",
      "\n",
      "pileDriverOperators First-Last Task DAG runtime: 0.05 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 26\n",
      "Number of valid execution plans: 15\n",
      "\n",
      "pileDriverOperators Conditioned First-Last Task DAG runtime: 0.04 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 52\n",
      "Number of valid execution plans: 27\n",
      "\n",
      "pileDriverOperators Partitioned DAG runtime: 0.08 seconds\n",
      "\n",
      "-------Running: pileDriverOperators - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 34\n",
      "Number of valid execution plans: 21\n",
      "\n",
      "pileDriverOperators Conditioned Partitioned DAG runtime: 0.05 seconds\n",
      "\n",
      "\n",
      "************* pileDriverOperators runtime: 0.01 minutes *************\n",
      "\n",
      "runtime since start: 0.01 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: dredgeOperators ----------------------\n",
      "Number of non-target tasks: 6\n",
      "\n",
      "-------Running: dredgeOperators - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 80\n",
      "Number of valid execution plans: 22\n",
      "\n",
      "dredgeOperators Naive DAG runtime: 0.13 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 32\n",
      "Number of valid execution plans: 26\n",
      "\n",
      "dredgeOperators Conditioned Naive DAG runtime: 0.06 seconds\n",
      "\n",
      "-------Running: dredgeOperators - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 60\n",
      "Number of valid execution plans: 21\n",
      "\n",
      "dredgeOperators First-Last Task DAG runtime: 0.11 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 32\n",
      "Number of valid execution plans: 14\n",
      "\n",
      "dredgeOperators Conditioned First-Last Task DAG runtime: 0.05 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 158\n",
      "Number of valid execution plans: 40\n",
      "\n",
      "dredgeOperators Partitioned DAG runtime: 0.28 seconds\n",
      "\n",
      "-------Running: dredgeOperators - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 32\n",
      "Number of valid execution plans: 29\n",
      "\n",
      "dredgeOperators Conditioned Partitioned DAG runtime: 0.06 seconds\n",
      "\n",
      "\n",
      "************* dredgeOperators runtime: 0.02 minutes *************\n",
      "\n",
      "runtime since start: 0.03 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: gradersAndSortersForAgriculturalProducts ----------------------\n",
      "Number of non-target tasks: 6\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 104\n",
      "Number of valid execution plans: 50\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Naive DAG runtime: 0.18 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 94\n",
      "Number of valid execution plans: 46\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Conditioned Naive DAG runtime: 0.19 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 141\n",
      "Number of valid execution plans: 78\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts First-Last Task DAG runtime: 0.25 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 94\n",
      "Number of valid execution plans: 90\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Conditioned First-Last Task DAG runtime: 0.16 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 104\n",
      "Number of valid execution plans: 58\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Partitioned DAG runtime: 0.22 seconds\n",
      "\n",
      "-------Running: gradersAndSortersForAgriculturalProducts - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 104\n",
      "Number of valid execution plans: 58\n",
      "\n",
      "gradersAndSortersForAgriculturalProducts Conditioned Partitioned DAG runtime: 0.18 seconds\n",
      "\n",
      "\n",
      "************* gradersAndSortersForAgriculturalProducts runtime: 0.03 minutes *************\n",
      "\n",
      "runtime since start: 0.05 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: insuranceUnderwriters ----------------------\n",
      "Number of non-target tasks: 7\n",
      "\n",
      "-------Running: insuranceUnderwriters - Manual DAG-------\n",
      "valid subsets dictionary creation: 0.39 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 192\n",
      "Number of valid execution plans: 82\n",
      "\n",
      "insuranceUnderwriters Manual DAG runtime: 23.60 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 758\n",
      "Number of valid execution plans: 119\n",
      "\n",
      "insuranceUnderwriters Naive DAG runtime: 1.55 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.01 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 200\n",
      "Number of valid execution plans: 79\n",
      "\n",
      "insuranceUnderwriters Conditioned Naive DAG runtime: 0.77 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 758\n",
      "Number of valid execution plans: 130\n",
      "\n",
      "insuranceUnderwriters First-Last Task DAG runtime: 1.31 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 383\n",
      "Number of valid execution plans: 130\n",
      "\n",
      "insuranceUnderwriters Conditioned First-Last Task DAG runtime: 0.72 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 758\n",
      "Number of valid execution plans: 115\n",
      "\n",
      "insuranceUnderwriters Partitioned DAG runtime: 1.55 seconds\n",
      "\n",
      "-------Running: insuranceUnderwriters - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 308\n",
      "Number of valid execution plans: 119\n",
      "\n",
      "insuranceUnderwriters Conditioned Partitioned DAG runtime: 0.71 seconds\n",
      "\n",
      "\n",
      "************* insuranceUnderwriters runtime: 0.51 minutes *************\n",
      "\n",
      "runtime since start: 0.56 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: insuranceAppraisersForAutoDamage ----------------------\n",
      "Number of non-target tasks: 7\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 758\n",
      "Number of valid execution plans: 210\n",
      "\n",
      "insuranceAppraisersForAutoDamage Naive DAG runtime: 1.42 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 616\n",
      "Number of valid execution plans: 163\n",
      "\n",
      "insuranceAppraisersForAutoDamage Conditioned Naive DAG runtime: 1.32 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 616\n",
      "Number of valid execution plans: 165\n",
      "\n",
      "insuranceAppraisersForAutoDamage First-Last Task DAG runtime: 1.15 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 546\n",
      "Number of valid execution plans: 139\n",
      "\n",
      "insuranceAppraisersForAutoDamage Conditioned First-Last Task DAG runtime: 0.97 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 825\n",
      "Number of valid execution plans: 244\n",
      "\n",
      "insuranceAppraisersForAutoDamage Partitioned DAG runtime: 1.76 seconds\n",
      "\n",
      "-------Running: insuranceAppraisersForAutoDamage - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 641\n",
      "Number of valid execution plans: 173\n",
      "\n",
      "insuranceAppraisersForAutoDamage Conditioned Partitioned DAG runtime: 1.38 seconds\n",
      "\n",
      "\n",
      "************* insuranceAppraisersForAutoDamage runtime: 0.14 minutes *************\n",
      "\n",
      "runtime since start: 0.70 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: floorSandersAndFinishers ----------------------\n",
      "Number of non-target tasks: 7\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 624\n",
      "Number of valid execution plans: 115\n",
      "\n",
      "floorSandersAndFinishers Naive DAG runtime: 1.12 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 551\n",
      "Number of valid execution plans: 104\n",
      "\n",
      "floorSandersAndFinishers Conditioned Naive DAG runtime: 0.95 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 551\n",
      "Number of valid execution plans: 106\n",
      "\n",
      "floorSandersAndFinishers First-Last Task DAG runtime: 1.02 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 403\n",
      "Number of valid execution plans: 82\n",
      "\n",
      "floorSandersAndFinishers Conditioned First-Last Task DAG runtime: 0.78 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 778\n",
      "Number of valid execution plans: 143\n",
      "\n",
      "floorSandersAndFinishers Partitioned DAG runtime: 1.61 seconds\n",
      "\n",
      "-------Running: floorSandersAndFinishers - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 576\n",
      "Number of valid execution plans: 109\n",
      "\n",
      "floorSandersAndFinishers Conditioned Partitioned DAG runtime: 1.10 seconds\n",
      "\n",
      "\n",
      "************* floorSandersAndFinishers runtime: 0.12 minutes *************\n",
      "\n",
      "runtime since start: 0.82 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: reinforcingIronAndRebarWorkers ----------------------\n",
      "Number of non-target tasks: 7\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 825\n",
      "Number of valid execution plans: 321\n",
      "\n",
      "reinforcingIronAndRebarWorkers Naive DAG runtime: 1.90 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 437\n",
      "Number of valid execution plans: 165\n",
      "\n",
      "reinforcingIronAndRebarWorkers Conditioned Naive DAG runtime: 0.86 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 376\n",
      "Number of valid execution plans: 93\n",
      "\n",
      "reinforcingIronAndRebarWorkers First-Last Task DAG runtime: 0.76 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 306\n",
      "Number of valid execution plans: 80\n",
      "\n",
      "reinforcingIronAndRebarWorkers Conditioned First-Last Task DAG runtime: 0.60 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 825\n",
      "Number of valid execution plans: 339\n",
      "\n",
      "reinforcingIronAndRebarWorkers Partitioned DAG runtime: 1.88 seconds\n",
      "\n",
      "-------Running: reinforcingIronAndRebarWorkers - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 641\n",
      "Number of valid execution plans: 260\n",
      "\n",
      "reinforcingIronAndRebarWorkers Conditioned Partitioned DAG runtime: 1.26 seconds\n",
      "\n",
      "\n",
      "************* reinforcingIronAndRebarWorkers runtime: 0.13 minutes *************\n",
      "\n",
      "runtime since start: 0.95 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: travelAgents ----------------------\n",
      "Number of non-target tasks: 8\n",
      "\n",
      "-------Running: travelAgents - Manual DAG-------\n",
      "valid subsets dictionary creation: 0.88 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 192\n",
      "Number of valid execution plans: 107\n",
      "\n",
      "travelAgents Manual DAG runtime: 52.92 seconds\n",
      "\n",
      "-------Running: travelAgents - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 2106\n",
      "Number of valid execution plans: 125\n",
      "\n",
      "travelAgents Naive DAG runtime: 3.63 seconds\n",
      "\n",
      "-------Running: travelAgents - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 428\n",
      "Number of valid execution plans: 38\n",
      "\n",
      "travelAgents Conditioned Naive DAG runtime: 0.73 seconds\n",
      "\n",
      "-------Running: travelAgents - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 2428\n",
      "Number of valid execution plans: 244\n",
      "\n",
      "travelAgents First-Last Task DAG runtime: 4.72 seconds\n",
      "\n",
      "-------Running: travelAgents - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 1432\n",
      "Number of valid execution plans: 152\n",
      "\n",
      "travelAgents Conditioned First-Last Task DAG runtime: 2.53 seconds\n",
      "\n",
      "-------Running: travelAgents - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 3749\n",
      "Number of valid execution plans: 317\n",
      "\n",
      "travelAgents Partitioned DAG runtime: 6.56 seconds\n",
      "\n",
      "-------Running: travelAgents - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 550\n",
      "Number of valid execution plans: 78\n",
      "\n",
      "travelAgents Conditioned Partitioned DAG runtime: 1.03 seconds\n",
      "\n",
      "\n",
      "************* travelAgents runtime: 1.21 minutes *************\n",
      "\n",
      "runtime since start: 2.15 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: dataEntryKeyer ----------------------\n",
      "Number of non-target tasks: 9\n",
      "\n",
      "-------Running: dataEntryKeyer - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 3355\n",
      "Number of valid execution plans: 2660\n",
      "\n",
      "dataEntryKeyer Naive DAG runtime: 9.40 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Conditioned Naive DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 2204\n",
      "Number of valid execution plans: 1783\n",
      "\n",
      "dataEntryKeyer Conditioned Naive DAG runtime: 6.07 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 3112\n",
      "Number of valid execution plans: 2200\n",
      "\n",
      "dataEntryKeyer First-Last Task DAG runtime: 8.46 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Conditioned First-Last Task DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 2262\n",
      "Number of valid execution plans: 1659\n",
      "\n",
      "dataEntryKeyer Conditioned First-Last Task DAG runtime: 6.16 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 2189\n",
      "Number of valid execution plans: 1926\n",
      "\n",
      "dataEntryKeyer Partitioned DAG runtime: 6.07 seconds\n",
      "\n",
      "-------Running: dataEntryKeyer - Conditioned Partitioned DAG-------\n",
      "valid subsets dictionary creation: 0.00 minutes\n",
      "valid execution plans generation: 0.00 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 672\n",
      "Number of valid execution plans: 655\n",
      "\n",
      "dataEntryKeyer Conditioned Partitioned DAG runtime: 1.90 seconds\n",
      "\n",
      "\n",
      "************* dataEntryKeyer runtime: 0.64 minutes *************\n",
      "\n",
      "runtime since start: 2.79 minutes\n",
      "\n",
      "\n",
      "---------------------- Running: audiovisualEquipmentInstallerAndRepairers ----------------------\n",
      "Number of non-target tasks: 11\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Naive DAG-------\n",
      "valid subsets dictionary creation: 0.01 minutes\n",
      "valid execution plans generation: 0.37 minutes\n",
      "Number of valid partitioning schemes given DAG structure: 404556\n",
      "Number of valid execution plans: 13074\n",
      "\n",
      "audiovisualEquipmentInstallerAndRepairers Naive DAG runtime: 1024.33 seconds\n",
      "\n",
      "-------Running: audiovisualEquipmentInstallerAndRepairers - Conditioned Naive DAG-------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 13\u001b[0m, in \u001b[0;36mget_valid_DAG_subsets.<locals>.valid_subsets_recursive\u001b[0;34m(adjacency_matrix, active_dict, memory_dict, node)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory_dict[node]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m memory_dict[node]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 13\u001b[0m, in \u001b[0;36mget_valid_DAG_subsets.<locals>.valid_subsets_recursive\u001b[0;34m(adjacency_matrix, active_dict, memory_dict, node)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory_dict[node]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m memory_dict[node]\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moccupation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDAG_indicator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m DAG_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 61\u001b[0m DAG_costMin(input_path, output_path, n)\n\u001b[1;32m     62\u001b[0m DAG_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     64\u001b[0m DAG_execution_time \u001b[38;5;241m=\u001b[39m DAG_end_time \u001b[38;5;241m-\u001b[39m DAG_start_time\n",
      "Cell \u001b[0;32mIn[76], line 52\u001b[0m, in \u001b[0;36mDAG_costMin\u001b[0;34m(input_path, output_path, n)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# get all valid subsets in a dict\u001b[39;00m\n\u001b[1;32m     51\u001b[0m valid_subsets_creation_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 52\u001b[0m valid_subsets_dict \u001b[38;5;241m=\u001b[39m get_valid_DAG_subsets(adjacency_matrix)\n\u001b[1;32m     53\u001b[0m valid_subsets_creation_time \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m valid_subsets_creation_start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid subsets dictionary creation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_subsets_creation_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[72], line 89\u001b[0m, in \u001b[0;36mget_valid_DAG_subsets\u001b[0;34m(adjacency_matrix)\u001b[0m\n\u001b[1;32m     86\u001b[0m global_memory_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# get valid subsets of node 0 in current adjacency matrix\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m valid_subsets \u001b[38;5;241m=\u001b[39m valid_subsets_recursive(my_adjacency_matrix, global_active_dict, global_memory_dict, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# adjust output above for actual node number in main adjacency matrix\u001b[39;00m\n\u001b[1;32m     92\u001b[0m valid_subsets_dict[node] \u001b[38;5;241m=\u001b[39m [[element \u001b[38;5;241m+\u001b[39m node \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m subset] \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m valid_subsets]\n",
      "Cell \u001b[0;32mIn[72], line 42\u001b[0m, in \u001b[0;36mget_valid_DAG_subsets.<locals>.valid_subsets_recursive\u001b[0;34m(adjacency_matrix, active_dict, memory_dict, node)\u001b[0m\n\u001b[1;32m     39\u001b[0m neighbor_active_dict \u001b[38;5;241m=\u001b[39m active_dict\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     40\u001b[0m neighbor_memory_dict \u001b[38;5;241m=\u001b[39m memory_dict\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 42\u001b[0m valid_subsets \u001b[38;5;241m=\u001b[39m valid_subsets_recursive(adjacency_matrix, neighbor_active_dict, neighbor_memory_dict, neighbor)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# add node itself to valid subsets of neighbor\u001b[39;00m\n\u001b[1;32m     45\u001b[0m valid_subsets \u001b[38;5;241m=\u001b[39m [subset \u001b[38;5;241m+\u001b[39m [node] \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m valid_subsets]\n",
      "Cell \u001b[0;32mIn[72], line 55\u001b[0m, in \u001b[0;36mget_valid_DAG_subsets.<locals>.valid_subsets_recursive\u001b[0;34m(adjacency_matrix, active_dict, memory_dict, node)\u001b[0m\n\u001b[1;32m     52\u001b[0m     inactive_neighbors_valid_subsets[neighbor] \u001b[38;5;241m=\u001b[39m valid_subsets\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# create all combinations of valid subsets of inactive neighbors\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m inactive_neighbors_valid_subsets_combinations \u001b[38;5;241m=\u001b[39m create_inactive_node_neighbor_subset_combinations(inactive_neighbors_valid_subsets)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# add node itself as a single node subset\u001b[39;00m\n\u001b[1;32m     58\u001b[0m inactive_neighbors_valid_subsets_combinations\u001b[38;5;241m.\u001b[39mappend([node])\n",
      "Cell \u001b[0;32mIn[71], line 13\u001b[0m, in \u001b[0;36mcreate_inactive_node_neighbor_subset_combinations\u001b[0;34m(inactive_neighbors_valid_subsets_dict)\u001b[0m\n\u001b[1;32m     11\u001b[0m flattened_combo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain(\u001b[38;5;241m*\u001b[39mcombo))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Remove duplicates within the flattened list and sort for consistency\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m unique_combo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(flattened_combo)))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Add the unique combination to the output set\u001b[39;00m\n\u001b[1;32m     15\u001b[0m output_set\u001b[38;5;241m.\u001b[39madd(unique_combo)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_tasks_current = 0\n",
    "num_tasks_previous = 0\n",
    "for occupation in occupation_list:\n",
    "    print(f'\\n---------------------- Running: {occupation} ----------------------')\n",
    "    occupation_start_time = time.time()\n",
    "\n",
    "    # generate occupation-specific strings\n",
    "    GPT_input_occupation, plot_title_occupation, occupation_code, occupation_folder = pick_occupation(occupation)\n",
    "\n",
    "\n",
    "    # Get occupation tasks to create all possible partitions\n",
    "    tasks = get_tasks(onet_data_path, occupation_code)\n",
    "    num_tasks_current = len(tasks)\n",
    "    print(f'Number of non-target tasks: {num_tasks_current}')\n",
    "\n",
    "    # Manual DAG\n",
    "    M_input_path = f'{occupation_folder}/{occupation}_M_DAG_df.csv'\n",
    "    M_output_path = f'{occupation_folder}/{occupation}_costMin_M.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    N_input_path = f'{occupation_folder}/{occupation}_N_GPT_DAG_df.csv'\n",
    "    N_output_path = f'{occupation_folder}/{occupation}_costMin_N.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    CN_input_path = f'{occupation_folder}/{occupation}_CN_GPT_DAG_df.csv'\n",
    "    CN_output_path = f'{occupation_folder}/{occupation}_costMin_CN.csv'\n",
    "\n",
    "    # First Last Task DAG\n",
    "    FLT_input_path = f'{occupation_folder}/{occupation}_FLT_GPT_DAG_df.csv'\n",
    "    FLT_output_path = f'{occupation_folder}/{occupation}_costMin_FLT.csv'\n",
    "\n",
    "    # Conditioned First Last Task DAG\n",
    "    CFLT_input_path = f'{occupation_folder}/{occupation}_CFLT_GPT_DAG_df.csv'\n",
    "    CFLT_output_path = f'{occupation_folder}/{occupation}_costMin_CFLT.csv'\n",
    "\n",
    "    # Partitioned DAG\n",
    "    P_input_path = f'{occupation_folder}/{occupation}_P_GPT_DAG_df.csv'\n",
    "    P_output_path = f'{occupation_folder}/{occupation}_costMin_P.csv'\n",
    "\n",
    "    # Conditioned Partitioned DAG\n",
    "    CP_input_path = f'{occupation_folder}/{occupation}_CP_GPT_DAG_df.csv'\n",
    "    CP_output_path = f'{occupation_folder}/{occupation}_costMin_CP.csv'\n",
    "    \n",
    "\n",
    "\n",
    "    # create list of all DAGs\n",
    "    if occupation in ['travelAgents', 'insuranceUnderwriters', 'pileDriverOperators']:\n",
    "        DAG_indicator_list = ['Manual DAG', 'Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [M_input_path, N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_paths_list = [M_output_path, N_output_path, CN_output_path, FLT_output_path, CFLT_output_path, P_output_path, CP_output_path]\n",
    "    else:\n",
    "        DAG_indicator_list = ['Naive DAG', 'Conditioned Naive DAG', 'First-Last Task DAG', 'Conditioned First-Last Task DAG', 'Partitioned DAG', 'Conditioned Partitioned DAG']\n",
    "        input_paths_list = [N_input_path, CN_input_path, FLT_input_path, CFLT_input_path, P_input_path, CP_input_path]\n",
    "        output_paths_list = [N_output_path, CN_output_path, FLT_output_path, CFLT_output_path, P_output_path, CP_output_path]\n",
    "\n",
    "\n",
    "    for DAG_indicator, input_path, output_path in zip(DAG_indicator_list, input_paths_list, output_paths_list):\n",
    "        print(f'\\n-------Running: {occupation} - {DAG_indicator}-------')\n",
    "        \n",
    "        DAG_start_time = time.time()\n",
    "        DAG_costMin(input_path, output_path, n)\n",
    "        DAG_end_time = time.time()\n",
    "\n",
    "        DAG_execution_time = DAG_end_time - DAG_start_time\n",
    "        print(f\"\\n{occupation} {DAG_indicator} runtime: {DAG_execution_time:.2f} seconds\")\n",
    "\n",
    "    occupation_end_time = time.time()\n",
    "    occupation_execution_time = (occupation_end_time - occupation_start_time)/60\n",
    "    print(f\"\\n\\n************* {occupation} runtime: {occupation_execution_time:.2f} minutes *************\")\n",
    "    runtime_since_start = (time.time() - start_time)/60\n",
    "    print(f\"\\nruntime since start: {runtime_since_start:.2f} minutes\\n\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"\\n\\nTotal Runtime: {execution_time:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
